{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modules.data_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import uproot as ur\n",
    "import time\n",
    "from multiprocessing import Process, Queue, set_start_method\n",
    "import compress_pickle as pickle\n",
    "from scipy.stats import circmean\n",
    "import random\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 13:17:56.680326: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\n"
     ]
    }
   ],
   "source": [
    "# tf.config.list_physical_devices('GPU')\n",
    "from graph_nets.graphs import GraphsTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphDataGenerator with tracks and no calorimeter data \n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TrackGraphDataGenerator:\n",
    "    \"\"\"\n",
    "    DataGenerator class for extracting and formating data from list of root files\n",
    "    This data generator uses the cell_geo file to create the input graph structure\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 pion_file_list: list,\n",
    "                 n_clusters: int,\n",
    "                 batch_size: int,\n",
    "                 shuffle: bool = True,\n",
    "                 num_procs = 32,\n",
    "                 preprocess = False,\n",
    "                 output_dir = None):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            self.pion_file_list = pion_file_list\n",
    "            self.num_files = len(self.pion_file_list)\n",
    "        else:\n",
    "            self.file_list = pion_file_list\n",
    "            self.num_files = len(self.file_list)\n",
    "        \n",
    "        self.nodeFeatureNames = ['cluster_E', 'track_pt', 'track_eta']\n",
    "        self.num_nodeFeatures = len(self.nodeFeatureNames)\n",
    "        \n",
    "        self.track_feature_names = ['trackPt','trackD0','trackZ0', 'trackEta_EMB2','trackPhi_EMB2',\n",
    "                                    'trackEta','trackPhi','truthPartE', 'truthPartPt']\n",
    "        self.cluster_feature_names = ['cluster_E', 'cluster_Eta', 'cluster_Phi', 'cluster_ENG_CALIB_TOT', \n",
    "                                      'cluster_EM_PROBABILITY','cluster_E_LCCalib','cluster_HAD_WEIGHT']\n",
    "        \n",
    "        self.dr_thresh = 1.2\n",
    "        self.clusterThresh = .5\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "        if self.shuffle: np.random.shuffle(self.file_list)\n",
    "        \n",
    "        self.num_procs = np.min([num_procs, self.num_files])\n",
    "        self.procs = []\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            os.makedirs(self.output_dir, exist_ok=True)\n",
    "            self.preprocess_data()\n",
    "    \n",
    "    def get_meta(self, event_data, event_ind, c_inds):\n",
    "        \"\"\" \n",
    "        Reading meta data\n",
    "        \"\"\"  \n",
    "        track_meta_data = []\n",
    "        for f in self.track_feature_names:\n",
    "            track_meta_data.append(event_data[f][event_ind])\n",
    "        \n",
    "        cluster_meta_data = []\n",
    "        for c in c_inds:\n",
    "            curr_meta = []\n",
    "            \n",
    "            for f in self.cluster_feature_names:\n",
    "                curr_meta.append(event_data[f][event_ind][c])\n",
    "            \n",
    "            cluster_meta_data.append(curr_meta)\n",
    "            \n",
    "        return np.array(track_meta_data, dtype=np.float32), np.array(cluster_meta_data, dtype=np.float32)\n",
    "    \n",
    "    def get_nodes(self, event_data, event_ind, c_inds):\n",
    "        \"\"\" Reading Node features \"\"\" \n",
    "\n",
    "        nodes = []\n",
    "        for c in c_inds:\n",
    "            cluster_E = np.log10(event_data['cluster_E'][event_ind][c])\n",
    "            curr_node = [cluster_E, 0, 0]\n",
    "            nodes.append(curr_node)\n",
    "        \n",
    "        # add the track node\n",
    "        trackPt = np.log10(event_data['trackPt'][event_ind][0])\n",
    "        nodes.append([0, trackPt, event_data['trackEta'][event_ind][0]])\n",
    "        \n",
    "        return np.array(nodes, dtype=np.float32)\n",
    "    \n",
    "    def get_cluster_inds(self, event_data, event_ind):\n",
    "        \n",
    "        if self.n_clusters==-1:   # get all nodes satisfying dR criterion\n",
    "            c_inds = range(event_data['nCluster'][event_ind])\n",
    "            c_inds = [c for c in c_inds if (event_data['dR'][event_ind][c]<self.dr_thresh) and \n",
    "                      (event_data['cluster_E'][event_ind][c]>self.clusterThresh)]\n",
    "        else:                # get n leading nodes satisfying dR criterion\n",
    "            c_inds = np.argsort(event_data['cluster_E'][event_ind])[::-1]\n",
    "            c_inds = [c for c in c_inds if (event_data['dR'][event_ind][c]<self.dr_thresh) and \n",
    "                      (event_data['cluster_E'][event_ind][c]>self.clusterThresh)]\n",
    "            c_inds = c_inds[:self.n_clusters]\n",
    "        \n",
    "        return c_inds\n",
    "    \n",
    "    def preprocessor(self, worker_id):\n",
    "        \"\"\"\n",
    "        Prerocessing root file data for faster data \n",
    "        generation during multiple training epochs\n",
    "        \"\"\"\n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            print(f\"Processing file number {file_num}\")\n",
    "            \n",
    "            ### Pions\n",
    "            if len(self.pion_file_list) == 0:\n",
    "                print(\"No pion files.\")\n",
    "            elif len(self.pion_file_list) > 0:\n",
    "                file = self.pion_file_list[file_num]\n",
    "                event_data = np.load(file, allow_pickle=True).item()\n",
    "                num_events = len(event_data[[key for key in event_data.keys()][0]])\n",
    "\n",
    "                preprocessed_data = []\n",
    "\n",
    "                for event_ind in range(num_events):\n",
    "                    truth_particle_E = np.log10(event_data['truthPartE'][event_ind][0]) # first one is the pion! \n",
    "                    trackPt = event_data['trackPt'][event_ind][0]\n",
    "                    if trackPt>5000:\n",
    "                        continue\n",
    "                        \n",
    "                    c_inds = self.get_cluster_inds(event_data, event_ind)\n",
    "                    if not len(c_inds):\n",
    "                        continue\n",
    "                    nodes = self.get_nodes(event_data, event_ind, c_inds)\n",
    "                    num_nodes = len(nodes)\n",
    "                    senders = [i for i in range(num_nodes) for j in range(num_nodes) if i != j]\n",
    "                    receivers = [j for i in range(num_nodes) for j in range(num_nodes) if i != j]\n",
    "                    n_edges = len(senders)\n",
    "                    edges = np.zeros(shape=[n_edges, 0], dtype=np.float32)\n",
    "                    global_node = np.zeros(shape=[1, 0], dtype=np.float32)\n",
    "                    track_meta_data, cluster_meta_data = self.get_meta(event_data, event_ind, c_inds)\n",
    "                    \n",
    "                    graph = {'nodes': nodes, \n",
    "                             'globals': global_node,\n",
    "                             'senders': np.array(senders, dtype=np.int64), \n",
    "                             'receivers': np.array(receivers, dtype=np.int64),\n",
    "                             'edges': edges}\n",
    "\n",
    "                    target = truth_particle_E.astype(np.float32)\n",
    "\n",
    "                    preprocessed_data.append((graph, target, track_meta_data, cluster_meta_data))\n",
    "\n",
    "            random.shuffle(preprocessed_data)\n",
    "\n",
    "            pickle.dump(preprocessed_data, open(self.output_dir + f'data_{file_num:03d}.p', 'wb'), compression='gzip')\n",
    "            \n",
    "            print(f\"Finished processing {file_num} files\")\n",
    "            file_num += self.num_procs\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        print('\\nPreprocessing and saving data to {}'.format(self.output_dir))\n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.preprocessor, args=(i,), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()\n",
    "\n",
    "        self.file_list = [self.output_dir + f'data_{i:03d}.p' for i in range(self.num_files)]\n",
    "\n",
    "    def preprocessed_worker(self, worker_id, batch_queue):\n",
    "        batch_graphs = []\n",
    "        batch_targets = []\n",
    "        batch_track_meta = []\n",
    "        batch_cluster_meta = []\n",
    "        \n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
    "\n",
    "            for i in range(len(file_data)):\n",
    "                batch_graphs.append(file_data[i][0])\n",
    "                batch_targets.append(file_data[i][1])\n",
    "                batch_track_meta.append(file_data[i][2])\n",
    "                batch_cluster_meta.append(file_data[i][3])\n",
    "                    \n",
    "                if len(batch_graphs) == self.batch_size:\n",
    "                    batch_targets = np.array(batch_targets).astype(np.float32)\n",
    "                    batch_queue.put((batch_graphs, batch_targets, batch_track_meta, batch_cluster_meta))\n",
    "                    \n",
    "                    batch_graphs = []\n",
    "                    batch_targets = []\n",
    "                    batch_track_meta = []\n",
    "                    batch_cluster_meta = []\n",
    "\n",
    "            file_num += self.num_procs\n",
    "                    \n",
    "        if len(batch_graphs) > 0:\n",
    "            batch_targets = np.array(batch_targets).astype(np.float32)\n",
    "            batch_queue.put((batch_graphs, batch_targets, batch_track_meta, batch_cluster_meta))\n",
    "                    \n",
    "\n",
    "    def worker(self, worker_id, batch_queue):\n",
    "        if self.preprocess:\n",
    "            self.preprocessed_worker(worker_id, batch_queue)\n",
    "        else:\n",
    "            raise Exception('Preprocessing is required for combined classification/regression models.')\n",
    "        \n",
    "    def check_procs(self):\n",
    "        for p in self.procs:\n",
    "            if p.is_alive(): return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def kill_procs(self):\n",
    "        for p in self.procs:\n",
    "            p.kill()\n",
    "\n",
    "        self.procs = []\n",
    "    \n",
    "    def generator(self):\n",
    "        \"\"\"\n",
    "        Generator that returns processed batches during training\n",
    "        \"\"\"\n",
    "        batch_queue = Queue(2 * self.num_procs)\n",
    "            \n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.worker, args=(i, batch_queue), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        while self.check_procs() or not batch_queue.empty():\n",
    "            try:\n",
    "                batch = batch_queue.get(True, 0.0001)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            yield batch\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the data generation step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pion_dir = '/usr/workspace/hip/ML4Jets/regression_images/graphs.v01-45-gaa27bcb/onetrack_multicluster/pion_files/'\n",
    "pion_files = np.sort(glob.glob(pion_dir+\"*.npy\"))\n",
    "n_files = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing and saving data to ./\n",
      "Processing file number 0\n",
      "Processing file number 1\n",
      "Processing file number 2\n",
      "Processing file number 3\n",
      "Processing file number 4\n",
      "Processing file number 5\n",
      "Processing file number 7\n",
      "Processing file number 8Processing file number 6\n",
      "\n",
      "Processing file number 9\n",
      "Finished processing 0 files\n",
      "Finished processing 1 files\n",
      "Finished processing 6 files\n",
      "Finished processing 4 files\n",
      "Finished processing 8 files\n",
      "Finished processing 2 files\n",
      "Finished processing 5 files\n",
      "Finished processing 7 files\n",
      "Finished processing 3 files\n",
      "Finished processing 9 files\n"
     ]
    }
   ],
   "source": [
    "data_gen = TrackGraphDataGenerator(pion_file_list=pion_files[:10], \n",
    "                                   batch_size=32,\n",
    "                                   n_clusters=1,\n",
    "                                   shuffle=False,\n",
    "                                   num_procs=32,\n",
    "                                   preprocess=True,\n",
    "                                   output_dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, target, track_meta_data, cluster_meta_data = next(data_gen.generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[1.875, 0.   , 0.   ],\n",
       "         [0.   , 1.909, 0.362]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[ 3.096,  0.   ,  0.   ],\n",
       "         [ 0.   ,  3.367, -0.714]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[ 2.594,  0.   ,  0.   ],\n",
       "         [ 0.   ,  2.437, -1.363]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[ 3.096,  0.   ,  0.   ],\n",
       "         [ 0.   ,  3.367, -0.714]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[2.248, 0.   , 0.   ],\n",
       "         [0.   , 2.148, 1.397]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[ 2.594,  0.   ,  0.   ],\n",
       "         [ 0.   ,  2.437, -1.363]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[ 3.096,  0.   ,  0.   ],\n",
       "         [ 0.   ,  3.367, -0.714]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[ 3.096,  0.   ,  0.   ],\n",
       "         [ 0.   ,  3.367, -0.714]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[2.631, 0.   , 0.   ],\n",
       "         [0.   , 2.615, 1.166]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[ 3.096,  0.   ,  0.   ],\n",
       "         [ 0.   ,  3.367, -0.714]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[ 2.594,  0.   ,  0.   ],\n",
       "         [ 0.   ,  2.437, -1.363]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[ 2.88 ,  0.   ,  0.   ],\n",
       "         [ 0.   ,  2.977, -1.343]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[ 2.88 ,  0.   ,  0.   ],\n",
       "         [ 0.   ,  2.977, -1.343]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[ 3.096,  0.   ,  0.   ],\n",
       "         [ 0.   ,  3.367, -0.714]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[2.752, 0.   , 0.   ],\n",
       "         [0.   , 2.973, 0.3  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[ 2.88 ,  0.   ,  0.   ],\n",
       "         [ 0.   ,  2.977, -1.343]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)},\n",
       " {'nodes': array([[0.1  , 0.   , 0.   ],\n",
       "         [0.   , 0.424, 1.6  ]], dtype=float32),\n",
       "  'globals': array([], shape=(1, 0), dtype=float32),\n",
       "  'senders': array([0, 1]),\n",
       "  'receivers': array([1, 0]),\n",
       "  'edges': array([], shape=(2, 0), dtype=float32)}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_thresh = 1.2\n",
    "clusterThresh = .5\n",
    "\n",
    "def get_cluster_inds(event_data, event_ind, n_clusters, dr_thresh):\n",
    "    if n_clusters==-1:   # get all nodes satisfying dR criterion\n",
    "        c_inds = range(event_data['nCluster'][event_ind])\n",
    "        c_inds = [c for c in c_inds if (event_data['dR'][event_ind][c]<dr_thresh) and \n",
    "                      (event_data['cluster_E'][event_ind][c]>clusterThresh)]\n",
    "    else:                # get n leading nodes satisfying dR criterion\n",
    "        c_inds = np.argsort(event_data['cluster_E'][event_ind])[::-1]\n",
    "        c_inds = [c for c in c_inds if (event_data['dR'][event_ind][c]<dr_thresh) and\n",
    "                      (event_data['cluster_E'][event_ind][c]>clusterThresh)]\n",
    "        c_inds = c_inds[:n_clusters]\n",
    "\n",
    "    return c_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5790 2 [0.01  0.315] [1.142e+03 1.055e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pion_files[0]\n",
    "event_data = np.load(file, allow_pickle=True).item()\n",
    "num_events = len(event_data[[key for key in event_data.keys()][0]])\n",
    "event_ind = np.random.randint(num_events)\n",
    "print(event_ind, event_data['nCluster'][event_ind], \n",
    "      event_data['dR'][event_ind], event_data['cluster_E'][event_ind])\n",
    "get_cluster_inds(event_data, event_ind, n_clusters=2, dr_thresh=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'event_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48875/1223134469.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mevent_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nCluster'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'event_data' is not defined"
     ]
    }
   ],
   "source": [
    "event_ind = np.random.randint(100)\n",
    "print(event_ind, event_data['nCluster'][event_ind], event_data['dR'][event_ind]<1.2)\n",
    "\n",
    "n_clusters = -1\n",
    "nodes = []\n",
    "if n_clusters==-1:\n",
    "    c_inds = range(event_data['nCluster'][event_ind])\n",
    "else:\n",
    "    c_inds = np.argsort(event_data['cluster_E'][event_ind])[::-1]\n",
    "    c_inds = c_inds[:n_clusters]\n",
    "    \n",
    "for c in c_inds:\n",
    "    if event_data['dR'][event_ind][c]>1.2:\n",
    "        continue\n",
    "    cluster_E = np.log10(event_data['cluster_E'][event_ind][c])\n",
    "    curr_node = [cluster_E, 0, 0]\n",
    "    nodes.append(curr_node)\n",
    "\n",
    "trackPt = np.log10(event_data['trackPt'][event_ind][0])\n",
    "nodes.append([0, trackPt, event_data['trackEta'][event_ind][0]])\n",
    "print(f'All nodes: {nodes}')\n",
    "\n",
    "n_clusters = 1\n",
    "nodes = []\n",
    "if n_clusters==-1:\n",
    "    c_inds = range(event_data['nCluster'][event_ind])\n",
    "else:\n",
    "    c_inds = np.argsort(event_data['cluster_E'][event_ind])[::-1]\n",
    "    c_inds = c_inds[:n_clusters]\n",
    "    \n",
    "for c in c_inds:\n",
    "    if event_data['dR'][event_ind][c]>1.2:\n",
    "        continue\n",
    "    cluster_E = np.log10(event_data['cluster_E'][event_ind][c])\n",
    "    curr_node = [cluster_E, 0, 0]\n",
    "    nodes.append(curr_node)\n",
    "\n",
    "trackPt = np.log10(event_data['trackPt'][event_ind][0])\n",
    "nodes.append([0, trackPt, event_data['trackEta'][event_ind][0]])\n",
    "print(f'Leading node: {nodes}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opence",
   "language": "python",
   "name": "opence"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
