{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modules.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import uproot as ur\n",
    "import time\n",
    "from multiprocessing import Process, Queue, set_start_method\n",
    "import compress_pickle as pickle\n",
    "from scipy.stats import circmean\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt # optional\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "})\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphDataGenerator\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../../data/onetrack_multicluster/pion_files/001.npy\"\n",
    "event_data = np.load(file, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(event_data['cluster_E'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.concatenate(event_data['cluster_E']).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.array(event_data['trackPt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data['cluster_E'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data['trackPt'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "bins = np.linspace(0,2500,30)\n",
    "# plt.hist(np.concatenate(event_data['truthPartE']).ravel(), color='black', bins=bins, histtype='step', label=\"truthPartE\");\n",
    "plt.hist(np.concatenate(event_data['cluster_E']).ravel(), bins=bins, histtype='step', label=\"cluster_E\");\n",
    "plt.hist(np.array(event_data['trackPt']), bins=bins, histtype='step', label='trackPt');\n",
    "plt.legend();\n",
    "plt.yscale('log');\n",
    "plt.xlabel('[MeV]');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = []\n",
    "for i in range(len(event_data['cluster_E'])):\n",
    "# for i in range(3):\n",
    "    for j in range(event_data['nCluster'][i]):\n",
    "        deltas.append(np.abs(event_data['trackPt'][i]-event_data['cluster_E'][i][j])/event_data['trackPt'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.hist(np.concatenate(deltas), bins=np.linspace(0,1, 30));\n",
    "plt.xlabel(\"abs(trackPt - clusterE) / trackPt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "bins = np.linspace(0,2500,30)\n",
    "# plt.hist(np.concatenate(event_data['truthPartE']).ravel(), color='black', bins=bins, histtype='step', label=\"truthPartE\");\n",
    "plt.hist(np.concatenate(event_data['cluster_E']).ravel() - np.concatenate(event_data['trackPt']).ravel(), bins=bins, histtype='step', label=\"cluster_E\");\n",
    "plt.legend();\n",
    "# plt.yscale('log');\n",
    "plt.xlabel('[MeV]');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "scales = {\n",
    "    'truthPartE_mean': 215.88905,\n",
    "    'truthPartE_std':  411.5805,\n",
    "    'cluster_cell_e_mean': 1.5849265,\n",
    "    'cluster_cell_e_std': 15.110136,\n",
    "    'cluster_e_mean': 92.877556,\n",
    "    'cluster_e_std': 228.13757,\n",
    "    'track_pt_mean': 235.44725,\n",
    "    'track_pt_std': 1182.93997489,\n",
    "    'track_z0_mean': 0.08022017,\n",
    "    'track_z0_std': 42.53320004,    \n",
    "    'track_eta_mean': -0.00563187,\n",
    "    'track_eta_std': 1.35242735,    \n",
    "    'track_phi_mean': 0.00206431,\n",
    "    'track_phi_std': 1.81240248,  \n",
    "         }\n",
    "\n",
    "node_means = [1.5849265, 3.8827391420197177, 0.0005979097, -2.8938382e-05, 1478.9285, 0.026611786, 0.068693615]\n",
    "node_stds =  [15.110136, 3.9588233603576204, 1.4709069, 1.813651, 434.60815, 0.03396141, 0.038586758]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'event_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5124/2269245710.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Plot preprocessed track inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrack_pt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trackPt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mscales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'track_pt_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mscales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'track_pt_std'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrack_z0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trackZ0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mscales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'track_z0_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mscales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'track_z0_std'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrack_eta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trackEta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mscales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'track_eta_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mscales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'track_eta_std'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrack_phi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trackPhi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mscales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'track_phi_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mscales\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'track_phi_std'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'event_data' is not defined"
     ]
    }
   ],
   "source": [
    "### Plot preprocessed track inputs \n",
    "track_pt = np.log10(np.abs(np.concatenate(event_data['trackPt']) - scales['track_pt_mean'])/scales['track_pt_std'])\n",
    "track_z0 = (np.concatenate(event_data['trackZ0']) - scales['track_z0_mean'])/scales['track_z0_std']\n",
    "track_eta = (np.concatenate(event_data['trackEta']) - scales['track_eta_mean'])/scales['track_eta_std']\n",
    "track_phi = (np.concatenate(event_data['trackPhi']) - scales['track_phi_mean'])/scales['track_phi_std']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(12,3), dpi=200, tight_layout=True)\n",
    "fig.suptitle(\"Scaled Track Inputs\")\n",
    "\n",
    "ax[0].hist(track_pt, bins=30)\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_xlabel(r'Scaled Track $p_T$')\n",
    "\n",
    "ax[1].hist(track_z0, bins=30)\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_xlabel(r'Scaled Track $Z_0$')\n",
    "\n",
    "ax[2].hist(track_eta, bins=30)\n",
    "ax[2].set_yscale('log')\n",
    "ax[2].set_xlabel(r'Scaled Track $\\eta$')\n",
    "\n",
    "ax[3].hist(track_phi, bins=30)\n",
    "ax[3].set_yscale('log')\n",
    "ax[3].set_xlabel(r'Scaled Track $\\phi$');\n",
    "plt.savefig(\"scaled_track_inputs.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = []\n",
    "\n",
    "n_events = len(event_data['index'])\n",
    "for event_ind in range(n_events):\n",
    "    for cluster_ind in range(event_data['nCluster'][event_ind]):\n",
    "        nodes = event_data['cluster_cell_E'][event_ind][cluster_ind]\n",
    "        cell_IDs = event_data['cluster_cell_ID'][event_ind][cluster_ind]\n",
    "        cell_IDmap = self.sorter[np.searchsorted(self.cellGeo_ID, cell_IDs, sorter=self.sorter)]\n",
    "        nodes = np.append(nodes, cell_geo_data['cell_geo_sampling'][0][cell_IDmap])\n",
    "        nodes = np.reshape(nodes, (1, -1)).T\n",
    "        all_nodes.append(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = event_data['cluster_cell_E'][event_ind][cluster_ind]\n",
    "# nodes = np.append(nodes, self.cellGeo_data['cell_geo_sampling'][0][cell_IDmap])\n",
    "# for f in self.nodeFeatureNames[2:4]:\n",
    "#     nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "# nodes = np.append(nodes, self.cellGeo_data['cell_geo_rPerp'][0][cell_IDmap])\n",
    "# for f in self.nodeFeatureNames[5:]:\n",
    "#     nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "\n",
    "nodes = np.reshape(nodes, (1, -1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot preprocessed energy inputs \n",
    "cluster_cell_e = np.log10(np.abs(np.concatenate(all_nodes) - scales['cluster_cell_e_mean'])/scales['cluster_cell_e_std'])\n",
    "cluster_e = np.log10(np.abs(np.concatenate(event_data['cluster_E']) - scales['cluster_e_mean'])/scales['cluster_e_std'])\n",
    "truth_particle_e = np.log10(np.abs(np.concatenate(event_data['truthPartE']) - scales['truthPartE_mean'])/scales['truthPartE_std'])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(8,3), dpi=200, tight_layout=True)\n",
    "fig.suptitle(\"Scaled Energy Inputs\")\n",
    "\n",
    "ax[0].hist(truth_particle_e, bins=30)\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_xlabel(r'Truth Particle $E$')\n",
    "\n",
    "ax[1].hist(cluster_e, bins=30)\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_xlabel(r'Cluster $E$')\n",
    "\n",
    "ax[2].hist(cluster_cell_e, bins=30)\n",
    "ax[2].set_yscale('log')\n",
    "ax[2].set_xlabel(r'Cluster Cell $E$');\n",
    "plt.savefig(\"scaled_energy_inputs.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class GraphDataGenerator:\n",
    "    \"\"\"\n",
    "    DataGenerator class for extracting and formating data from list of root files\n",
    "    This data generator uses the cell_geo file to create the input graph structure\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 pi0_file_list: list,\n",
    "                 pion_file_list: list,\n",
    "                 cellGeo_file: str,\n",
    "                 batch_size: int,\n",
    "                 shuffle: bool = True,\n",
    "                 num_procs = 32,\n",
    "                 preprocess = False,\n",
    "                 output_dir = None):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            self.pi0_file_list = pi0_file_list\n",
    "            self.pion_file_list = pion_file_list\n",
    "#             assert len(pi0_file_list) == len(pion_file_list)\n",
    "            self.num_files = len(self.pion_file_list)\n",
    "        else:\n",
    "            self.file_list = pion_file_list\n",
    "            self.num_files = len(self.file_list)\n",
    "        \n",
    "        self.cellGeo_file = cellGeo_file\n",
    "        \n",
    "        self.cellGeo_data = ur.open(self.cellGeo_file)['CellGeo']\n",
    "        self.geoFeatureNames = self.cellGeo_data.keys()[1:9]\n",
    "        self.nodeFeatureNames = ['cluster_cell_E']\n",
    "#         self.nodeFeatureNames = ['cluster_cell_E', *self.geoFeatureNames[:-2]]\n",
    "        self.edgeFeatureNames = self.cellGeo_data.keys()[9:]\n",
    "        self.num_nodeFeatures = len(self.nodeFeatureNames)\n",
    "        self.num_edgeFeatures = len(self.edgeFeatureNames)\n",
    "        self.cellGeo_data = self.cellGeo_data.arrays(library='np')\n",
    "        self.cellGeo_ID = self.cellGeo_data['cell_geo_ID'][0]\n",
    "        self.sorter = np.argsort(self.cellGeo_ID)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        if self.shuffle: np.random.shuffle(self.file_list)\n",
    "        \n",
    "        self.num_procs = np.min([num_procs, self.num_files])\n",
    "        self.procs = []\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            os.makedirs(self.output_dir, exist_ok=True)\n",
    "            self.preprocess_data()\n",
    "    \n",
    "    def get_cluster_calib(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading cluster calibration energy \"\"\" \n",
    "        cluster_calib_E = event_data['cluster_ENG_CALIB_TOT'][event_ind][cluster_ind]\n",
    "\n",
    "        if cluster_calib_E <= 0:\n",
    "            return None\n",
    "\n",
    "        return cluster_calib_E\n",
    "    \n",
    "    def get_cluster_eta(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading cluster eta \"\"\" \n",
    "        cluster_eta = event_data['cluster_Eta'][event_ind][cluster_ind]\n",
    "        return cluster_eta\n",
    "    \n",
    "    def get_nodes(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading Node features \"\"\" \n",
    "\n",
    "        cell_IDs = event_data['cluster_cell_ID'][event_ind][cluster_ind]\n",
    "        cell_IDmap = self.sorter[np.searchsorted(self.cellGeo_ID, cell_IDs, sorter=self.sorter)]\n",
    "        \n",
    "        nodes = event_data['cluster_cell_E'][event_ind][cluster_ind]\n",
    "        global_node = event_data['cluster_E'][event_ind][cluster_ind]\n",
    "#         nodes = np.append(nodes, self.cellGeo_data['cell_geo_sampling'][0][cell_IDmap])\n",
    "#         for f in self.nodeFeatureNames[2:4]:\n",
    "#             nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "#         nodes = np.append(nodes, self.cellGeo_data['cell_geo_rPerp'][0][cell_IDmap])\n",
    "#         for f in self.nodeFeatureNames[5:]:\n",
    "#             nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "\n",
    "        nodes = np.reshape(nodes, (len(self.nodeFeatureNames), -1)).T\n",
    "        nodes_scaled = np.log10(np.abs(np.concatenate(nodes) - scales['cluster_cell_e_mean'])/scales['cluster_cell_e_std'])\n",
    "        nodes_scaled = np.reshape(nodes, (len(self.nodeFeatureNames), -1)).T\n",
    "        \n",
    "#         nodes_scaled = (nodes - node_means)/node_stds\n",
    "        cluster_num_nodes = len(nodes)\n",
    "\n",
    "#         # add dummy placeholder nodes for track features (not used in cluster cell nodes)\n",
    "#         nodes = np.hstack((nodes, np.zeros((cluster_num_nodes, 4))))\n",
    "        \n",
    "        return nodes_scaled, np.array([global_node]), cluster_num_nodes, cell_IDmap\n",
    "    \n",
    "#     # track section ----------------------------------------------------------------\n",
    "    \n",
    "#     def get_track_node(self, event_data, event_index, track_index):\n",
    "#         \"\"\"\n",
    "#         Creates node features for tracks\n",
    "#         Inputs:\n",
    "        \n",
    "#         Returns:\n",
    "#             1 Dimensional array of node features for a single node\n",
    "#                 NOTE the cluster get_node function is a 2D array of multiple nodes\n",
    "#                 This function is used in a for loop so the end result is a 2D array\n",
    "#         \"\"\"\n",
    "#         node_features = np.array(event_data[\"trackPt\"][event_index][track_index])\n",
    "#         node_features = np.append(node_features, event_data[\"trackZ0\"][event_index][track_index])\n",
    "#         node_features = np.append(node_features, event_data[\"trackEta_EMB2\"][event_index][track_index])\n",
    "#         node_features = np.append(node_features, event_data[\"trackPhi_EMB2\"][event_index][track_index])\n",
    "#         node_features = np.reshape(node_features, (len(node_features))).T\n",
    "        \n",
    "#         # add dummy placeholder nodes for track features (not used in track cell nodes)\n",
    "#         node_features = np.hstack((np.zeros(7), node_features))\n",
    "    \n",
    "#         return node_features\n",
    "    \n",
    "#     def get_track_edges(self, num_track_nodes, start_index):\n",
    "#         \"\"\"\n",
    "#         Creates the edge senders and recievers and edge features\n",
    "#         Inputs:\n",
    "#         (int) num_track_nodes: number of track nodes\n",
    "#         (int) start_index: the index of senders/recievers to start with. We should start with num_cluster_edges+1 to avoid overlap\n",
    "\n",
    "#         Returns:\n",
    "#         (np.array) edge_features:\n",
    "#         (np.array) senders:\n",
    "#         (np.array) recievers:\n",
    "#         \"\"\"\n",
    "#         # Full Connected tracks\n",
    "#         # since we are fully connected, the order of senders and recievers doesn't matter\n",
    "#         # we just need to count each node - edges will have a placeholder feature\n",
    "#         connections = list(itertools.permutations(range(start_index, start_index + num_track_nodes),2))\n",
    "#         for i in range(5):\n",
    "#             connections.append((i, i))\n",
    "\n",
    "#         senders = np.array([x[0] for x in connections])\n",
    "#         recievers = np.array([x[0] for x in connections])\n",
    "#         edge_features = np.zeros((len(connections), 10))\n",
    "\n",
    "#         return senders, recievers, edge_features\n",
    "    \n",
    "#     # end track section ----------------------------------------------------------------\n",
    "    \n",
    "    def get_edges(self, cluster_num_nodes, cell_IDmap):\n",
    "        \"\"\" \n",
    "        Reading edge features \n",
    "        Returns senders, receivers, and edges    \n",
    "        \"\"\" \n",
    "        \n",
    "        edge_inds = np.zeros((cluster_num_nodes, self.num_edgeFeatures))\n",
    "        for i, f in enumerate(self.edgeFeatureNames):\n",
    "            edge_inds[:, i] = self.cellGeo_data[f][0][cell_IDmap]\n",
    "        edge_inds[np.logical_not(np.isin(edge_inds, cell_IDmap))] = np.nan\n",
    "        \n",
    "        senders, edge_on_inds = np.isin(edge_inds, cell_IDmap).nonzero()\n",
    "        cluster_num_edges = len(senders)\n",
    "        edges = np.zeros((cluster_num_edges, self.num_edgeFeatures))\n",
    "        edges[np.arange(cluster_num_edges), edge_on_inds] = 1\n",
    "        \n",
    "        cell_IDmap_sorter = np.argsort(cell_IDmap)\n",
    "        rank = np.searchsorted(cell_IDmap, edge_inds , sorter=cell_IDmap_sorter)\n",
    "        receivers = cell_IDmap_sorter[rank[rank!=cluster_num_nodes]]\n",
    "        \n",
    "        return senders, receivers, edges\n",
    "\n",
    "    def preprocessor(self, worker_id):\n",
    "        \"\"\"\n",
    "        Prerocessing root file data for faster data \n",
    "        generation during multiple training epochs\n",
    "        \"\"\"\n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            print(f\"Processing file number {file_num}\")\n",
    "            \n",
    "            ### Pions\n",
    "            file = self.pion_file_list[file_num]\n",
    "            event_data = np.load(file, allow_pickle=True).item()\n",
    "            num_events = len(event_data[[key for key in event_data.keys()][0]])\n",
    "\n",
    "            preprocessed_data = []\n",
    "\n",
    "            for event_ind in range(num_events):\n",
    "                num_clusters = event_data['nCluster'][event_ind]\n",
    "                truth_particle_E = event_data['truthPartE'][event_ind][0] # first one is the pion! \n",
    "                truth_particle_E_scaled = np.log10(np.abs(truth_particle_E - scales['truthPartE_mean'])/scales['truthPartE_std'])\n",
    "                truthPartPt = event_data['truthPartPt'][event_ind][0]\n",
    "                \n",
    "                for i in range(num_clusters):\n",
    "                    if event_data['dR_pass'][event_ind][i] == False:\n",
    "                        continue\n",
    "                    cluster_calib_E = self.get_cluster_calib(event_data, event_ind, i)\n",
    "                    cluster_EM_prob = event_data['cluster_EM_PROBABILITY'][event_ind][i]\n",
    "                    cluster_E = event_data['cluster_E'][event_ind][i]\n",
    "                    cluster_E_scaled = np.log10(np.abs(cluster_E - scales['cluster_e_mean'])/scales['cluster_e_std'])\n",
    "                    cluster_HAD_WEIGHT = event_data['cluster_HAD_WEIGHT'][event_ind][i]\n",
    "                    \n",
    "                    if cluster_calib_E is None:\n",
    "                        continue\n",
    "                        \n",
    "                    cluster_eta = self.get_cluster_eta(event_data, event_ind, i)\n",
    "                        \n",
    "                    nodes, global_node, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, i)\n",
    "                    senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
    "                    \n",
    "#                 # track section ----------------------------------------------------------------\n",
    "#                     track_nodes = np.empty((0, 11))\n",
    "                    num_tracks = event_data['nTrack'][event_ind]\n",
    "                    for track_index in range(num_tracks):\n",
    "#                         np.append(track_nodes, self.get_track_node(event_data, event_ind, track_index).reshape(1, -1), axis=0)\n",
    "                        track_pt = event_data[\"trackPt\"][event_ind][track_index]\n",
    "                        track_z0 = event_data[\"trackZ0\"][event_ind][track_index]\n",
    "                        track_eta = event_data[\"trackEta\"][event_ind][track_index]\n",
    "                        track_phi = event_data[\"trackPhi\"][event_ind][track_index]\n",
    "                \n",
    "                    track_pt_scaled = np.log10(np.abs(track_pt - scales['track_pt_mean'])/scales['track_pt_std'])\n",
    "                    track_z0_scaled = (track_z0 - scales['track_z0_mean'])/scales['track_z0_std']\n",
    "                    track_eta_scaled = (track_eta - scales['track_eta_mean'])/scales['track_eta_std']\n",
    "                    track_phi_scaled = (track_phi - scales['track_phi_mean'])/scales['track_phi_std']\n",
    "#                     track_senders, track_receivers, track_edge_features = self.get_track_edges(len(track_nodes), cluster_num_nodes)\n",
    "                    \n",
    "#                     # append on the track nodes and edges to the cluster ones\n",
    "#                     nodes = np.append(nodes, np.array(track_nodes), axis=0)\n",
    "#                     edges = np.append(edges, track_edge_features, axis=0)\n",
    "#                     senders = np.append(senders, track_senders, axis=0)\n",
    "#                     receivers = np.append(receivers, track_receivers, axis=0)\n",
    "                    \n",
    "#                     # end track section ----------------------------------------------------------------\n",
    "                \n",
    "                    globals_list = np.array([\n",
    "                                             cluster_E_scaled.astype(np.float32), \n",
    "                                             track_pt_scaled.astype(np.float32), \n",
    "                                             track_z0_scaled.astype(np.float32),\n",
    "                                             track_eta_scaled.astype(np.float32),\n",
    "                                             track_phi_scaled.astype(np.float32),\n",
    "                                             ])\n",
    "                    \n",
    "                    graph = {'nodes': nodes.astype(np.float32), \n",
    "#                              'globals': global_node.astype(np.float32),\n",
    "                             'globals': globals_list,                             \n",
    "                        'senders': senders.astype(np.int32), 'receivers': receivers.astype(np.int32),\n",
    "                        'edges': edges.astype(np.float32), 'cluster_calib_E': cluster_calib_E.astype(np.float32), \n",
    "                        'cluster_eta': cluster_eta.astype(np.float32), 'cluster_EM_prob': cluster_EM_prob.astype(np.float32),\n",
    "                        'cluster_E': cluster_E.astype(np.float32), 'cluster_HAD_WEIGHT': cluster_HAD_WEIGHT.astype(np.float32), \n",
    "                        'truthPartPt': truthPartPt.astype(np.float32), 'track_pt': track_pt.astype(np.float32), \n",
    "                             'track_eta': track_eta.astype(np.float32)}\n",
    "                    target = np.reshape([truth_particle_E_scaled.astype(np.float32), 1], [1,2])\n",
    "                    preprocessed_data.append((graph, target))\n",
    "\n",
    "#             ### Pi0\n",
    "#             file = self.pi0_file_list[file_num]\n",
    "#             event_data = np.load(file, allow_pickle=True).item()\n",
    "#             num_events = len(event_data[[key for key in event_data.keys()][0]])\n",
    "\n",
    "#             for event_ind in range(num_events):\n",
    "#                 num_clusters = event_data['nCluster'][event_ind]\n",
    "#                 truth_particle_E = np.log10(event_data['truthPartE'][event_ind][0]) # first one is the pion! \n",
    "#                 truthPartPt = event_data['truthPartPt'][event_ind][0]\n",
    "                \n",
    "#                 for i in range(num_clusters):\n",
    "#                     cluster_calib_E = self.get_cluster_calib(event_data, event_ind, i)\n",
    "#                     cluster_EM_prob = event_data['cluster_EM_PROBABILITY'][event_ind][i]\n",
    "#                     cluster_E = np.log10(event_data['cluster_E'][event_ind][i])\n",
    "#                     cluster_HAD_WEIGHT = event_data['cluster_HAD_WEIGHT'][event_ind][i]\n",
    "\n",
    "#                     if event_data['dR_pass'][event_ind][i] == False:\n",
    "#                         continue\n",
    "                        \n",
    "#                     if cluster_calib_E is None:\n",
    "#                         continue\n",
    "                        \n",
    "#                     cluster_eta = self.get_cluster_eta(event_data, event_ind, i)\n",
    "                        \n",
    "#                     nodes, global_node, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, i)\n",
    "#                     senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)         \n",
    "                    \n",
    "#                     # track section ----------------------------------------------------------------\n",
    "#                     track_nodes = np.empty((0, 11))\n",
    "#                     num_tracks = event_data['nTrack'][event_ind]\n",
    "#                     for track_index in range(num_tracks):\n",
    "#                         np.append(track_nodes, self.get_track_node(event_data, event_ind, track_index).reshape(1, -1), axis=0)\n",
    "#                         track_pt = np.array([np.log10(event_data[\"trackPt\"][event_ind][track_index])])\n",
    "                        \n",
    "#                     track_senders, track_receivers, track_edge_features = self.get_track_edges(len(track_nodes), cluster_num_nodes)\n",
    "                    \n",
    "#                     nodes = np.append(nodes, np.array(track_nodes), axis=0)\n",
    "#                     edges = np.append(edges, track_edge_features, axis=0)\n",
    "#                     senders = np.append(senders, track_senders, axis=0)\n",
    "#                     receivers = np.append(receivers, track_receivers, axis=0)\n",
    "                    \n",
    "#                     # end track section ----------------------------------------------------------------\n",
    "                                        \n",
    "#                     graph = {'nodes': nodes.astype(np.float32), \n",
    "# #                              'globals': global_node.astype(np.float32),\n",
    "#                              'globals': track_pt.astype(np.float32),                             \n",
    "#                         'senders': senders.astype(np.int32), 'receivers': receivers.astype(np.int32),\n",
    "#                         'edges': edges.astype(np.float32), 'cluster_calib_E': cluster_calib_E.astype(np.float32), \n",
    "#                         'cluster_eta': cluster_eta.astype(np.float32), 'cluster_EM_prob': cluster_EM_prob.astype(np.float32),\n",
    "#                         'cluster_E': cluster_E.astype(np.float32), 'cluster_HAD_WEIGHT': cluster_HAD_WEIGHT.astype(np.float32),\n",
    "#                         'truthPartPt': truthPartPt.astype(np.float32), 'track_pt': track_pt.astype(np.float32)}\n",
    "#                     target = np.reshape([truth_particle_E.astype(np.float32), 0], [1,2])\n",
    "    \n",
    "#                     preprocessed_data.append((graph, target))\n",
    "\n",
    "            random.shuffle(preprocessed_data)\n",
    "\n",
    "            pickle.dump(preprocessed_data, open(self.output_dir + f'data_{file_num:03d}.p', 'wb'), compression='gzip')\n",
    "            \n",
    "            print(f\"Finished processing {file_num} files\")\n",
    "            file_num += self.num_procs\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        print('\\nPreprocessing and saving data to {}'.format(self.output_dir))\n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.preprocessor, args=(i,), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()\n",
    "\n",
    "        self.file_list = [self.output_dir + f'data_{i:03d}.p' for i in range(self.num_files)]\n",
    "\n",
    "    def preprocessed_worker(self, worker_id, batch_queue):\n",
    "        batch_graphs = []\n",
    "        batch_targets = []\n",
    "        \n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
    "\n",
    "            for i in range(len(file_data)):\n",
    "                batch_graphs.append(file_data[i][0])\n",
    "                batch_targets.append(file_data[i][1])\n",
    "                    \n",
    "                if len(batch_graphs) == self.batch_size:\n",
    "                    batch_targets = np.reshape(np.array(batch_targets), [-1,2]).astype(np.float32)\n",
    "                    \n",
    "                    batch_queue.put((batch_graphs, batch_targets))\n",
    "                    \n",
    "                    batch_graphs = []\n",
    "                    batch_targets = []\n",
    "\n",
    "            file_num += self.num_procs\n",
    "                    \n",
    "        if len(batch_graphs) > 0:\n",
    "            batch_targets = np.reshape(np.array(batch_targets), [-1,2]).astype(np.float32)\n",
    "            batch_queue.put((batch_graphs, batch_targets))\n",
    "\n",
    "    def worker(self, worker_id, batch_queue):\n",
    "        if self.preprocess:\n",
    "            self.preprocessed_worker(worker_id, batch_queue)\n",
    "        else:\n",
    "            raise Exception('Preprocessing is required for combined classification/regression models.')\n",
    "        \n",
    "    def check_procs(self):\n",
    "        for p in self.procs:\n",
    "            if p.is_alive(): return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def kill_procs(self):\n",
    "        for p in self.procs:\n",
    "            p.kill()\n",
    "\n",
    "        self.procs = []\n",
    "    \n",
    "    def generator(self):\n",
    "        \"\"\"\n",
    "        Generator that returns processed batches during training\n",
    "        \"\"\"\n",
    "        batch_queue = Queue(2 * self.num_procs)\n",
    "            \n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.worker, args=(i, batch_queue), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        while self.check_procs() or not batch_queue.empty():\n",
    "            try:\n",
    "                batch = batch_queue.get(True, 0.0001)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            yield batch\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the data generation step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/clusterfs/ml4hep/mpettee/ml4pions/data/onetrack_multicluster/'\n",
    "pi0_files = []#np.sort(glob.glob(data_dir+'pi0_files/*.npy'))[:1]\n",
    "pion_files = np.sort(glob.glob(data_dir+'pion_files/*.npy'))[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing and saving data to /clusterfs/ml4hep/mpettee/ml4pions/data/onetrack_multicluster/\n",
      "Processing file number 0\n",
      "Finished processing 0 files\n"
     ]
    }
   ],
   "source": [
    "data_gen = GraphDataGenerator(pion_file_list=pion_files, \n",
    "                              pi0_file_list=pi0_files,\n",
    "                              cellGeo_file='/clusterfs/ml4hep/mpettee/ml4pions/data/cell_geo.root',\n",
    "                              batch_size=32,\n",
    "                              shuffle=False,\n",
    "                              num_procs=32,\n",
    "                              preprocess=True,\n",
    "                              output_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "nbdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
