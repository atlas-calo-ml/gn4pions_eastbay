{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modules.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import uproot as ur\n",
    "import time\n",
    "from multiprocessing import Process, Queue, set_start_method\n",
    "import compress_pickle as pickle\n",
    "from scipy.stats import circmean\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphDataGenerator\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/clusterfs/ml4hep/mpettee/ml4pions/data/'\n",
    "out_dir = '/clusterfs/ml4hep/mpettee/ml4pions/data/tracks_withcuts/'\n",
    "pi0_files = np.sort(glob.glob(data_dir+'*pi0*/*.root'))[10:12]\n",
    "pion_files = np.sort(glob.glob(data_dir+'*pion*/*.root'))[10:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_geo_df = ur.open(data_dir+\"cell_geo.root\", library=\"pd\")[\"CellGeo\"].arrays(library=\"pd\")\n",
    "event_data = ur.open(pion_files[0])[\"EventTree\"].arrays(library=\"np\")\n",
    "# test_pi0_df = pd.DataFrame(test_pi0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GraphDataGenerator:\n",
    "    \"\"\"\n",
    "    DataGenerator class for extracting and formating data from list of root files\n",
    "    This data generator uses the cell_geo file to create the input graph structure\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 pi0_file_list: list,\n",
    "                 pion_file_list: list,\n",
    "                 cellGeo_file: str,\n",
    "                 batch_size: int,\n",
    "                 shuffle: bool = True,\n",
    "                 num_procs = 32,\n",
    "                 preprocess = False,\n",
    "                 output_dir = None):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            self.pi0_file_list = pi0_file_list\n",
    "            self.pion_file_list = pion_file_list\n",
    "            assert len(pi0_file_list) == len(pion_file_list)\n",
    "            self.num_files = len(self.pi0_file_list)\n",
    "        else:\n",
    "            self.file_list = pi0_file_list\n",
    "            self.num_files = len(self.file_list)\n",
    "        \n",
    "        self.cellGeo_file = cellGeo_file\n",
    "        \n",
    "        self.cellGeo_data = ur.open(self.cellGeo_file)['CellGeo']\n",
    "        self.geoFeatureNames = self.cellGeo_data.keys()[1:9]\n",
    "        self.nodeFeatureNames = ['cluster_cell_E', *self.geoFeatureNames[:-2]]\n",
    "        self.edgeFeatureNames = self.cellGeo_data.keys()[9:]\n",
    "        self.num_nodeFeatures = len(self.nodeFeatureNames)\n",
    "        self.num_edgeFeatures = len(self.edgeFeatureNames)\n",
    "        self.cellGeo_data = self.cellGeo_data.arrays(library='np')\n",
    "        self.cellGeo_ID = self.cellGeo_data['cell_geo_ID'][0]\n",
    "        self.sorter = np.argsort(self.cellGeo_ID)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        if self.shuffle: np.random.shuffle(self.file_list)\n",
    "        \n",
    "        self.num_procs = np.min([num_procs, self.num_files])\n",
    "        self.procs = []\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            os.makedirs(self.output_dir, exist_ok=True)\n",
    "            self.preprocess_data()\n",
    "            \n",
    "#         print(self.nodeFeatureNames)\n",
    "#         print(self.edgeFeatureNames)\n",
    "\n",
    "    def get_cluster_calib(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading cluster calibration energy \"\"\" \n",
    "            \n",
    "        cluster_calib_E = event_data['cluster_ENG_CALIB_TOT'][event_ind][cluster_ind]\n",
    "\n",
    "        if cluster_calib_E <= 0:\n",
    "            return None\n",
    "\n",
    "        return np.log10(cluster_calib_E)\n",
    "            \n",
    "    def get_nodes(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading Node features \"\"\" \n",
    "\n",
    "        cell_IDs = event_data['cluster_cell_ID'][event_ind][cluster_ind]\n",
    "        cell_IDmap = self.sorter[np.searchsorted(self.cellGeo_ID, cell_IDs, sorter=self.sorter)]\n",
    "        \n",
    "        nodes = np.log10(event_data['cluster_cell_E'][event_ind][cluster_ind])\n",
    "        global_node = np.log10(event_data['cluster_E'][event_ind][cluster_ind])\n",
    "        \n",
    "        # Scaling the cell_geo_sampling by 28\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_sampling'][0][cell_IDmap]/28.)\n",
    "        for f in self.nodeFeatureNames[2:4]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "        # Scaling the cell_geo_rPerp by 3000\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_rPerp'][0][cell_IDmap]/3000.)\n",
    "        for f in self.nodeFeatureNames[5:]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "\n",
    "        nodes = np.reshape(nodes, (len(self.nodeFeatureNames), -1)).T\n",
    "        cluster_num_nodes = len(nodes)\n",
    "    \n",
    "        # add dummy placeholder nodes for track features (not used in cluster cell nodes)\n",
    "        nodes = np.hstack((nodes, np.zeros((cluster_num_nodes, 4))))\n",
    "        \n",
    "#         print(\"nodes\", nodes)\n",
    "#         print(\"global_node\", np.array([global_node]))\n",
    "#         print(\"cluster_num_nodes\", cluster_num_nodes)\n",
    "#         print(cell_IDmap)\n",
    "#         raise Exception(\"asdfasdfasdf\")\n",
    "        return nodes, np.array([global_node]), cluster_num_nodes, cell_IDmap\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # WIP ----------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_track_node(self, event_data, event_index, track_index):\n",
    "        \"\"\"\n",
    "        Creates node features for tracks\n",
    "        Inputs:\n",
    "        \n",
    "        Returns:\n",
    "            1 Dimensional array of node features for a single node\n",
    "                NOTE the cluster get_node function is a 2D array of multiple nodes\n",
    "                This function is used in a for loop so the end result is a 2D array\n",
    "        \"\"\"\n",
    "        node_features = np.array(event_data[\"trackPt\"][event_index][track_index])\n",
    "        node_features = np.append(node_features, event_data[\"trackZ0\"][event_index][track_index])\n",
    "        node_features = np.append(node_features, event_data[\"trackEta_EMB2\"][event_index][track_index])\n",
    "        node_features = np.append(node_features, event_data[\"trackPhi_EMB2\"][event_index][track_index])\n",
    "        node_features = np.reshape(node_features, (len(node_features))).T\n",
    "        \n",
    "#         print(\"node_features before\", node_features)\n",
    "        # add dummy placeholder nodes for track features (not used in track cell nodes)\n",
    "        node_features = np.hstack((np.zeros(7), node_features))\n",
    "        \n",
    "#         print(node_features)\n",
    "#         raise Exception(\"asdfasdfasdf\")\n",
    "        return node_features\n",
    "    \n",
    "    def get_track_edges(self, num_track_nodes, start_index):\n",
    "        \"\"\"\n",
    "        Creates the edge senders and recievers and edge features\n",
    "        Inputs:\n",
    "        (int) num_track_nodes: number of track nodes\n",
    "        (int) start_index: the index of senders/recievers to start with. We should start with num_cluster_edges+1 to avoid overlap\n",
    "\n",
    "        Returns:\n",
    "        (np.array) edge_features:\n",
    "        (np.array) senders:\n",
    "        (np.array) recievers:\n",
    "        \"\"\"\n",
    "        # Full Connected tracks\n",
    "        # since we are fully connected, the order of senders and recievers doesn't matter\n",
    "        # we just need to count each node - edges will have a placeholder feature\n",
    "        connections = list(itertools.permutations(range(start_index, start_index + num_track_nodes),2))\n",
    "        for i in range(5):\n",
    "            connections.append((i, i))\n",
    "\n",
    "        senders = np.array([x[0] for x in connections])\n",
    "        recievers = np.array([x[0] for x in connections])\n",
    "        edge_features = np.zeros((len(connections), 10))\n",
    "\n",
    "        return senders, recievers, edge_features\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # end WIP ----------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_edges(self, cluster_num_nodes, cell_IDmap):\n",
    "        \"\"\" \n",
    "        Reading edge features \n",
    "        Returns senders, receivers, and edges    \n",
    "        \"\"\" \n",
    "        \n",
    "        edge_inds = np.zeros((cluster_num_nodes, self.num_edgeFeatures))\n",
    "        for i, f in enumerate(self.edgeFeatureNames):\n",
    "            edge_inds[:, i] = self.cellGeo_data[f][0][cell_IDmap]\n",
    "        edge_inds[np.logical_not(np.isin(edge_inds, cell_IDmap))] = np.nan\n",
    "        \n",
    "        senders, edge_on_inds = np.isin(edge_inds, cell_IDmap).nonzero()\n",
    "        cluster_num_edges = len(senders)\n",
    "        edges = np.zeros((cluster_num_edges, self.num_edgeFeatures))\n",
    "        edges[np.arange(cluster_num_edges), edge_on_inds] = 1\n",
    "        \n",
    "        cell_IDmap_sorter = np.argsort(cell_IDmap)\n",
    "        rank = np.searchsorted(cell_IDmap, edge_inds , sorter=cell_IDmap_sorter)\n",
    "        receivers = cell_IDmap_sorter[rank[rank!=cluster_num_nodes]]\n",
    "        \n",
    "        return senders, receivers, edges\n",
    "\n",
    "    def preprocessor(self, worker_id):\n",
    "        \"\"\"\n",
    "        Prerocessing root file data for faster data \n",
    "        generation during multiple training epochs\n",
    "        \"\"\"\n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            print(f\"Processing file number {file_num}\")\n",
    "            file = self.pion_file_list[file_num]\n",
    "            event_data = np.load(file, allow_pickle=True).item()\n",
    "            num_events = len(event_data[[key for key in event_data.keys()][0]])\n",
    "\n",
    "            preprocessed_data = []\n",
    "\n",
    "            for event_ind in range(num_events):\n",
    "                num_clusters = event_data['nCluster'][event_ind]\n",
    "                \n",
    "                for i in range(num_clusters):\n",
    "                    cluster_calib_E = self.get_cluster_calib(event_data, event_ind, i)\n",
    "                    \n",
    "                    if cluster_calib_E is None:\n",
    "                        continue\n",
    "                        \n",
    "                    nodes, global_node, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, i)\n",
    "                    senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    # WIP add track nodes and edges ----------------------------------------------------------------\n",
    "                    track_nodes = np.empty((0, 11))\n",
    "                    num_tracks = event_data['nTrack'][event_ind]\n",
    "                    for track_index in range(num_tracks):\n",
    "#                         print(\"get func reshaped\", self.get_track_node(event_data, event_ind, track_index).reshape(1, -1))\n",
    "#                         print(\"track_nodes\", track_nodes)\n",
    "#                         raise Exception(\"stop\")\n",
    "                        np.append(track_nodes, self.get_track_node(event_data, event_ind, track_index).reshape(1, -1), axis=0)\n",
    "                    \n",
    "                    track_senders, track_receivers, track_edge_features = self.get_track_edges(len(track_nodes), cluster_num_nodes)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "#                     print(\"old nodes\", nodes)\n",
    "#                     print(\"track nodes\", track_nodes)\n",
    "                    \n",
    "                    \n",
    "                    # append on the track nodes and edges to the cluster ones\n",
    "                \n",
    "                    nodes = np.append(nodes, np.array(track_nodes), axis=0)\n",
    "                    edges = np.append(edges, track_edge_features, axis=0)\n",
    "                    senders = np.append(senders, track_senders, axis=0)\n",
    "                    receivers = np.append(receivers, track_receivers, axis=0)\n",
    "                    \n",
    "                    # end WIP ----------------------------------------------------------------\n",
    "                \n",
    "                    graph = {'nodes': nodes.astype(np.float32), 'globals': global_node.astype(np.float32),\n",
    "                        'senders': senders.astype(np.int32), 'receivers': receivers.astype(np.int32),\n",
    "                        'edges': edges.astype(np.float32)}\n",
    "                    target = np.reshape([cluster_calib_E.astype(np.float32), 1], [1,2])\n",
    "\n",
    "                    preprocessed_data.append((graph, target))\n",
    "\n",
    "            file = self.pi0_file_list[file_num]\n",
    "            event_data = np.load(file, allow_pickle=True).item()\n",
    "            num_events = len(event_data[[key for key in event_data.keys()][0]])\n",
    "\n",
    "            for event_ind in range(num_events):\n",
    "                num_clusters = event_data['nCluster'][event_ind]\n",
    "                \n",
    "                for i in range(num_clusters):\n",
    "                    cluster_calib_E = self.get_cluster_calib(event_data, event_ind, i)\n",
    "                    \n",
    "                    if cluster_calib_E is None:\n",
    "                        continue\n",
    "                        \n",
    "                    nodes, global_node, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, i)\n",
    "                    senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)         \n",
    "                    \n",
    "                    # WIP add track nodes and edges ----------------------------------------------------------------\n",
    "                    track_nodes = np.empty((0, 11))\n",
    "                    num_tracks = event_data['nTrack'][event_ind]\n",
    "                    for track_index in range(num_tracks):\n",
    "                        np.append(track_nodes, self.get_track_node(event_data, event_ind, track_index).reshape(1, -1), axis=0)\n",
    "                    \n",
    "                    track_senders, track_receivers, track_edge_features = self.get_track_edges(len(track_nodes), cluster_num_nodes)\n",
    "                    \n",
    "                    \n",
    "                    # append on the track nodes and edges to the cluster ones\n",
    "#                     if track_nodes:\n",
    "                    nodes = np.append(nodes, np.array(track_nodes), axis=0)\n",
    "                    edges = np.append(edges, track_edge_features, axis=0)\n",
    "                    senders = np.append(senders, track_senders, axis=0)\n",
    "                    receivers = np.append(receivers, track_receivers, axis=0)\n",
    "                    \n",
    "                    \n",
    "#                     print(\"nodes\", nodes)\n",
    "#                     print(\"edges\", edges)\n",
    "#                     raise Exception(\"asdf\")\n",
    "                    \n",
    "                    # end WIP ----------------------------------------------------------------\n",
    "                    \n",
    "                    graph = {'nodes': nodes.astype(np.float32), 'globals': global_node.astype(np.float32),\n",
    "                        'senders': senders.astype(np.int32), 'receivers': receivers.astype(np.int32),\n",
    "                        'edges': edges.astype(np.float32)}\n",
    "                    target = np.reshape([cluster_calib_E.astype(np.float32), 0], [1,2])\n",
    "\n",
    "                    preprocessed_data.append((graph, target))\n",
    "\n",
    "            random.shuffle(preprocessed_data)\n",
    "\n",
    "            pickle.dump(preprocessed_data, open(self.output_dir + f'data_{file_num:03d}.p', 'wb'), compression='gzip')\n",
    "            \n",
    "            print(f\"Finished processing {file_num} files\")\n",
    "            file_num += self.num_procs\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        print('\\nPreprocessing and saving data to {}'.format(self.output_dir))\n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.preprocessor, args=(i,), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()\n",
    "\n",
    "        self.file_list = [self.output_dir + f'data_{i:03d}.p' for i in range(self.num_files)]\n",
    "\n",
    "    def preprocessed_worker(self, worker_id, batch_queue):\n",
    "        batch_graphs = []\n",
    "        batch_targets = []\n",
    "\n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
    "\n",
    "            for i in range(len(file_data)):\n",
    "                batch_graphs.append(file_data[i][0])\n",
    "                batch_targets.append(file_data[i][1])\n",
    "                    \n",
    "                if len(batch_graphs) == self.batch_size:\n",
    "                    batch_targets = np.reshape(np.array(batch_targets), [-1,2]).astype(np.float32)\n",
    "                    \n",
    "                    batch_queue.put((batch_graphs, batch_targets))\n",
    "                    \n",
    "                    batch_graphs = []\n",
    "                    batch_targets = []\n",
    "\n",
    "            file_num += self.num_procs\n",
    "                    \n",
    "        if len(batch_graphs) > 0:\n",
    "            batch_targets = np.reshape(np.array(batch_targets), [-1,2]).astype(np.float32)\n",
    "            \n",
    "            batch_queue.put((batch_graphs, batch_targets))\n",
    "\n",
    "    def worker(self, worker_id, batch_queue):\n",
    "        if self.preprocess:\n",
    "            self.preprocessed_worker(worker_id, batch_queue)\n",
    "        else:\n",
    "            raise Exception('Preprocessing is required for combined classification/regression models.')\n",
    "        \n",
    "    def check_procs(self):\n",
    "        for p in self.procs:\n",
    "            if p.is_alive(): return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def kill_procs(self):\n",
    "        for p in self.procs:\n",
    "            p.kill()\n",
    "\n",
    "        self.procs = []\n",
    "    \n",
    "    def generator(self):\n",
    "        \"\"\"\n",
    "        Generator that returns processed batches during training\n",
    "        \"\"\"\n",
    "        batch_queue = Queue(2 * self.num_procs)\n",
    "            \n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.worker, args=(i, batch_queue), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        while self.check_procs() or not batch_queue.empty():\n",
    "            try:\n",
    "                batch = batch_queue.get(True, 0.0001)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            yield batch\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/clusterfs/ml4hep/mpettee/ml4pions/data/'\n",
    "out_dir = '/clusterfs/ml4hep/mpettee/ml4pions/data/tracks_withcuts/'\n",
    "pi0_files = np.sort(glob.glob(data_dir+'*pi0*/*.npy'))#[10:11]\n",
    "pion_files = np.sort(glob.glob(data_dir+'*pion*/*.npy'))#[10:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '/clusterfs/ml4hep/mpettee/ml4pions/data/'\n",
    "# out_dir = '/clusterfs/ml4hep/mpettee/ml4pions/data/tracks_withcuts/'\n",
    "# pi0_files = np.sort(glob.glob(data_dir+'*pi0*/*.root'))[:2]\n",
    "# pion_files = np.sort(glob.glob(data_dir+'*pion*/*.root'))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load(pi0_files[0], allow_pickle=True).item().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing and saving data to /clusterfs/ml4hep/mpettee/ml4pions/data/tracks_withcuts/\n",
      "Processing file number 0\n",
      "Processing file number 1\n",
      "Processing file number 2\n",
      "Processing file number 3\n",
      "Processing file number 4\n",
      "Processing file number 5\n",
      "Processing file number 6\n",
      "Processing file number 7\n",
      "Processing file number 8\n",
      "Processing file number 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-3:\n",
      "Process Process-1:\n",
      "Process Process-2:\n",
      "Process Process-10:\n",
      "Process Process-7:\n",
      "Process Process-8:\n",
      "Process Process-9:\n",
      "Process Process-4:\n",
      "Process Process-6:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "TypeError: int() argument must be a string, a bytes-like object or a number, not 'STLVector'\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 207, in preprocessor\n",
      "    senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 207, in preprocessor\n",
      "    senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 207, in preprocessor\n",
      "    senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 207, in preprocessor\n",
      "    senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 207, in preprocessor\n",
      "    senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 206, in preprocessor\n",
      "    nodes, global_node, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, i)\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 206, in preprocessor\n",
      "    nodes, global_node, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, i)\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 179, in get_edges\n",
      "    receivers = cell_IDmap_sorter[rank[rank!=cluster_num_nodes]]\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 233, in preprocessor\n",
      "    nodes = np.append(nodes, np.array(track_nodes), axis=0)\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 170, in get_edges\n",
      "    edge_inds[np.logical_not(np.isin(edge_inds, cell_IDmap))] = np.nan\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 172, in get_edges\n",
      "    senders, edge_on_inds = np.isin(edge_inds, cell_IDmap).nonzero()\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 172, in get_edges\n",
      "    senders, edge_on_inds = np.isin(edge_inds, cell_IDmap).nonzero()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 170, in get_edges\n",
      "    edge_inds[np.logical_not(np.isin(edge_inds, cell_IDmap))] = np.nan\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 82, in get_nodes\n",
      "    nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
      "KeyboardInterrupt\n",
      "  File \"<__array_function__ internals>\", line 5, in append\n",
      "  File \"<__array_function__ internals>\", line 5, in isin\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 72, in get_nodes\n",
      "    nodes = np.log10(event_data['cluster_cell_E'][event_ind][cluster_ind])\n",
      "  File \"<__array_function__ internals>\", line 5, in isin\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 206, in preprocessor\n",
      "    nodes, global_node, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, i)\n",
      "  File \"<__array_function__ internals>\", line 5, in append\n",
      "  File \"<__array_function__ internals>\", line 5, in isin\n",
      "  File \"<__array_function__ internals>\", line 5, in isin\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/uproot/containers.py\", line 1290, in __iter__\n",
      "    def __iter__(self):\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/lib/function_base.py\", line 4671, in append\n",
      "    return concatenate((arr, values), axis=axis)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 707, in isin\n",
      "    return in1d(element, test_elements, assume_unique=assume_unique,\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/lib/function_base.py\", line 4669, in append\n",
      "    values = ravel(values)\n",
      "  File \"/tmp/ipykernel_13483/3998933964.py\", line 70, in get_nodes\n",
      "    cell_IDmap = self.sorter[np.searchsorted(self.cellGeo_ID, cell_IDs, sorter=self.sorter)]\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 707, in isin\n",
      "    return in1d(element, test_elements, assume_unique=assume_unique,\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 707, in isin\n",
      "    return in1d(element, test_elements, assume_unique=assume_unique,\n",
      "KeyboardInterrupt\n",
      "  File \"<__array_function__ internals>\", line 5, in in1d\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 707, in isin\n",
      "    return in1d(element, test_elements, assume_unique=assume_unique,\n",
      "  File \"<__array_function__ internals>\", line 5, in ravel\n",
      "  File \"<__array_function__ internals>\", line 5, in searchsorted\n",
      "  File \"<__array_function__ internals>\", line 5, in in1d\n",
      "  File \"<__array_function__ internals>\", line 5, in concatenate\n",
      "  File \"<__array_function__ internals>\", line 5, in in1d\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 585, in in1d\n",
      "    ar1, rev_idx = np.unique(ar1, return_inverse=True)\n",
      "  File \"<__array_function__ internals>\", line 5, in in1d\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/core/fromnumeric.py\", line 1809, in ravel\n",
      "    return asanyarray(a).ravel(order=order)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 585, in in1d\n",
      "    ar1, rev_idx = np.unique(ar1, return_inverse=True)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/core/fromnumeric.py\", line 1343, in searchsorted\n",
      "    return _wrapfunc(a, 'searchsorted', v, side=side, sorter=sorter)\n",
      "  File \"<__array_function__ internals>\", line 5, in unique\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 605, in in1d\n",
      "    return ret[rev_idx]\n",
      "  File \"<__array_function__ internals>\", line 5, in unique\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/core/fromnumeric.py\", line 58, in _wrapfunc\n",
      "    return bound(*args, **kwds)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 261, in unique\n",
      "    ret = _unique1d(ar, return_index, return_inverse, return_counts)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 580, in in1d\n",
      "    mask |= (ar1 == a)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13483/3650998771.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m data_gen = GraphDataGenerator(pion_file_list=pion_files, \n\u001b[0m\u001b[1;32m      2\u001b[0m                               \u001b[0mpi0_file_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpi0_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               \u001b[0mcellGeo_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'cell_geo.root'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13483/3998933964.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pi0_file_list, pion_file_list, cellGeo_file, batch_size, shuffle, num_procs, preprocess, output_dir)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodeFeatureNames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13483/3998933964.py\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'data_{i:03d}.p'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 261, in unique\n",
      "    ret = _unique1d(ar, return_index, return_inverse, return_counts)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 319, in _unique1d\n",
      "    perm = ar.argsort(kind='mergesort' if return_index else 'quicksort')\n",
      "ValueError: setting an array element with a sequence.\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/numpy/lib/arraysetops.py\", line 314, in _unique1d\n",
      "    ar = np.asanyarray(ar).flatten()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "data_gen = GraphDataGenerator(pion_file_list=pion_files, \n",
    "                              pi0_file_list=pi0_files,\n",
    "                              cellGeo_file=data_dir+'cell_geo.root',\n",
    "                              batch_size=32,\n",
    "                              shuffle=False,\n",
    "                              num_procs=32,\n",
    "                              preprocess=True,\n",
    "                              output_dir=out_dir)\n",
    "\n",
    "# gen = data_gen.generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(data_gen.generator())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.append(np.empty((0, 11)), np.array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
    "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  5.69970337e+02,\n",
    "        6.30014613e-02, -4.63907570e-02,  5.21522835e-02]).reshape(1, -1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.append(np.append(a, a, axis=0), a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/clusterfs/ml4hep/mfong/ML4Pions/graph_data_tracks/'\n",
    "pi0_files = np.sort(glob.glob(data_dir+'*pi0*/*.root'))[20:30]\n",
    "pion_files = np.sort(glob.glob(data_dir+'*pion*/*.root'))[20:30]\n",
    "data_gen_test = GraphDataGenerator(pion_file_list=pion_files, \n",
    "                                   pi0_file_list=pi0_files,\n",
    "                                   cellGeo_file=data_dir+'CellGeo.neighbours.root',\n",
    "                                   batch_size=32,\n",
    "                                   shuffle=False,\n",
    "                                   num_procs=32,\n",
    "                                   preprocess=True,\n",
    "                                   output_dir=out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "nbdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
