{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modules.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import uproot as ur\n",
    "import time\n",
    "from multiprocessing import Process, Queue, set_start_method\n",
    "import compress_pickle as pickle\n",
    "from scipy.stats import circmean\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphDataGenerator\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GraphDataGenerator:\n",
    "    \"\"\"\n",
    "    DataGenerator class for extracting and formating data from list of root files\n",
    "    This data generator uses the cell_geo file to create the input graph structure\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 pi0_file_list: list,\n",
    "                 pion_file_list: list,\n",
    "                 cellGeo_file: str,\n",
    "                 batch_size: int,\n",
    "                 shuffle: bool = True,\n",
    "                 num_procs = 32,\n",
    "                 preprocess = False,\n",
    "                 output_dir = None):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            self.pi0_file_list = pi0_file_list\n",
    "            self.pion_file_list = pion_file_list\n",
    "#             assert len(pi0_file_list) == len(pion_file_list)\n",
    "            self.num_files = len(self.pion_file_list)\n",
    "        else:\n",
    "            self.file_list = pion_file_list\n",
    "            self.num_files = len(self.file_list)\n",
    "        \n",
    "        self.cellGeo_file = cellGeo_file\n",
    "        \n",
    "        self.cellGeo_data = ur.open(self.cellGeo_file)['CellGeo']\n",
    "        self.geoFeatureNames = self.cellGeo_data.keys()[1:9]\n",
    "        self.nodeFeatureNames = ['cluster_cell_E', *self.geoFeatureNames[:-2]]\n",
    "        self.edgeFeatureNames = self.cellGeo_data.keys()[9:]\n",
    "        self.num_nodeFeatures = len(self.nodeFeatureNames)\n",
    "        self.num_edgeFeatures = len(self.edgeFeatureNames)\n",
    "        self.cellGeo_data = self.cellGeo_data.arrays(library='np')\n",
    "        self.cellGeo_ID = self.cellGeo_data['cell_geo_ID'][0]\n",
    "        self.sorter = np.argsort(self.cellGeo_ID)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        if self.shuffle: np.random.shuffle(self.file_list)\n",
    "        \n",
    "        self.num_procs = np.min([num_procs, self.num_files])\n",
    "        self.procs = []\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            os.makedirs(self.output_dir, exist_ok=True)\n",
    "            self.preprocess_data()\n",
    "    \n",
    "    def get_cluster_calib(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading cluster calibration energy \"\"\" \n",
    "        cluster_calib_E = event_data['cluster_ENG_CALIB_TOT'][event_ind][cluster_ind]\n",
    "\n",
    "        if cluster_calib_E <= 0:\n",
    "            return None\n",
    "\n",
    "        return np.log10(cluster_calib_E)\n",
    "    \n",
    "    def get_cluster_eta(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading cluster eta \"\"\" \n",
    "        cluster_eta = event_data['cluster_Eta'][event_ind][cluster_ind]\n",
    "        return cluster_eta\n",
    "    \n",
    "    def get_nodes(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading Node features \"\"\" \n",
    "\n",
    "        cell_IDs = event_data['cluster_cell_ID'][event_ind][cluster_ind]\n",
    "        cell_IDmap = self.sorter[np.searchsorted(self.cellGeo_ID, cell_IDs, sorter=self.sorter)]\n",
    "        \n",
    "        nodes = np.log10(event_data['cluster_cell_E'][event_ind][cluster_ind])\n",
    "        global_node = np.log10(event_data['cluster_E'][event_ind][cluster_ind])\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_sampling'][0][cell_IDmap]/28.)\n",
    "        for f in self.nodeFeatureNames[2:4]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "        # Scaling the cell_geo_rPerp by 3000\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_rPerp'][0][cell_IDmap]/3000.)\n",
    "        for f in self.nodeFeatureNames[5:]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "\n",
    "        nodes = np.reshape(nodes, (len(self.nodeFeatureNames), -1)).T\n",
    "        cluster_num_nodes = len(nodes)\n",
    "    \n",
    "        # add dummy placeholder nodes for track features (not used in cluster cell nodes)\n",
    "        nodes = np.hstack((nodes, np.zeros((cluster_num_nodes, 4))))\n",
    "        \n",
    "        return nodes, np.array([global_node]), cluster_num_nodes, cell_IDmap\n",
    "    \n",
    "    # track section ----------------------------------------------------------------\n",
    "    \n",
    "    def get_track_node(self, event_data, event_index, track_index):\n",
    "        \"\"\"\n",
    "        Creates node features for tracks\n",
    "        Inputs:\n",
    "        \n",
    "        Returns:\n",
    "            1 Dimensional array of node features for a single node\n",
    "                NOTE the cluster get_node function is a 2D array of multiple nodes\n",
    "                This function is used in a for loop so the end result is a 2D array\n",
    "        \"\"\"\n",
    "        node_features = np.array(event_data[\"trackPt\"][event_index][track_index])\n",
    "        node_features = np.append(node_features, event_data[\"trackZ0\"][event_index][track_index])\n",
    "        node_features = np.append(node_features, event_data[\"trackEta_EMB2\"][event_index][track_index])\n",
    "        node_features = np.append(node_features, event_data[\"trackPhi_EMB2\"][event_index][track_index])\n",
    "        node_features = np.reshape(node_features, (len(node_features))).T\n",
    "        \n",
    "        # add dummy placeholder nodes for track features (not used in track cell nodes)\n",
    "        node_features = np.hstack((np.zeros(7), node_features))\n",
    "    \n",
    "        return node_features\n",
    "    \n",
    "    def get_track_edges(self, num_track_nodes, start_index):\n",
    "        \"\"\"\n",
    "        Creates the edge senders and recievers and edge features\n",
    "        Inputs:\n",
    "        (int) num_track_nodes: number of track nodes\n",
    "        (int) start_index: the index of senders/recievers to start with. We should start with num_cluster_edges+1 to avoid overlap\n",
    "\n",
    "        Returns:\n",
    "        (np.array) edge_features:\n",
    "        (np.array) senders:\n",
    "        (np.array) recievers:\n",
    "        \"\"\"\n",
    "        # Full Connected tracks\n",
    "        # since we are fully connected, the order of senders and recievers doesn't matter\n",
    "        # we just need to count each node - edges will have a placeholder feature\n",
    "        connections = list(itertools.permutations(range(start_index, start_index + num_track_nodes),2))\n",
    "        for i in range(5):\n",
    "            connections.append((i, i))\n",
    "\n",
    "        senders = np.array([x[0] for x in connections])\n",
    "        recievers = np.array([x[0] for x in connections])\n",
    "        edge_features = np.zeros((len(connections), 10))\n",
    "\n",
    "        return senders, recievers, edge_features\n",
    "    \n",
    "    # end track section ----------------------------------------------------------------\n",
    "    \n",
    "    def get_edges(self, cluster_num_nodes, cell_IDmap):\n",
    "        \"\"\" \n",
    "        Reading edge features \n",
    "        Returns senders, receivers, and edges    \n",
    "        \"\"\" \n",
    "        \n",
    "        edge_inds = np.zeros((cluster_num_nodes, self.num_edgeFeatures))\n",
    "        for i, f in enumerate(self.edgeFeatureNames):\n",
    "            edge_inds[:, i] = self.cellGeo_data[f][0][cell_IDmap]\n",
    "        edge_inds[np.logical_not(np.isin(edge_inds, cell_IDmap))] = np.nan\n",
    "        \n",
    "        senders, edge_on_inds = np.isin(edge_inds, cell_IDmap).nonzero()\n",
    "        cluster_num_edges = len(senders)\n",
    "        edges = np.zeros((cluster_num_edges, self.num_edgeFeatures))\n",
    "        edges[np.arange(cluster_num_edges), edge_on_inds] = 1\n",
    "        \n",
    "        cell_IDmap_sorter = np.argsort(cell_IDmap)\n",
    "        rank = np.searchsorted(cell_IDmap, edge_inds , sorter=cell_IDmap_sorter)\n",
    "        receivers = cell_IDmap_sorter[rank[rank!=cluster_num_nodes]]\n",
    "        \n",
    "        return senders, receivers, edges\n",
    "\n",
    "    def preprocessor(self, worker_id):\n",
    "        \"\"\"\n",
    "        Prerocessing root file data for faster data \n",
    "        generation during multiple training epochs\n",
    "        \"\"\"\n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            print(f\"Processing file number {file_num}\")\n",
    "            \n",
    "            ### Pions\n",
    "            if len(self.pion_file_list) == 0:\n",
    "                print(\"No pion files.\")\n",
    "            elif len(self.pion_file_list) > 0:\n",
    "                file = self.pion_file_list[file_num]\n",
    "                event_data = np.load(file, allow_pickle=True).item()\n",
    "                num_events = len(event_data[[key for key in event_data.keys()][0]])\n",
    "\n",
    "                preprocessed_data = []\n",
    "\n",
    "                for event_ind in range(num_events):\n",
    "                    num_clusters = event_data['nCluster'][event_ind]\n",
    "                    truth_particle_E = np.log10(event_data['truthPartE'][event_ind][0]) # first one is the pion! \n",
    "                    truthPartPt = event_data['truthPartPt'][event_ind][0]\n",
    "\n",
    "                    for i in range(num_clusters):\n",
    "                        if event_data['dR_pass'][event_ind][i] == False:\n",
    "                            continue\n",
    "                        cluster_calib_E = self.get_cluster_calib(event_data, event_ind, i)\n",
    "                        cluster_EM_prob = event_data['cluster_EM_PROBABILITY'][event_ind][i]\n",
    "                        cluster_E = np.log10(event_data['cluster_E'][event_ind][i])\n",
    "                        cluster_HAD_WEIGHT = event_data['cluster_HAD_WEIGHT'][event_ind][i]\n",
    "\n",
    "                        if cluster_calib_E is None:\n",
    "                            continue\n",
    "\n",
    "                        cluster_eta = self.get_cluster_eta(event_data, event_ind, i)\n",
    "\n",
    "                        nodes, global_node, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, i)\n",
    "                        senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
    "\n",
    "                    # track section ----------------------------------------------------------------\n",
    "                        track_nodes = np.empty((0, 11))\n",
    "                        num_tracks = event_data['nTrack'][event_ind]\n",
    "                        if num_tracks != 1: \n",
    "                            print(\"Num tracks:\", num_tracks)\n",
    "                        for track_index in range(num_tracks):\n",
    "                            np.append(track_nodes, self.get_track_node(event_data, event_ind, track_index).reshape(1, -1), axis=0)\n",
    "                            track_pt = event_data[\"trackPt\"][event_ind][track_index]\n",
    "\n",
    "                        track_senders, track_receivers, track_edge_features = self.get_track_edges(len(track_nodes), cluster_num_nodes)\n",
    "\n",
    "                        # append on the track nodes and edges to the cluster ones\n",
    "                        nodes = np.append(nodes, np.array(track_nodes), axis=0)\n",
    "                        edges = np.append(edges, track_edge_features, axis=0)\n",
    "                        senders = np.append(senders, track_senders, axis=0)\n",
    "                        receivers = np.append(receivers, track_receivers, axis=0)\n",
    "\n",
    "                        # end track section ----------------------------------------------------------------\n",
    "\n",
    "                        graph = {'nodes': nodes.astype(np.float32), \n",
    "    #                              'globals': global_node.astype(np.float32), # cluster_E\n",
    "                                 'globals': track_pt.astype(np.float32), # track_pt\n",
    "                            'senders': senders.astype(np.int32), 'receivers': receivers.astype(np.int32),\n",
    "                            'edges': edges.astype(np.float32), 'cluster_calib_E': cluster_calib_E.astype(np.float32), \n",
    "                            'cluster_eta': cluster_eta.astype(np.float32), 'cluster_EM_prob': cluster_EM_prob.astype(np.float32),\n",
    "                            'cluster_E': cluster_E.astype(np.float32), 'cluster_HAD_WEIGHT': cluster_HAD_WEIGHT.astype(np.float32), \n",
    "                            'truthPartPt': truthPartPt.astype(np.float32), 'track_pt': track_pt.astype(np.float32)}\n",
    "                        target = np.reshape([truth_particle_E.astype(np.float32), 1], [1,2])\n",
    "\n",
    "                        preprocessed_data.append((graph, target))\n",
    "\n",
    "            ### Pi0\n",
    "            if len(self.pi0_file_list) == 0:\n",
    "                print(\"No pi0 files.\")\n",
    "            elif len(self.pi0_file_list) > 0:\n",
    "                file = self.pi0_file_list[file_num]\n",
    "                event_data = np.load(file, allow_pickle=True).item()\n",
    "                num_events = len(event_data[[key for key in event_data.keys()][0]])\n",
    "\n",
    "                for event_ind in range(num_events):\n",
    "                    num_clusters = event_data['nCluster'][event_ind]\n",
    "                    truth_particle_E = np.log10(event_data['truthPartE'][event_ind][0]) # first one is the pion! \n",
    "                    truthPartPt = event_data['truthPartPt'][event_ind][0]\n",
    "\n",
    "                    for i in range(num_clusters):\n",
    "                        cluster_calib_E = self.get_cluster_calib(event_data, event_ind, i)\n",
    "                        cluster_EM_prob = event_data['cluster_EM_PROBABILITY'][event_ind][i]\n",
    "                        cluster_E = np.log10(event_data['cluster_E'][event_ind][i])\n",
    "                        cluster_HAD_WEIGHT = event_data['cluster_HAD_WEIGHT'][event_ind][i]\n",
    "\n",
    "                        if event_data['dR_pass'][event_ind][i] == False:\n",
    "                            continue\n",
    "\n",
    "                        if cluster_calib_E is None:\n",
    "                            continue\n",
    "\n",
    "                        cluster_eta = self.get_cluster_eta(event_data, event_ind, i)\n",
    "\n",
    "                        nodes, global_node, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, i)\n",
    "                        senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)         \n",
    "\n",
    "                        # track section ----------------------------------------------------------------\n",
    "                        track_nodes = np.empty((0, 11))\n",
    "                        num_tracks = event_data['nTrack'][event_ind]\n",
    "                        for track_index in range(num_tracks):\n",
    "                            np.append(track_nodes, self.get_track_node(event_data, event_ind, track_index).reshape(1, -1), axis=0)\n",
    "                            track_pt = event_data[\"trackPt\"][event_ind][track_index]\n",
    "\n",
    "                        track_senders, track_receivers, track_edge_features = self.get_track_edges(len(track_nodes), cluster_num_nodes)\n",
    "\n",
    "                        nodes = np.append(nodes, np.array(track_nodes), axis=0)\n",
    "                        edges = np.append(edges, track_edge_features, axis=0)\n",
    "                        senders = np.append(senders, track_senders, axis=0)\n",
    "                        receivers = np.append(receivers, track_receivers, axis=0)\n",
    "\n",
    "                        # end track section ----------------------------------------------------------------\n",
    "\n",
    "                        graph = {'nodes': nodes.astype(np.float32), 'globals': global_node.astype(np.float32),\n",
    "                            'senders': senders.astype(np.int32), 'receivers': receivers.astype(np.int32),\n",
    "                            'edges': edges.astype(np.float32), 'cluster_calib_E': cluster_calib_E.astype(np.float32), \n",
    "                            'cluster_eta': cluster_eta.astype(np.float32), 'cluster_EM_prob': cluster_EM_prob.astype(np.float32),\n",
    "                            'cluster_E': cluster_E.astype(np.float32), 'cluster_HAD_WEIGHT': cluster_HAD_WEIGHT.astype(np.float32),\n",
    "                            'truthPartPt': truthPartPt.astype(np.float32), 'track_pt': track_pt.astype(np.float32)}\n",
    "                        target = np.reshape([truth_particle_E.astype(np.float32), 0], [1,2])\n",
    "\n",
    "                        preprocessed_data.append((graph, target))\n",
    "\n",
    "            random.shuffle(preprocessed_data)\n",
    "\n",
    "            pickle.dump(preprocessed_data, open(self.output_dir + f'data_{file_num:03d}.p', 'wb'), compression='gzip')\n",
    "            \n",
    "            print(f\"Finished processing {file_num} files\")\n",
    "            file_num += self.num_procs\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        print('\\nPreprocessing and saving data to {}'.format(self.output_dir))\n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.preprocessor, args=(i,), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()\n",
    "\n",
    "        self.file_list = [self.output_dir + f'data_{i:03d}.p' for i in range(self.num_files)]\n",
    "\n",
    "    def preprocessed_worker(self, worker_id, batch_queue):\n",
    "        batch_graphs = []\n",
    "        batch_targets = []\n",
    "        \n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
    "\n",
    "            for i in range(len(file_data)):\n",
    "                batch_graphs.append(file_data[i][0])\n",
    "                batch_targets.append(file_data[i][1])\n",
    "                    \n",
    "                if len(batch_graphs) == self.batch_size:\n",
    "                    batch_targets = np.reshape(np.array(batch_targets), [-1,2]).astype(np.float32)\n",
    "                    \n",
    "                    batch_queue.put((batch_graphs, batch_targets))\n",
    "                    \n",
    "                    batch_graphs = []\n",
    "                    batch_targets = []\n",
    "\n",
    "            file_num += self.num_procs\n",
    "                    \n",
    "        if len(batch_graphs) > 0:\n",
    "            batch_targets = np.reshape(np.array(batch_targets), [-1,2]).astype(np.float32)\n",
    "            batch_queue.put((batch_graphs, batch_targets))\n",
    "\n",
    "    def worker(self, worker_id, batch_queue):\n",
    "        if self.preprocess:\n",
    "            self.preprocessed_worker(worker_id, batch_queue)\n",
    "        else:\n",
    "            raise Exception('Preprocessing is required for combined classification/regression models.')\n",
    "        \n",
    "    def check_procs(self):\n",
    "        for p in self.procs:\n",
    "            if p.is_alive(): return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def kill_procs(self):\n",
    "        for p in self.procs:\n",
    "            p.kill()\n",
    "\n",
    "        self.procs = []\n",
    "    \n",
    "    def generator(self):\n",
    "        \"\"\"\n",
    "        Generator that returns processed batches during training\n",
    "        \"\"\"\n",
    "        batch_queue = Queue(2 * self.num_procs)\n",
    "            \n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.worker, args=(i, batch_queue), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        while self.check_procs() or not batch_queue.empty():\n",
    "            try:\n",
    "                batch = batch_queue.get(True, 0.0001)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            yield batch\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the data generation step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/clusterfs/ml4hep/mpettee/ml4pions/data/onetrack_multicluster/'\n",
    "# pi0_files = np.sort(glob.glob(data_dir+'pi0_files/*.npy'))[:1]\n",
    "pi0_files = []\n",
    "pion_files = np.sort(glob.glob(data_dir+'pion_files/*.npy'))[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing and saving data to /clusterfs/ml4hep/mpettee/ml4pions/data/onetrack_multicluster/\n",
      "Processing file number 0\n",
      "No pi0 files.\n",
      "Finished processing 0 files\n"
     ]
    }
   ],
   "source": [
    "data_gen = GraphDataGenerator(pion_file_list=pion_files, \n",
    "                              pi0_file_list=pi0_files,\n",
    "                              cellGeo_file='/clusterfs/ml4hep/mpettee/ml4pions/data/cell_geo.root',\n",
    "                              batch_size=32,\n",
    "                              shuffle=False,\n",
    "                              num_procs=32,\n",
    "                              preprocess=True,\n",
    "                              output_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "nbdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
