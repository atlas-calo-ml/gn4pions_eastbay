{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modules.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import uproot as ur\n",
    "import time\n",
    "from multiprocessing import Process, Queue, set_start_method\n",
    "import compress_pickle as pickle\n",
    "from scipy.stats import circmean\n",
    "import random\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt # optional\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "})\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphDataGenerator\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../../data/onetrack_multicluster/pion_files/001.npy\"\n",
    "event_data = np.load(file, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(pd.DataFrame(event_data).trackEta.explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "bins = np.linspace(0,2500,30)\n",
    "# plt.hist(np.concatenate(event_data['truthPartE']).ravel(), color='black', bins=bins, histtype='step', label=\"truthPartE\");\n",
    "plt.hist(np.concatenate(event_data['cluster_E']).ravel(), bins=bins, histtype='step', label=\"cluster_E\");\n",
    "plt.hist(np.array(event_data['trackPt']), bins=bins, histtype='step', label='trackPt');\n",
    "plt.legend();\n",
    "plt.yscale('log');\n",
    "plt.xlabel('[MeV]');\n",
    "\n",
    "deltas = []\n",
    "for i in range(len(event_data['cluster_E'])):\n",
    "    for j in range(event_data['nCluster'][i]):\n",
    "        deltas.append(np.abs(event_data['trackPt'][i]-event_data['cluster_E'][i][j])/event_data['trackPt'][i])\n",
    "        \n",
    "plt.figure(dpi=150)\n",
    "plt.hist(np.concatenate(deltas), bins=np.linspace(0,1, 30));\n",
    "plt.xlabel(\"abs(trackPt - clusterE) / trackPt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "scales = {\n",
    "     'track_pt_mean': 1.633278727,\n",
    "     'track_pt_std': 0.8481947183,\n",
    "     'track_z0_mean': 0.08022017,\n",
    "     'track_z0_std': 42.53320004,\n",
    "     'track_eta_mean': -0.00563187,\n",
    "     'track_eta_std': 1.35242735,\n",
    "     'track_phi_mean': 0.00206431,\n",
    "     'track_phi_std': 1.81240248,\n",
    "     'truth_part_e_mean': 1.92469358,\n",
    "     'truth_part_e_std': 0.8289864,\n",
    "     'cluster_cell_e_mean': -1.0121697,\n",
    "     'cluster_cell_e_std': 0.818357,\n",
    "     'cluster_e_mean': 0.89923394,\n",
    "     'cluster_e_std': 1.0585934,\n",
    "     'cluster_eta_mean': 0.016195267,\n",
    "     'cluster_eta_std': 1.3400925,\n",
    "     'cluster_phi_mean': 0.0050816955,\n",
    "     'cluster_phi_std': 1.8100655,\n",
    "     'cell_geo_sampling_mean': 3.8827391420197177,\n",
    "     'cell_geo_sampling_std': 3.9588233603576204,\n",
    "     'cell_geo_eta_mean': 0.0005979097,\n",
    "     'cell_geo_eta_std': 1.4709069,\n",
    "     'cell_geo_phi_mean': -2.8938382e-05,\n",
    "     'cell_geo_phi_std': 1.813651,\n",
    "     'cell_geo_rPerp_mean': 1478.9285,\n",
    "     'cell_geo_rPerp_std': 434.60815,\n",
    "     'cell_geo_deta_mean': 0.026611786,\n",
    "     'cell_geo_deta_std': 0.03396141,\n",
    "     'cell_geo_dphi_mean': 0.068693615,\n",
    "     'cell_geo_dphi_std': 0.038586758}\n",
    "\n",
    "node_means = [\n",
    "    scales[\"cluster_cell_e_mean\"], \n",
    "    scales[\"cell_geo_sampling_mean\"],\n",
    "    scales[\"cell_geo_eta_mean\"],\n",
    "    scales[\"cell_geo_phi_mean\"],\n",
    "    scales[\"cell_geo_rPerp_mean\"],\n",
    "    scales[\"cell_geo_deta_mean\"],\n",
    "    scales[\"cell_geo_dphi_mean\"],\n",
    "] \n",
    "\n",
    "node_stds = [\n",
    "    scales[\"cluster_cell_e_std\"], \n",
    "    scales[\"cell_geo_sampling_std\"],\n",
    "    scales[\"cell_geo_eta_std\"],\n",
    "    scales[\"cell_geo_phi_std\"],\n",
    "    scales[\"cell_geo_rPerp_std\"],\n",
    "    scales[\"cell_geo_deta_std\"],\n",
    "    scales[\"cell_geo_dphi_std\"],\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot preprocessed track inputs \n",
    "track_pt = (np.log10(np.concatenate(event_data['trackPt'])) - scales['track_pt_mean'])/scales['track_pt_std']\n",
    "track_z0 = (np.concatenate(event_data['trackZ0']) - scales['track_z0_mean'])/scales['track_z0_std']\n",
    "track_eta = (np.concatenate(event_data['trackEta']) - scales['track_eta_mean'])/scales['track_eta_std']\n",
    "track_phi = (np.concatenate(event_data['trackPhi']) - scales['track_phi_mean'])/scales['track_phi_std']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(12,3), dpi=200, tight_layout=True)\n",
    "fig.suptitle(\"Scaled Track Inputs\")\n",
    "\n",
    "ax[0].hist(track_pt, bins=30)\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_xlabel(r'Scaled Track $p_T$')\n",
    "\n",
    "ax[1].hist(track_z0, bins=30)\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_xlabel(r'Scaled Track $Z_0$')\n",
    "\n",
    "ax[2].hist(track_eta, bins=30)\n",
    "ax[2].set_yscale('log')\n",
    "ax[2].set_xlabel(r'Scaled Track $\\eta$')\n",
    "\n",
    "ax[3].hist(track_phi, bins=30)\n",
    "ax[3].set_yscale('log')\n",
    "ax[3].set_xlabel(r'Scaled Track $\\phi$');\n",
    "# plt.savefig(\"scaled_track_inputs.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot preprocessed energy inputs \n",
    "\n",
    "all_nodes = []\n",
    "\n",
    "n_events = len(event_data['index'])\n",
    "for event_ind in range(n_events):\n",
    "    for cluster_ind in range(event_data['nCluster'][event_ind]):\n",
    "        nodes = event_data['cluster_cell_E'][event_ind][cluster_ind]\n",
    "#         cell_IDs = event_data['cluster_cell_ID'][event_ind][cluster_ind]\n",
    "#         cell_IDmap = self.sorter[np.searchsorted(self.cellGeo_ID, cell_IDs, sorter=self.sorter)]\n",
    "#         nodes = np.append(nodes, cell_geo_data['cell_geo_sampling'][0][cell_IDmap])\n",
    "        nodes = np.reshape(nodes, (1, -1)).T\n",
    "        all_nodes.append(nodes)\n",
    "\n",
    "truth_particle_e = (np.log10(np.concatenate(event_data['truthPartE'])) - scales['truth_part_e_mean'])/scales['truth_part_e_std']\n",
    "cluster_cell_e = (np.log10(np.concatenate(all_nodes)) - scales['cluster_cell_e_mean'])/scales['cluster_cell_e_std']\n",
    "cluster_e = (np.log10(np.concatenate(event_data['cluster_E'])) - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(8,3), dpi=200, tight_layout=True)\n",
    "fig.suptitle(\"Scaled Energy Inputs\")\n",
    "\n",
    "ax[0].hist(truth_particle_e, bins=30)\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_xlabel(r'Truth Particle $E$')\n",
    "\n",
    "ax[1].hist(cluster_e, bins=30)\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_xlabel(r'Cluster $E$')\n",
    "\n",
    "ax[2].hist(cluster_cell_e, bins=30)\n",
    "ax[2].set_yscale('log')\n",
    "ax[2].set_xlabel(r'Cluster Cell $E$');\n",
    "# plt.savefig(\"scaled_energy_inputs.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "class GraphDataGenerator:\n",
    "    \"\"\"\n",
    "    DataGenerator class for extracting and formating data from list of root files\n",
    "    This data generator uses the cell_geo file to create the input graph structure\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 pi0_file_list: list,\n",
    "                 pion_file_list: list,\n",
    "                 cellGeo_file: str,\n",
    "                 batch_size: int,\n",
    "                 shuffle: bool = True,\n",
    "                 num_procs = 32,\n",
    "                 preprocess = False,\n",
    "                 output_dir = None):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            self.pi0_file_list = pi0_file_list\n",
    "            self.pion_file_list = pion_file_list\n",
    "#             assert len(pi0_file_list) == len(pion_file_list)\n",
    "            self.num_files = len(self.pion_file_list)\n",
    "        else:\n",
    "            self.file_list = pion_file_list\n",
    "            self.num_files = len(self.file_list)\n",
    "\n",
    "        self.cellGeo_file = cellGeo_file\n",
    "\n",
    "        self.cellGeo_data = ur.open(self.cellGeo_file)['CellGeo']\n",
    "        self.geoFeatureNames = self.cellGeo_data.keys()[1:9]\n",
    "        self.nodeFeatureNames = ['cluster_cell_E', *self.geoFeatureNames[:-2]]\n",
    "        self.edgeFeatureNames = self.cellGeo_data.keys()[9:]\n",
    "        self.num_nodeFeatures = len(self.nodeFeatureNames)\n",
    "        self.num_edgeFeatures = len(self.edgeFeatureNames)\n",
    "        self.cellGeo_data = self.cellGeo_data.arrays(library='np')\n",
    "        self.cellGeo_ID = self.cellGeo_data['cell_geo_ID'][0]\n",
    "        self.sorter = np.argsort(self.cellGeo_ID)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        if self.shuffle: np.random.shuffle(self.file_list)\n",
    "\n",
    "        self.num_procs = np.min([num_procs, self.num_files])\n",
    "        self.procs = []\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            os.makedirs(self.output_dir, exist_ok=True)\n",
    "            self.preprocess_data()\n",
    "\n",
    "    def get_cluster_calib(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading cluster calibration energy \"\"\"\n",
    "        cluster_calib_E = event_data['cluster_ENG_CALIB_TOT'][event_ind][cluster_ind]\n",
    "\n",
    "        if cluster_calib_E <= 0:\n",
    "            return None\n",
    "\n",
    "        return np.log10(cluster_calib_E)\n",
    "\n",
    "    def get_cluster_eta(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading cluster eta \"\"\"\n",
    "        cluster_eta = event_data['cluster_Eta'][event_ind][cluster_ind]\n",
    "        return cluster_eta\n",
    "\n",
    "    def get_nodes(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading Node features \"\"\"\n",
    "\n",
    "        cell_IDs = event_data['cluster_cell_ID'][event_ind][cluster_ind]\n",
    "        cell_IDmap = self.sorter[np.searchsorted(self.cellGeo_ID, cell_IDs, sorter=self.sorter)]\n",
    "\n",
    "        global_node = np.log10(event_data['cluster_E'][event_ind][cluster_ind])\n",
    "        nodes = np.log10(event_data['cluster_cell_E'][event_ind][cluster_ind])\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_sampling'][0][cell_IDmap])\n",
    "        for f in self.nodeFeatureNames[2:4]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_rPerp'][0][cell_IDmap])\n",
    "        for f in self.nodeFeatureNames[5:]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "\n",
    "        nodes = np.reshape(nodes, (len(self.nodeFeatureNames), -1)).T\n",
    "        nodes_scaled = (np.concatenate(nodes) - scales['cluster_cell_e_mean'])/scales['cluster_cell_e_std']\n",
    "        nodes_scaled = np.reshape(nodes, (len(self.nodeFeatureNames), -1)).T\n",
    "\n",
    "        nodes_scaled = (nodes - node_means)/node_stds\n",
    "        cluster_num_nodes = len(nodes)\n",
    "\n",
    "#         # add dummy placeholder nodes for track features (not used in cluster cell nodes)\n",
    "#         nodes = np.hstack((nodes, np.zeros((cluster_num_nodes, 4))))\n",
    "\n",
    "        return nodes_scaled, np.array([global_node]), cluster_num_nodes, cell_IDmap\n",
    "\n",
    "#     # track section ----------------------------------------------------------------\n",
    "\n",
    "#     def get_track_node(self, event_data, event_index, track_index):\n",
    "#         \"\"\"\n",
    "#         Creates node features for tracks\n",
    "#         Inputs:\n",
    "\n",
    "#         Returns:\n",
    "#             1 Dimensional array of node features for a single node\n",
    "#                 NOTE the cluster get_node function is a 2D array of multiple nodes\n",
    "#                 This function is used in a for loop so the end result is a 2D array\n",
    "#         \"\"\"\n",
    "#         node_features = np.array(event_data[\"trackPt\"][event_index][track_index])\n",
    "#         node_features = np.append(node_features, event_data[\"trackZ0\"][event_index][track_index])\n",
    "#         node_features = np.append(node_features, event_data[\"trackEta_EMB2\"][event_index][track_index])\n",
    "#         node_features = np.append(node_features, event_data[\"trackPhi_EMB2\"][event_index][track_index])\n",
    "#         node_features = np.reshape(node_features, (len(node_features))).T\n",
    "\n",
    "#         # add dummy placeholder nodes for track features (not used in track cell nodes)\n",
    "#         node_features = np.hstack((np.zeros(7), node_features))\n",
    "\n",
    "#         return node_features\n",
    "\n",
    "#     def get_track_edges(self, num_track_nodes, start_index):\n",
    "#         \"\"\"\n",
    "#         Creates the edge senders and recievers and edge features\n",
    "#         Inputs:\n",
    "#         (int) num_track_nodes: number of track nodes\n",
    "#         (int) start_index: the index of senders/recievers to start with. We should start with num_cluster_edges+1 to avoid overlap\n",
    "\n",
    "#         Returns:\n",
    "#         (np.array) edge_features:\n",
    "#         (np.array) senders:\n",
    "#         (np.array) recievers:\n",
    "#         \"\"\"\n",
    "#         # Full Connected tracks\n",
    "#         # since we are fully connected, the order of senders and recievers doesn't matter\n",
    "#         # we just need to count each node - edges will have a placeholder feature\n",
    "#         connections = list(itertools.permutations(range(start_index, start_index + num_track_nodes),2))\n",
    "#         for i in range(5):\n",
    "#             connections.append((i, i))\n",
    "\n",
    "#         senders = np.array([x[0] for x in connections])\n",
    "#         recievers = np.array([x[0] for x in connections])\n",
    "#         edge_features = np.zeros((len(connections), 10))\n",
    "\n",
    "#         return senders, recievers, edge_features\n",
    "\n",
    "#     # end track section ----------------------------------------------------------------\n",
    "\n",
    "    def get_edges(self, cluster_num_nodes, cell_IDmap):\n",
    "        \"\"\"\n",
    "        Reading edge features\n",
    "        Returns senders, receivers, and edges\n",
    "        \"\"\"\n",
    "\n",
    "        edge_inds = np.zeros((cluster_num_nodes, self.num_edgeFeatures))\n",
    "        for i, f in enumerate(self.edgeFeatureNames):\n",
    "            edge_inds[:, i] = self.cellGeo_data[f][0][cell_IDmap]\n",
    "        edge_inds[np.logical_not(np.isin(edge_inds, cell_IDmap))] = np.nan\n",
    "\n",
    "        senders, edge_on_inds = np.isin(edge_inds, cell_IDmap).nonzero()\n",
    "        cluster_num_edges = len(senders)\n",
    "        edges = np.zeros((cluster_num_edges, self.num_edgeFeatures))\n",
    "        edges[np.arange(cluster_num_edges), edge_on_inds] = 1\n",
    "\n",
    "        cell_IDmap_sorter = np.argsort(cell_IDmap)\n",
    "        rank = np.searchsorted(cell_IDmap, edge_inds , sorter=cell_IDmap_sorter)\n",
    "        receivers = cell_IDmap_sorter[rank[rank!=cluster_num_nodes]]\n",
    "\n",
    "        return senders, receivers, edges\n",
    "\n",
    "    def preprocessor(self, worker_id):\n",
    "        \"\"\"\n",
    "        Prerocessing root file data for faster data\n",
    "        generation during multiple training epochs\n",
    "        \"\"\"\n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            print(f\"Processing file number {file_num}\")\n",
    "\n",
    "            ### Pions\n",
    "            file = self.pion_file_list[file_num]\n",
    "            event_data = np.load(file, allow_pickle=True).item()\n",
    "            num_events = len(event_data[[key for key in event_data.keys()][0]])\n",
    "\n",
    "            preprocessed_data = []\n",
    "\n",
    "            for event_ind in range(num_events):\n",
    "                num_clusters = event_data['nCluster'][event_ind]\n",
    "                truth_particle_E = np.log10(event_data['truthPartE'][event_ind][0])\n",
    "                truth_particle_E_scaled = (truth_particle_E - scales['truth_part_e_mean'])/scales['truth_part_e_std']\n",
    "                truthPartPt = event_data['truthPartPt'][event_ind][0]\n",
    "                sum_cluster_E = np.sum(event_data['cluster_E'][event_ind]) # sum of all cluster energies in the event\n",
    "                sum_lcw_E = np.sum(event_data['cluster_E'][event_ind]*event_data['cluster_HAD_WEIGHT'][event_ind])\n",
    "                \n",
    "                for i in range(num_clusters):\n",
    "#                     if event_data['dR_pass'][event_ind][i] == False:\n",
    "#                         continue\n",
    "                    cluster_calib_E = self.get_cluster_calib(event_data, event_ind, i)\n",
    "                    cluster_EM_prob = event_data['cluster_EM_PROBABILITY'][event_ind][i]\n",
    "                    cluster_E_0 = np.log10(event_data['cluster_E'][event_ind][0])\n",
    "                    cluster_E_0_scaled = (cluster_E_0 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "\n",
    "                    cluster_E_1_scaled = np.array(0)\n",
    "                    cluster_E_2_scaled = np.array(0)\n",
    "                    cluster_E_3_scaled = np.array(0)\n",
    "                    cluster_E_4_scaled = np.array(0)\n",
    "                    cluster_E_5_scaled = np.array(0)\n",
    "                    cluster_E_6_scaled = np.array(0)\n",
    "                    cluster_E_7_scaled = np.array(0)\n",
    "                    cluster_E_8_scaled = np.array(0)\n",
    "                    cluster_E_9_scaled = np.array(0)\n",
    "\n",
    "                    if num_clusters > 1:\n",
    "                        cluster_E_1 = np.log10(event_data['cluster_E'][event_ind][1])\n",
    "                        cluster_E_1_scaled = (cluster_E_1 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                        if num_clusters > 2:\n",
    "                            cluster_E_2 = np.log10(event_data['cluster_E'][event_ind][2])\n",
    "                            cluster_E_2_scaled = (cluster_E_2 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                            if num_clusters > 3:\n",
    "                                cluster_E_3 = np.log10(event_data['cluster_E'][event_ind][3])\n",
    "                                cluster_E_3_scaled = (cluster_E_3 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                                if num_clusters > 4:\n",
    "                                    cluster_E_4 = np.log10(event_data['cluster_E'][event_ind][4])\n",
    "                                    cluster_E_4_scaled = (cluster_E_4 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                                    if num_clusters > 5:\n",
    "                                        cluster_E_5 = np.log10(event_data['cluster_E'][event_ind][5])\n",
    "                                        cluster_E_5_scaled = (cluster_E_5 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                                        if num_clusters > 6:\n",
    "                                            cluster_E_6 = np.log10(event_data['cluster_E'][event_ind][6])\n",
    "                                            cluster_E_6_scaled = (cluster_E_6 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                                            if num_clusters > 7:\n",
    "                                                cluster_E_7 = np.log10(event_data['cluster_E'][event_ind][7])\n",
    "                                                cluster_E_7_scaled = (cluster_E_7 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                                                if num_clusters > 8:\n",
    "                                                    cluster_E_8 = np.log10(event_data['cluster_E'][event_ind][8])\n",
    "                                                    cluster_E_8_scaled = (cluster_E_8 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                                                    if num_clusters > 9:\n",
    "                                                        cluster_E_9 = np.log10(event_data['cluster_E'][event_ind][9])\n",
    "                                                        cluster_E_9_scaled = (cluster_E_9 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "\n",
    "                    cluster_HAD_WEIGHT = event_data['cluster_HAD_WEIGHT'][event_ind][i]\n",
    "\n",
    "                    if cluster_calib_E is None:\n",
    "                        continue\n",
    "\n",
    "                    cluster_eta = self.get_cluster_eta(event_data, event_ind, i)\n",
    "\n",
    "                    nodes, global_node, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, i)\n",
    "                    senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
    "\n",
    "#                 # track section ----------------------------------------------------------------\n",
    "#                     track_nodes = np.empty((0, 11))\n",
    "                    num_tracks = event_data['nTrack'][event_ind]\n",
    "                    if num_tracks > 0:\n",
    "                        for track_index in range(num_tracks):\n",
    "    #                         np.append(track_nodes, self.get_track_node(event_data, event_ind, track_index).reshape(1, -1), axis=0)\n",
    "                            track_pt = np.log10(event_data[\"trackPt\"][event_ind][track_index])\n",
    "                            track_z0 = event_data[\"trackZ0\"][event_ind][track_index]\n",
    "                            track_eta = event_data[\"trackEta\"][event_ind][track_index]\n",
    "                            track_phi = event_data[\"trackPhi\"][event_ind][track_index]\n",
    "\n",
    "                        track_pt_scaled = (track_pt - scales['track_pt_mean'])/scales['track_pt_std']\n",
    "                        track_z0_scaled = (track_z0 - scales['track_z0_mean'])/scales['track_z0_std']\n",
    "                        track_eta_scaled = (track_eta - scales['track_eta_mean'])/scales['track_eta_std']\n",
    "                        track_phi_scaled = (track_phi - scales['track_phi_mean'])/scales['track_phi_std']\n",
    "\n",
    "                    else:\n",
    "                        track_pt = np.array(0)\n",
    "                        track_eta = np.array(0)\n",
    "                        track_pt_scaled = np.array(0)\n",
    "                        track_z0_scaled = np.array(0)\n",
    "                        track_eta_scaled = np.array(0)\n",
    "                        track_phi_scaled = np.array(0)\n",
    "\n",
    "#                     track_senders, track_receivers, track_edge_features = self.get_track_edges(len(track_nodes), cluster_num_nodes)\n",
    "\n",
    "#                     # append on the track nodes and edges to the cluster ones\n",
    "#                     nodes = np.append(nodes, np.array(track_nodes), axis=0)\n",
    "#                     edges = np.append(edges, track_edge_features, axis=0)\n",
    "#                     senders = np.append(senders, track_senders, axis=0)\n",
    "#                     receivers = np.append(receivers, track_receivers, axis=0)\n",
    "\n",
    "#                     # end track section ----------------------------------------------------------------\n",
    "\n",
    "                    globals_list = np.array([\n",
    "                                             cluster_E_0_scaled.astype(np.float32),\n",
    "                                             cluster_E_1_scaled.astype(np.float32),\n",
    "                                             cluster_E_2_scaled.astype(np.float32),\n",
    "                                             cluster_E_3_scaled.astype(np.float32),\n",
    "                                             cluster_E_4_scaled.astype(np.float32),\n",
    "                                             cluster_E_5_scaled.astype(np.float32),\n",
    "                                             cluster_E_6_scaled.astype(np.float32),\n",
    "                                             cluster_E_7_scaled.astype(np.float32),\n",
    "                                             cluster_E_8_scaled.astype(np.float32),\n",
    "                                             cluster_E_9_scaled.astype(np.float32),\n",
    "                                             track_pt_scaled.astype(np.float32),\n",
    "                                             track_z0_scaled.astype(np.float32),\n",
    "                                             track_eta_scaled.astype(np.float32),\n",
    "                                             track_phi_scaled.astype(np.float32),\n",
    "                                             ])\n",
    "\n",
    "                    graph = {'nodes': nodes.astype(np.float32),\n",
    "                            'globals': globals_list,\n",
    "                            'senders': senders.astype(np.int32),\n",
    "                            'receivers': receivers.astype(np.int32),\n",
    "                            'edges': edges.astype(np.float32),\n",
    "                            'cluster_calib_E': cluster_calib_E.astype(np.float32),\n",
    "                            'cluster_eta': cluster_eta.astype(np.float32), 'cluster_EM_prob': cluster_EM_prob.astype(np.float32),\n",
    "                            'cluster_E_0': cluster_E_0.astype(np.float32), 'cluster_HAD_WEIGHT': cluster_HAD_WEIGHT.astype(np.float32),\n",
    "                            'truthPartPt': truthPartPt.astype(np.float32), 'truthPartE': truth_particle_E.astype(np.float32),\n",
    "                             'track_pt': track_pt.astype(np.float32), 'track_eta': track_eta.astype(np.float32), \n",
    "                            'sum_cluster_E': sum_cluster_E.astype(np.float32), 'sum_lcw_E': sum_lcw_E.astype(np.float32)}\n",
    "                    target = np.reshape([truth_particle_E_scaled.astype(np.float32), 1], [1,2])\n",
    "                    preprocessed_data.append((graph, target))\n",
    "\n",
    "#             ### Pi0\n",
    "#             file = self.pi0_file_list[file_num]\n",
    "#             event_data = np.load(file, allow_pickle=True).item()\n",
    "#             num_events = len(event_data[[key for key in event_data.keys()][0]])\n",
    "\n",
    "#             for event_ind in range(num_events):\n",
    "#                 num_clusters = event_data['nCluster'][event_ind]\n",
    "#                 truth_particle_E = np.log10(event_data['truthPartE'][event_ind][0]) # first one is the pion!\n",
    "#                 truthPartPt = event_data['truthPartPt'][event_ind][0]\n",
    "\n",
    "#                 for i in range(num_clusters):\n",
    "#                     cluster_calib_E = self.get_cluster_calib(event_data, event_ind, i)\n",
    "#                     cluster_EM_prob = event_data['cluster_EM_PROBABILITY'][event_ind][i]\n",
    "#                     cluster_E = np.log10(event_data['cluster_E'][event_ind][i])\n",
    "#                     cluster_HAD_WEIGHT = event_data['cluster_HAD_WEIGHT'][event_ind][i]\n",
    "\n",
    "#                     if event_data['dR_pass'][event_ind][i] == False:\n",
    "#                         continue\n",
    "\n",
    "#                     if cluster_calib_E is None:\n",
    "#                         continue\n",
    "\n",
    "#                     cluster_eta = self.get_cluster_eta(event_data, event_ind, i)\n",
    "\n",
    "#                     nodes, global_node, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, i)\n",
    "#                     senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
    "\n",
    "#                     # track section ----------------------------------------------------------------\n",
    "#                     track_nodes = np.empty((0, 11))\n",
    "#                     num_tracks = event_data['nTrack'][event_ind]\n",
    "#                     for track_index in range(num_tracks):\n",
    "#                         np.append(track_nodes, self.get_track_node(event_data, event_ind, track_index).reshape(1, -1), axis=0)\n",
    "#                         track_pt = np.array([np.log10(event_data[\"trackPt\"][event_ind][track_index])])\n",
    "\n",
    "#                     track_senders, track_receivers, track_edge_features = self.get_track_edges(len(track_nodes), cluster_num_nodes)\n",
    "\n",
    "#                     nodes = np.append(nodes, np.array(track_nodes), axis=0)\n",
    "#                     edges = np.append(edges, track_edge_features, axis=0)\n",
    "#                     senders = np.append(senders, track_senders, axis=0)\n",
    "#                     receivers = np.append(receivers, track_receivers, axis=0)\n",
    "\n",
    "#                     # end track section ----------------------------------------------------------------\n",
    "\n",
    "#                     graph = {'nodes': nodes.astype(np.float32),\n",
    "# #                              'globals': global_node.astype(np.float32),\n",
    "#                              'globals': track_pt.astype(np.float32),\n",
    "#                         'senders': senders.astype(np.int32), 'receivers': receivers.astype(np.int32),\n",
    "#                         'edges': edges.astype(np.float32), 'cluster_calib_E': cluster_calib_E.astype(np.float32),\n",
    "#                         'cluster_eta': cluster_eta.astype(np.float32), 'cluster_EM_prob': cluster_EM_prob.astype(np.float32),\n",
    "#                         'cluster_E': cluster_E.astype(np.float32), 'cluster_HAD_WEIGHT': cluster_HAD_WEIGHT.astype(np.float32),\n",
    "#                         'truthPartPt': truthPartPt.astype(np.float32), 'track_pt': track_pt.astype(np.float32)}\n",
    "#                     target = np.reshape([truth_particle_E.astype(np.float32), 0], [1,2])\n",
    "\n",
    "#                     preprocessed_data.append((graph, target))\n",
    "\n",
    "            random.shuffle(preprocessed_data)\n",
    "\n",
    "            pickle.dump(preprocessed_data, open(self.output_dir + f'data_{file_num:03d}.p', 'wb'), compression='gzip')\n",
    "\n",
    "            print(f\"Finished processing {file_num} files\")\n",
    "            file_num += self.num_procs\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        print('\\nPreprocessing and saving data to {}'.format(self.output_dir))\n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.preprocessor, args=(i,), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "\n",
    "        for p in self.procs:\n",
    "            p.join()\n",
    "\n",
    "        self.file_list = [self.output_dir + f'data_{i:03d}.p' for i in range(self.num_files)]\n",
    "\n",
    "    def preprocessed_worker(self, worker_id, batch_queue):\n",
    "        batch_graphs = []\n",
    "        batch_targets = []\n",
    "\n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
    "\n",
    "            for i in range(len(file_data)):\n",
    "                batch_graphs.append(file_data[i][0])\n",
    "                batch_targets.append(file_data[i][1])\n",
    "\n",
    "                if len(batch_graphs) == self.batch_size:\n",
    "                    batch_targets = np.reshape(np.array(batch_targets), [-1,2]).astype(np.float32)\n",
    "\n",
    "                    batch_queue.put((batch_graphs, batch_targets))\n",
    "\n",
    "                    batch_graphs = []\n",
    "                    batch_targets = []\n",
    "\n",
    "            file_num += self.num_procs\n",
    "\n",
    "        if len(batch_graphs) > 0:\n",
    "            batch_targets = np.reshape(np.array(batch_targets), [-1,2]).astype(np.float32)\n",
    "            batch_queue.put((batch_graphs, batch_targets))\n",
    "\n",
    "    def worker(self, worker_id, batch_queue):\n",
    "        if self.preprocess:\n",
    "            self.preprocessed_worker(worker_id, batch_queue)\n",
    "        else:\n",
    "            raise Exception('Preprocessing is required for combined classification/regression models.')\n",
    "\n",
    "    def check_procs(self):\n",
    "        for p in self.procs:\n",
    "            if p.is_alive(): return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def kill_procs(self):\n",
    "        for p in self.procs:\n",
    "            p.kill()\n",
    "\n",
    "        self.procs = []\n",
    "\n",
    "    def generator(self):\n",
    "        \"\"\"\n",
    "        Generator that returns processed batches during training\n",
    "        \"\"\"\n",
    "        batch_queue = Queue(2 * self.num_procs)\n",
    "\n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.worker, args=(i, batch_queue), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "\n",
    "        while self.check_procs() or not batch_queue.empty():\n",
    "            try:\n",
    "                batch = batch_queue.get(True, 0.0001)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            yield batch\n",
    "\n",
    "        for p in self.procs:\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the data generation step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/clusterfs/ml4hep/mpettee/ml4pions/data/onetrack_multicluster/'\n",
    "pi0_files = [] #np.sort(glob.glob(data_dir+'pi0_files/*.npy'))[:1]\n",
    "pion_files = np.sort(glob.glob(data_dir+'pion_files/*.npy'))[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing and saving data to /clusterfs/ml4hep/mpettee/ml4pions/data/onetrack_multicluster/\n",
      "Processing file number 0\n",
      "Finished processing 0 files\n"
     ]
    }
   ],
   "source": [
    "data_gen = GraphDataGenerator(pion_file_list=pion_files, \n",
    "                              pi0_file_list=pi0_files,\n",
    "                              cellGeo_file='/clusterfs/ml4hep/mpettee/ml4pions/data/cell_geo.root',\n",
    "                              batch_size=32,\n",
    "                              shuffle=False,\n",
    "                              num_procs=32,\n",
    "                              preprocess=True,\n",
    "                              output_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tuple(graphs):\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    global_nodes = []\n",
    "    senders = []\n",
    "    receivers = []\n",
    "    n_node = []\n",
    "    n_edge = []\n",
    "    offset = 0\n",
    "    cluster_energies = []\n",
    "    sum_cluster_energies = []\n",
    "    cluster_etas = []\n",
    "    cluster_EM_probs = []\n",
    "    cluster_calib_Es = []\n",
    "    cluster_had_weights = []\n",
    "    truth_particle_es = []\n",
    "    truth_particle_pts = []\n",
    "    track_pts = []\n",
    "    track_etas = []\n",
    "\n",
    "    for graph in graphs:\n",
    "        nodes.append(graph['nodes'])\n",
    "        edges.append(graph['edges'])\n",
    "        global_nodes.append([graph['globals']])\n",
    "        senders.append(graph['senders'] + offset)\n",
    "        receivers.append(graph['receivers'] + offset)\n",
    "        n_node.append(graph['nodes'].shape[:1])\n",
    "        n_edge.append(graph['edges'].shape[:1])\n",
    "        cluster_energies.append(graph['cluster_E_0'])\n",
    "        sum_cluster_energies.append(graph['sum_cluster_E'])\n",
    "        cluster_etas.append(graph['cluster_eta'])\n",
    "        cluster_EM_probs.append(graph['cluster_EM_prob'])\n",
    "        cluster_calib_Es.append(graph['cluster_calib_E'])\n",
    "        cluster_had_weights.append(graph['cluster_HAD_WEIGHT'])\n",
    "        truth_particle_es.append(graph['truthPartE'])\n",
    "        truth_particle_pts.append(graph['truthPartPt'])\n",
    "        track_pts.append(graph['track_pt'])\n",
    "        track_etas.append(graph['track_eta'])\n",
    "                                      \n",
    "        offset += len(graph['nodes'])\n",
    "\n",
    "    nodes = tf.convert_to_tensor(np.concatenate(nodes))\n",
    "    edges = tf.convert_to_tensor(np.concatenate(edges))\n",
    "    global_nodes = tf.convert_to_tensor(np.concatenate(global_nodes))\n",
    "    senders = tf.convert_to_tensor(np.concatenate(senders))\n",
    "    receivers = tf.convert_to_tensor(np.concatenate(receivers))\n",
    "    n_node = tf.convert_to_tensor(np.concatenate(n_node))\n",
    "    n_edge = tf.convert_to_tensor(np.concatenate(n_edge))\n",
    "\n",
    "    graph = GraphsTuple(\n",
    "            nodes=nodes,\n",
    "            edges=edges,\n",
    "            globals=global_nodes,\n",
    "            senders=senders,\n",
    "            receivers=receivers,\n",
    "            n_node=n_node,\n",
    "            n_edge=n_edge\n",
    "        )\n",
    "\n",
    "    return graph, cluster_energies, cluster_etas, cluster_EM_probs, cluster_calib_Es, cluster_had_weights, truth_particle_es, truth_particle_pts, track_pts, track_etas, sum_cluster_energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_27133/930695881.py\", line 411, in worker\n",
      "    self.preprocessed_worker(worker_id, batch_queue)\n",
      "  File \"/tmp/ipykernel_27133/930695881.py\", line 398, in preprocessed_worker\n",
      "    batch_queue.put((batch_graphs, batch_targets))\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/queues.py\", line 89, in put\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 318, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/queues.py\", line 201, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/threading.py\", line 1053, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/threading.py\", line 1069, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Get batch of data\n",
    "from graph_nets.graphs import GraphsTuple\n",
    "def get_batch(data_iter):\n",
    "    for graphs, targets in data_iter:\n",
    "        targets = tf.convert_to_tensor(targets)\n",
    "        graphs, energies, etas, em_probs, cluster_calib_es, cluster_had_weights, truth_particle_es, truth_particle_pts, track_pts, track_etas, sum_cluster_es = convert_to_tuple(graphs)\n",
    "        yield graphs, targets, energies, etas, em_probs, cluster_calib_es, cluster_had_weights, truth_particle_es, truth_particle_pts, track_pts, track_etas, sum_cluster_es\n",
    "\n",
    "samp_graph, samp_target, _, _, _, _, _, _, _, _, _, _ = next(get_batch(data_gen.generator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphsTuple(nodes=<tf.Tensor: shape=(2238, 7), dtype=float32, numpy=\n",
       "array([[ 3.8148108 , -0.47558048,  0.5360982 , ...,  0.5160702 ,\n",
       "        -0.04745933, -1.1441729 ],\n",
       "       [ 3.533929  , -0.47558048,  0.536122  , ...,  0.51586145,\n",
       "        -0.04745933, -1.1441729 ],\n",
       "       [ 2.555583  , -0.47558048,  0.53607404, ...,  0.51628166,\n",
       "        -0.04745933, -1.1441729 ],\n",
       "       ...,\n",
       "       [ 0.76535374,  3.8186247 , -0.9893554 , ...,  3.2007487 ,\n",
       "         2.1609297 ,  0.76402265],\n",
       "       [-0.51270485,  1.2926216 , -1.0596395 , ...,  1.5341085 ,\n",
       "         2.1609297 ,  0.76402265],\n",
       "       [-1.0710179 ,  3.8186247 , -0.9893554 , ...,  3.2007487 ,\n",
       "         2.1609297 ,  0.76402265]], dtype=float32)>, edges=<tf.Tensor: shape=(7045, 10), dtype=float32, numpy=\n",
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, receivers=<tf.Tensor: shape=(7045,), dtype=int32, numpy=array([   1,    2,    3, ..., 2231, 2235, 2232], dtype=int32)>, senders=<tf.Tensor: shape=(7045,), dtype=int32, numpy=array([   0,    0,    0, ..., 2236, 2237, 2237], dtype=int32)>, globals=<tf.Tensor: shape=(32, 14), dtype=float32, numpy=\n",
       "array([[ 1.85074544e+00,  1.36572480e+00, -2.65319973e-01,\n",
       "        -7.90317774e-01, -1.04889774e+00, -1.58899546e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  3.15704656e+00, -3.86353917e-02,\n",
       "         5.94621241e-01,  1.28698814e+00],\n",
       "       [ 7.83981502e-01,  6.77689016e-01,  1.93267521e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  5.89795470e-01,  5.43736756e-01,\n",
       "        -1.05240434e-01,  6.83837473e-01],\n",
       "       [ 1.11237359e+00,  1.07362211e+00, -6.39609098e-01,\n",
       "        -6.57871902e-01, -1.40410388e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  9.91299450e-01, -4.88589913e-01,\n",
       "         2.35036671e-01, -1.10611641e+00],\n",
       "       [ 1.81276309e+00,  1.65642881e+00,  5.23178339e-01,\n",
       "         3.71671915e-01, -2.14039795e-02, -6.00877404e-01,\n",
       "        -1.10891736e+00, -1.14843237e+00, -1.25900352e+00,\n",
       "         0.00000000e+00,  1.34150207e+00, -4.51347083e-01,\n",
       "         6.08940542e-01,  7.91464210e-01],\n",
       "       [ 1.52507317e+00,  9.48880672e-01,  5.75880051e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  4.19378132e-01,  7.00462997e-01,\n",
       "         1.75029719e+00,  7.91218758e-01],\n",
       "       [ 1.85202491e+00,  1.17432094e+00,  3.55047137e-01,\n",
       "         2.43675500e-01, -6.21451288e-02, -8.49821448e-01,\n",
       "        -9.29540157e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.20819342e+00,  3.15769643e-01,\n",
       "        -1.09179711e+00, -6.67112648e-01],\n",
       "       [ 1.55813992e+00,  1.53880656e+00,  1.06456327e+00,\n",
       "         8.77193630e-01,  3.36521506e-01,  1.03753038e-01,\n",
       "         3.90686467e-02, -2.49932006e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.30213320e+00, -6.37895912e-02,\n",
       "        -1.03607655e+00,  1.42543507e+00],\n",
       "       [ 1.68937135e+00,  1.11064124e+00,  1.10670283e-01,\n",
       "         7.01500773e-02, -1.36610359e-01, -9.30632651e-01,\n",
       "        -1.01242888e+00, -1.04247212e+00, -1.04368782e+00,\n",
       "        -1.37787998e+00,  2.24465346e+00,  3.70492727e-01,\n",
       "         5.70262253e-01, -7.05494940e-01],\n",
       "       [ 1.37255442e+00,  1.03129005e+00,  6.13754332e-01,\n",
       "         1.79940574e-02,  2.00163515e-04, -8.81223142e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  8.17841411e-01, -4.77605313e-01,\n",
       "        -1.01522422e+00, -5.92844248e-01],\n",
       "       [ 1.79370475e+00,  1.78618312e+00,  1.26582170e+00,\n",
       "         8.13722193e-01,  4.08363402e-01, -1.99311331e-01,\n",
       "        -3.98375690e-01, -6.32734418e-01, -6.92988813e-01,\n",
       "        -1.39360857e+00,  1.46239889e+00, -1.48678536e-03,\n",
       "         1.32518840e+00, -1.44476080e+00],\n",
       "       [ 1.60193026e+00,  1.31671643e+00,  5.63104808e-01,\n",
       "        -8.40941966e-01, -9.80163455e-01, -1.30084467e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.44002056e+00,  1.46979421e-01,\n",
       "        -5.05955696e-01,  4.77996200e-01],\n",
       "       [ 1.01040840e+00,  9.85078037e-01,  9.68949616e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  9.56711650e-01, -9.59564671e-02,\n",
       "        -5.74043453e-01, -3.63746375e-01],\n",
       "       [-5.81138730e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.79879713e+00, -1.24498689e+00,\n",
       "         1.76717055e+00,  1.14896321e+00],\n",
       "       [ 1.20671082e+00,  8.73233080e-01,  5.00222325e-01,\n",
       "         3.94080490e-01,  3.01065475e-01,  2.31519938e-01,\n",
       "        -4.77195112e-03, -2.20945105e-02, -8.23793650e-01,\n",
       "        -1.12258911e+00,  7.31112957e-01,  5.69625497e-01,\n",
       "         1.10818243e+00,  1.38147378e+00],\n",
       "       [ 8.46070647e-01, -4.78877276e-02, -3.83580446e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  4.86024290e-01, -1.14404000e-01,\n",
       "        -2.69234143e-02, -1.48441291e+00],\n",
       "       [ 9.01784599e-01,  3.55412096e-01,  2.58383304e-01,\n",
       "        -1.50288272e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  3.61899555e-01, -5.14739692e-01,\n",
       "        -9.74864721e-01,  7.55321145e-01],\n",
       "       [ 1.89873147e+00,  1.03569770e+00,  9.53129232e-01,\n",
       "         3.51497173e-01,  2.30272993e-01, -1.07184306e-01,\n",
       "        -2.20449150e-01, -4.04156148e-01, -5.05318701e-01,\n",
       "        -5.96754193e-01, -4.08207923e-01, -5.97455382e-01,\n",
       "         1.14832282e+00, -7.91378617e-01],\n",
       "       [ 4.85701829e-01, -1.07463360e+00, -1.47568154e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -4.27836120e-01,  4.52641964e-01,\n",
       "         6.84095323e-01,  1.55882025e+00],\n",
       "       [ 1.09090769e+00,  8.97659898e-01,  3.51687402e-01,\n",
       "         1.05356395e-01, -5.49049318e-01, -5.76818287e-01,\n",
       "        -5.86897671e-01, -9.57203269e-01, -1.31358004e+00,\n",
       "         0.00000000e+00,  5.02184987e-01,  2.02507615e+00,\n",
       "        -1.26285350e+00,  1.08241177e+00],\n",
       "       [ 3.09361517e-01,  1.40330791e-01, -3.98188084e-01,\n",
       "        -8.56648028e-01, -1.15085423e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.03941464e+00,  2.38228154e+00,\n",
       "         1.10928881e+00,  1.09425294e+00],\n",
       "       [ 1.22698784e+00,  8.47910583e-01,  9.76322964e-02,\n",
       "        -1.00001609e+00, -1.02735531e+00, -1.13402748e+00,\n",
       "        -1.66945922e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  9.10774887e-01, -6.91585243e-01,\n",
       "         5.81568301e-01,  3.22172552e-01],\n",
       "       [ 1.86695540e+00,  1.16229522e+00, -1.30238488e-01,\n",
       "        -3.52612942e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  8.70319128e-01, -1.32191747e-01,\n",
       "         1.84052670e+00,  4.85414378e-02],\n",
       "       [ 9.46110427e-01,  3.12574208e-01, -1.62180471e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -3.40246111e-02,  1.45206499e+00,\n",
       "         1.30695462e+00, -1.68813154e-01],\n",
       "       [ 5.16058743e-01, -1.55330503e+00, -1.60754299e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -5.74334599e-02,  4.89898920e-01,\n",
       "        -1.21054366e-01,  3.10393214e-01],\n",
       "       [ 1.75003159e+00,  1.69929934e+00,  1.65411448e+00,\n",
       "         2.91303784e-01,  2.50434309e-01,  8.23242292e-02,\n",
       "        -7.99385130e-01, -1.62299788e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.52497947e+00, -9.10239637e-01,\n",
       "         8.74589443e-01,  1.63149238e-01],\n",
       "       [ 1.58578181e+00,  9.65456128e-01,  7.52996027e-01,\n",
       "         5.83083570e-01,  4.13058817e-01, -2.47979730e-01,\n",
       "        -3.30623329e-01, -5.65760076e-01, -7.34303892e-01,\n",
       "        -7.86172390e-01,  9.19797599e-01,  1.10452282e+00,\n",
       "        -1.13606787e+00,  4.34264570e-01],\n",
       "       [-2.67502993e-01, -9.68279302e-01, -1.23284602e+00,\n",
       "        -1.35341001e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.36609077e+00, -2.01485634e-01,\n",
       "        -8.94318521e-01,  6.52360976e-01],\n",
       "       [ 1.64717250e-02, -1.31888930e-02, -5.19364834e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.85531825e-01, -5.78161716e-01,\n",
       "         4.96579446e-02, -1.47416866e+00],\n",
       "       [-6.63970411e-01, -8.73443902e-01, -1.06622171e+00,\n",
       "        -1.77341342e+00, -1.85401952e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.23558736e+00, -8.82304251e-01,\n",
       "        -6.37301505e-01,  1.57902002e+00],\n",
       "       [ 2.37016022e-01,  4.15315628e-02, -2.27143988e-01,\n",
       "        -2.32279733e-01, -2.60263622e-01, -3.95264298e-01,\n",
       "        -5.18804133e-01, -6.02842271e-01, -8.28387558e-01,\n",
       "        -1.46841741e+00, -1.85599282e-01, -1.09178591e+00,\n",
       "        -1.08972502e+00,  1.14141929e+00],\n",
       "       [ 3.99562836e-01, -1.82646364e-01, -1.90965667e-01,\n",
       "        -5.64567566e-01, -8.00657630e-01, -8.66366267e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.17837049e-01,  2.01491900e-02,\n",
       "         6.34791434e-01, -8.17843497e-01],\n",
       "       [ 1.92353451e+00, -8.22058439e-01, -8.65134060e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.23712981e+00, -1.65874493e+00,\n",
       "        -1.31480241e+00, -1.64326286e+00]], dtype=float32)>, n_node=<tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
       "array([414,  67,  21,  56, 196,  57,  21, 219, 150,  12,   6, 177,  14,\n",
       "        60,  69,   7,  17,  26, 101,  18, 257,  42,  51,   7,  75,   1,\n",
       "        10,  37,  29,   1,  13,   7])>, n_edge=<tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
       "array([1514,  185,   49,  138,  709,  163,   53,  756,  543,   24,   10,\n",
       "        539,   25,  155,  180,   13,   36,   61,  312,   38,  887,  140,\n",
       "        123,   15,  210,    0,   14,   70,   57,    0,   15,   11])>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "nbdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
