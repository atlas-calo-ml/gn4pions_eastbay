{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modules.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import uproot as ur\n",
    "import time\n",
    "from multiprocessing import Process, Queue, set_start_method\n",
    "import compress_pickle as pickle\n",
    "from scipy.stats import circmean\n",
    "import random\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt # optional\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "})\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphDataGenerator\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../../data/onetrack_multicluster/pion_files/001.npy\"\n",
    "event_data = np.load(file, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(pd.DataFrame(event_data).trackEta.explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "bins = np.linspace(0,2500,30)\n",
    "# plt.hist(np.concatenate(event_data['truthPartE']).ravel(), color='black', bins=bins, histtype='step', label=\"truthPartE\");\n",
    "plt.hist(np.concatenate(event_data['cluster_E']).ravel(), bins=bins, histtype='step', label=\"cluster_E\");\n",
    "plt.hist(np.array(event_data['trackPt']), bins=bins, histtype='step', label='trackPt');\n",
    "plt.legend();\n",
    "plt.yscale('log');\n",
    "plt.xlabel('[MeV]');\n",
    "\n",
    "deltas = []\n",
    "for i in range(len(event_data['cluster_E'])):\n",
    "    for j in range(event_data['nCluster'][i]):\n",
    "        deltas.append(np.abs(event_data['trackPt'][i]-event_data['cluster_E'][i][j])/event_data['trackPt'][i])\n",
    "        \n",
    "plt.figure(dpi=150)\n",
    "plt.hist(np.concatenate(deltas), bins=np.linspace(0,1, 30));\n",
    "plt.xlabel(\"abs(trackPt - clusterE) / trackPt\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "scales = {\n",
    "     'track_pt_mean': 1.633278727,\n",
    "     'track_pt_std': 0.8481947183,\n",
    "     'track_z0_mean': 0.08022017,\n",
    "     'track_z0_std': 42.53320004,\n",
    "     'track_eta_mean': -0.00563187,\n",
    "     'track_eta_std': 1.35242735,\n",
    "     'track_phi_mean': 0.00206431,\n",
    "     'track_phi_std': 1.81240248,\n",
    "     'truth_part_e_mean': 1.92469358,\n",
    "     'truth_part_e_std': 0.8289864,\n",
    "     'cluster_cell_e_mean': -1.0121697,\n",
    "     'cluster_cell_e_std': 0.818357,\n",
    "     'cluster_e_mean': 0.89923394,\n",
    "     'cluster_e_std': 1.0585934,\n",
    "     'cluster_eta_mean': 0.016195267,\n",
    "     'cluster_eta_std': 1.3400925,\n",
    "     'cluster_phi_mean': 0.0050816955,\n",
    "     'cluster_phi_std': 1.8100655,\n",
    "     'cell_geo_sampling_mean': 3.8827391420197177,\n",
    "     'cell_geo_sampling_std': 3.9588233603576204,\n",
    "     'cell_geo_eta_mean': 0.0005979097,\n",
    "     'cell_geo_eta_std': 1.4709069,\n",
    "     'cell_geo_phi_mean': -2.8938382e-05,\n",
    "     'cell_geo_phi_std': 1.813651,\n",
    "     'cell_geo_rPerp_mean': 1478.9285,\n",
    "     'cell_geo_rPerp_std': 434.60815,\n",
    "     'cell_geo_deta_mean': 0.026611786,\n",
    "     'cell_geo_deta_std': 0.03396141,\n",
    "     'cell_geo_dphi_mean': 0.068693615,\n",
    "     'cell_geo_dphi_std': 0.038586758}\n",
    "\n",
    "node_means = [\n",
    "    scales[\"cluster_cell_e_mean\"], \n",
    "    scales[\"cell_geo_sampling_mean\"],\n",
    "    scales[\"cell_geo_eta_mean\"],\n",
    "    scales[\"cell_geo_phi_mean\"],\n",
    "    scales[\"cell_geo_rPerp_mean\"],\n",
    "    scales[\"cell_geo_deta_mean\"],\n",
    "    scales[\"cell_geo_dphi_mean\"],\n",
    "] \n",
    "\n",
    "node_stds = [\n",
    "    scales[\"cluster_cell_e_std\"], \n",
    "    scales[\"cell_geo_sampling_std\"],\n",
    "    scales[\"cell_geo_eta_std\"],\n",
    "    scales[\"cell_geo_phi_std\"],\n",
    "    scales[\"cell_geo_rPerp_std\"],\n",
    "    scales[\"cell_geo_deta_std\"],\n",
    "    scales[\"cell_geo_dphi_std\"],\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot preprocessed track inputs \n",
    "track_pt = (np.log10(np.concatenate(event_data['trackPt'])) - scales['track_pt_mean'])/scales['track_pt_std']\n",
    "track_z0 = (np.concatenate(event_data['trackZ0']) - scales['track_z0_mean'])/scales['track_z0_std']\n",
    "track_eta = (np.concatenate(event_data['trackEta']) - scales['track_eta_mean'])/scales['track_eta_std']\n",
    "track_phi = (np.concatenate(event_data['trackPhi']) - scales['track_phi_mean'])/scales['track_phi_std']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(12,3), dpi=200, tight_layout=True)\n",
    "fig.suptitle(\"Scaled Track Inputs\")\n",
    "\n",
    "ax[0].hist(track_pt, bins=30)\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_xlabel(r'Scaled Track $p_T$')\n",
    "\n",
    "ax[1].hist(track_z0, bins=30)\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_xlabel(r'Scaled Track $Z_0$')\n",
    "\n",
    "ax[2].hist(track_eta, bins=30)\n",
    "ax[2].set_yscale('log')\n",
    "ax[2].set_xlabel(r'Scaled Track $\\eta$')\n",
    "\n",
    "ax[3].hist(track_phi, bins=30)\n",
    "ax[3].set_yscale('log')\n",
    "ax[3].set_xlabel(r'Scaled Track $\\phi$');\n",
    "# plt.savefig(\"scaled_track_inputs.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot preprocessed energy inputs \n",
    "\n",
    "all_nodes = []\n",
    "\n",
    "n_events = len(event_data['index'])\n",
    "for event_ind in range(n_events):\n",
    "    for cluster_ind in range(event_data['nCluster'][event_ind]):\n",
    "        nodes = event_data['cluster_cell_E'][event_ind][cluster_ind]\n",
    "#         cell_IDs = event_data['cluster_cell_ID'][event_ind][cluster_ind]\n",
    "#         cell_IDmap = self.sorter[np.searchsorted(self.cellGeo_ID, cell_IDs, sorter=self.sorter)]\n",
    "#         nodes = np.append(nodes, cell_geo_data['cell_geo_sampling'][0][cell_IDmap])\n",
    "        nodes = np.reshape(nodes, (1, -1)).T\n",
    "        all_nodes.append(nodes)\n",
    "\n",
    "truth_particle_e = (np.log10(np.concatenate(event_data['truthPartE'])) - scales['truth_part_e_mean'])/scales['truth_part_e_std']\n",
    "cluster_cell_e = (np.log10(np.concatenate(all_nodes)) - scales['cluster_cell_e_mean'])/scales['cluster_cell_e_std']\n",
    "cluster_e = (np.log10(np.concatenate(event_data['cluster_E'])) - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(8,3), dpi=200, tight_layout=True)\n",
    "fig.suptitle(\"Scaled Energy Inputs\")\n",
    "\n",
    "ax[0].hist(truth_particle_e, bins=30)\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_xlabel(r'Truth Particle $E$')\n",
    "\n",
    "ax[1].hist(cluster_e, bins=30)\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_xlabel(r'Cluster $E$')\n",
    "\n",
    "ax[2].hist(cluster_cell_e, bins=30)\n",
    "ax[2].set_yscale('log')\n",
    "ax[2].set_xlabel(r'Cluster Cell $E$');\n",
    "# plt.savefig(\"scaled_energy_inputs.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataGenerator:\n",
    "    \"\"\"\n",
    "    DataGenerator class for extracting and formating data from list of root files\n",
    "    This data generator uses the cell_geo file to create the input graph structure\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 pi0_file_list: list,\n",
    "                 pion_file_list: list,\n",
    "                 cellGeo_file: str,\n",
    "                 batch_size: int,\n",
    "                 shuffle: bool = True,\n",
    "                 num_procs = 32,\n",
    "                 preprocess = False,\n",
    "                 output_dir = None):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            self.pi0_file_list = pi0_file_list\n",
    "            self.pion_file_list = pion_file_list\n",
    "#             assert len(pi0_file_list) == len(pion_file_list)\n",
    "            self.num_files = len(self.pion_file_list)\n",
    "        else:\n",
    "            self.file_list = pion_file_list\n",
    "            self.num_files = len(self.file_list)\n",
    "\n",
    "        self.cellGeo_file = cellGeo_file\n",
    "\n",
    "        self.cellGeo_data = ur.open(self.cellGeo_file)['CellGeo']\n",
    "        self.geoFeatureNames = self.cellGeo_data.keys()[1:9]\n",
    "        self.nodeFeatureNames = ['cluster_cell_E', *self.geoFeatureNames[:-2]]\n",
    "        self.edgeFeatureNames = self.cellGeo_data.keys()[9:]\n",
    "        self.num_nodeFeatures = len(self.nodeFeatureNames)\n",
    "        self.num_edgeFeatures = len(self.edgeFeatureNames)\n",
    "        self.cellGeo_data = self.cellGeo_data.arrays(library='np')\n",
    "        self.cellGeo_ID = self.cellGeo_data['cell_geo_ID'][0]\n",
    "        self.sorter = np.argsort(self.cellGeo_ID)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        if self.shuffle: np.random.shuffle(self.file_list)\n",
    "\n",
    "        self.num_procs = np.min([num_procs, self.num_files])\n",
    "        self.procs = []\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            os.makedirs(self.output_dir, exist_ok=True)\n",
    "            self.preprocess_data()\n",
    "\n",
    "    def get_cluster_calib(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading cluster calibration energy \"\"\"\n",
    "        cluster_calib_E = event_data['cluster_ENG_CALIB_TOT'][event_ind][cluster_ind]\n",
    "\n",
    "        if cluster_calib_E <= 0:\n",
    "            return None\n",
    "\n",
    "        return np.log10(cluster_calib_E)\n",
    "\n",
    "    def get_cluster_eta(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading cluster eta \"\"\"\n",
    "        cluster_eta = event_data['cluster_Eta'][event_ind][cluster_ind]\n",
    "        return cluster_eta\n",
    "\n",
    "    def get_nodes(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading Node features \"\"\"\n",
    "\n",
    "        cell_IDs = event_data['cluster_cell_ID'][event_ind][cluster_ind]\n",
    "        cell_IDmap = self.sorter[np.searchsorted(self.cellGeo_ID, cell_IDs, sorter=self.sorter)]\n",
    "\n",
    "        global_node = np.log10(event_data['cluster_E'][event_ind][cluster_ind])\n",
    "        nodes = np.log10(event_data['cluster_cell_E'][event_ind][cluster_ind])\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_sampling'][0][cell_IDmap])\n",
    "        for f in self.nodeFeatureNames[2:4]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_rPerp'][0][cell_IDmap])\n",
    "        for f in self.nodeFeatureNames[5:]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "\n",
    "        nodes = np.reshape(nodes, (len(self.nodeFeatureNames), -1)).T\n",
    "        nodes_scaled = (np.concatenate(nodes) - scales['cluster_cell_e_mean'])/scales['cluster_cell_e_std']\n",
    "        nodes_scaled = np.reshape(nodes, (len(self.nodeFeatureNames), -1)).T\n",
    "\n",
    "        nodes_scaled = (nodes - node_means)/node_stds\n",
    "        cluster_num_nodes = len(nodes)\n",
    "\n",
    "#         # add dummy placeholder nodes for track features (not used in cluster cell nodes)\n",
    "#         nodes = np.hstack((nodes, np.zeros((cluster_num_nodes, 4))))\n",
    "\n",
    "        return nodes_scaled, np.array([global_node]), cluster_num_nodes, cell_IDmap\n",
    "\n",
    "#     # track section ----------------------------------------------------------------\n",
    "\n",
    "#     def get_track_node(self, event_data, event_index, track_index):\n",
    "#         \"\"\"\n",
    "#         Creates node features for tracks\n",
    "#         Inputs:\n",
    "\n",
    "#         Returns:\n",
    "#             1 Dimensional array of node features for a single node\n",
    "#                 NOTE the cluster get_node function is a 2D array of multiple nodes\n",
    "#                 This function is used in a for loop so the end result is a 2D array\n",
    "#         \"\"\"\n",
    "#         node_features = np.array(event_data[\"trackPt\"][event_index][track_index])\n",
    "#         node_features = np.append(node_features, event_data[\"trackZ0\"][event_index][track_index])\n",
    "#         node_features = np.append(node_features, event_data[\"trackEta_EMB2\"][event_index][track_index])\n",
    "#         node_features = np.append(node_features, event_data[\"trackPhi_EMB2\"][event_index][track_index])\n",
    "#         node_features = np.reshape(node_features, (len(node_features))).T\n",
    "\n",
    "#         # add dummy placeholder nodes for track features (not used in track cell nodes)\n",
    "#         node_features = np.hstack((np.zeros(7), node_features))\n",
    "\n",
    "#         return node_features\n",
    "\n",
    "#     def get_track_edges(self, num_track_nodes, start_index):\n",
    "#         \"\"\"\n",
    "#         Creates the edge senders and recievers and edge features\n",
    "#         Inputs:\n",
    "#         (int) num_track_nodes: number of track nodes\n",
    "#         (int) start_index: the index of senders/recievers to start with. We should start with num_cluster_edges+1 to avoid overlap\n",
    "\n",
    "#         Returns:\n",
    "#         (np.array) edge_features:\n",
    "#         (np.array) senders:\n",
    "#         (np.array) recievers:\n",
    "#         \"\"\"\n",
    "#         # Full Connected tracks\n",
    "#         # since we are fully connected, the order of senders and recievers doesn't matter\n",
    "#         # we just need to count each node - edges will have a placeholder feature\n",
    "#         connections = list(itertools.permutations(range(start_index, start_index + num_track_nodes),2))\n",
    "#         for i in range(5):\n",
    "#             connections.append((i, i))\n",
    "\n",
    "#         senders = np.array([x[0] for x in connections])\n",
    "#         recievers = np.array([x[0] for x in connections])\n",
    "#         edge_features = np.zeros((len(connections), 10))\n",
    "\n",
    "#         return senders, recievers, edge_features\n",
    "\n",
    "#     # end track section ----------------------------------------------------------------\n",
    "\n",
    "    def get_edges(self, cluster_num_nodes, cell_IDmap):\n",
    "        \"\"\"\n",
    "        Reading edge features\n",
    "        Returns senders, receivers, and edges\n",
    "        \"\"\"\n",
    "\n",
    "        edge_inds = np.zeros((cluster_num_nodes, self.num_edgeFeatures))\n",
    "        for i, f in enumerate(self.edgeFeatureNames):\n",
    "            edge_inds[:, i] = self.cellGeo_data[f][0][cell_IDmap]\n",
    "        edge_inds[np.logical_not(np.isin(edge_inds, cell_IDmap))] = np.nan\n",
    "\n",
    "        senders, edge_on_inds = np.isin(edge_inds, cell_IDmap).nonzero()\n",
    "        cluster_num_edges = len(senders)\n",
    "        edges = np.zeros((cluster_num_edges, self.num_edgeFeatures))\n",
    "        edges[np.arange(cluster_num_edges), edge_on_inds] = 1\n",
    "\n",
    "        cell_IDmap_sorter = np.argsort(cell_IDmap)\n",
    "        rank = np.searchsorted(cell_IDmap, edge_inds , sorter=cell_IDmap_sorter)\n",
    "        receivers = cell_IDmap_sorter[rank[rank!=cluster_num_nodes]]\n",
    "\n",
    "        return senders, receivers, edges\n",
    "\n",
    "    def preprocessor(self, worker_id):\n",
    "        \"\"\"\n",
    "        Prerocessing root file data for faster data\n",
    "        generation during multiple training epochs\n",
    "        \"\"\"\n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            print(f\"Processing file number {file_num}\")\n",
    "\n",
    "            ### Pions\n",
    "            file = self.pion_file_list[file_num]\n",
    "            event_data = np.load(file, allow_pickle=True).item()\n",
    "            num_events = len(event_data[[key for key in event_data.keys()][0]])\n",
    "\n",
    "            preprocessed_data = []\n",
    "\n",
    "            for event_ind in range(num_events):\n",
    "                num_clusters = event_data['nCluster'][event_ind]\n",
    "                truth_particle_E = np.log10(event_data['truthPartE'][event_ind][0])\n",
    "                truth_particle_E_scaled = (truth_particle_E - scales['truth_part_e_mean'])/scales['truth_part_e_std']\n",
    "                truthPartPt = event_data['truthPartPt'][event_ind][0]\n",
    "                for i in range(num_clusters):\n",
    "#                     if event_data['dR_pass'][event_ind][i] == False:\n",
    "#                         continue\n",
    "                    cluster_calib_E = self.get_cluster_calib(event_data, event_ind, i)\n",
    "                    cluster_EM_prob = event_data['cluster_EM_PROBABILITY'][event_ind][i]\n",
    "                    cluster_E_0 = np.log10(event_data['cluster_E'][event_ind][0])\n",
    "                    cluster_E_0_scaled = (cluster_E_0 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "\n",
    "                    cluster_E_1_scaled = np.array(0)\n",
    "                    cluster_E_2_scaled = np.array(0)\n",
    "                    cluster_E_3_scaled = np.array(0)\n",
    "                    cluster_E_4_scaled = np.array(0)\n",
    "                    cluster_E_5_scaled = np.array(0)\n",
    "                    cluster_E_6_scaled = np.array(0)\n",
    "                    cluster_E_7_scaled = np.array(0)\n",
    "                    cluster_E_8_scaled = np.array(0)\n",
    "                    cluster_E_9_scaled = np.array(0)\n",
    "\n",
    "                    if num_clusters > 1:\n",
    "                        cluster_E_1 = np.log10(event_data['cluster_E'][event_ind][1])\n",
    "                        cluster_E_1_scaled = (cluster_E_1 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                        if num_clusters > 2:\n",
    "                            cluster_E_2 = np.log10(event_data['cluster_E'][event_ind][2])\n",
    "                            cluster_E_2_scaled = (cluster_E_2 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                            if num_clusters > 3:\n",
    "                                cluster_E_3 = np.log10(event_data['cluster_E'][event_ind][3])\n",
    "                                cluster_E_3_scaled = (cluster_E_3 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                                if num_clusters > 4:\n",
    "                                    cluster_E_4 = np.log10(event_data['cluster_E'][event_ind][4])\n",
    "                                    cluster_E_4_scaled = (cluster_E_4 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                                    if num_clusters > 5:\n",
    "                                        cluster_E_5 = np.log10(event_data['cluster_E'][event_ind][5])\n",
    "                                        cluster_E_5_scaled = (cluster_E_5 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                                        if num_clusters > 6:\n",
    "                                            cluster_E_6 = np.log10(event_data['cluster_E'][event_ind][6])\n",
    "                                            cluster_E_6_scaled = (cluster_E_6 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                                            if num_clusters > 7:\n",
    "                                                cluster_E_7 = np.log10(event_data['cluster_E'][event_ind][7])\n",
    "                                                cluster_E_7_scaled = (cluster_E_7 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                                                if num_clusters > 8:\n",
    "                                                    cluster_E_8 = np.log10(event_data['cluster_E'][event_ind][8])\n",
    "                                                    cluster_E_8_scaled = (cluster_E_8 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "                                                    if num_clusters > 9:\n",
    "                                                        cluster_E_9 = np.log10(event_data['cluster_E'][event_ind][9])\n",
    "                                                        cluster_E_9_scaled = (cluster_E_9 - scales['cluster_e_mean'])/scales['cluster_e_std']\n",
    "\n",
    "                    cluster_HAD_WEIGHT = event_data['cluster_HAD_WEIGHT'][event_ind][i]\n",
    "\n",
    "                    if cluster_calib_E is None:\n",
    "                        continue\n",
    "\n",
    "                    cluster_eta = self.get_cluster_eta(event_data, event_ind, i)\n",
    "\n",
    "                    nodes, global_node, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, i)\n",
    "                    senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
    "\n",
    "#                 # track section ----------------------------------------------------------------\n",
    "#                     track_nodes = np.empty((0, 11))\n",
    "                    num_tracks = event_data['nTrack'][event_ind]\n",
    "                    if num_tracks > 0:\n",
    "                        for track_index in range(num_tracks):\n",
    "    #                         np.append(track_nodes, self.get_track_node(event_data, event_ind, track_index).reshape(1, -1), axis=0)\n",
    "                            track_pt = np.log10(event_data[\"trackPt\"][event_ind][track_index])\n",
    "                            track_z0 = event_data[\"trackZ0\"][event_ind][track_index]\n",
    "                            track_eta = event_data[\"trackEta\"][event_ind][track_index]\n",
    "                            track_phi = event_data[\"trackPhi\"][event_ind][track_index]\n",
    "\n",
    "                        track_pt_scaled = (track_pt - scales['track_pt_mean'])/scales['track_pt_std']\n",
    "                        track_z0_scaled = (track_z0 - scales['track_z0_mean'])/scales['track_z0_std']\n",
    "                        track_eta_scaled = (track_eta - scales['track_eta_mean'])/scales['track_eta_std']\n",
    "                        track_phi_scaled = (track_phi - scales['track_phi_mean'])/scales['track_phi_std']\n",
    "\n",
    "                    else:\n",
    "                        track_pt = np.array(0)\n",
    "                        track_eta = np.array(0)\n",
    "                        track_pt_scaled = np.array(0)\n",
    "                        track_z0_scaled = np.array(0)\n",
    "                        track_eta_scaled = np.array(0)\n",
    "                        track_phi_scaled = np.array(0)\n",
    "\n",
    "#                     track_senders, track_receivers, track_edge_features = self.get_track_edges(len(track_nodes), cluster_num_nodes)\n",
    "\n",
    "#                     # append on the track nodes and edges to the cluster ones\n",
    "#                     nodes = np.append(nodes, np.array(track_nodes), axis=0)\n",
    "#                     edges = np.append(edges, track_edge_features, axis=0)\n",
    "#                     senders = np.append(senders, track_senders, axis=0)\n",
    "#                     receivers = np.append(receivers, track_receivers, axis=0)\n",
    "\n",
    "#                     # end track section ----------------------------------------------------------------\n",
    "\n",
    "                    globals_list = np.array([\n",
    "                                             cluster_E_0_scaled.astype(np.float32),\n",
    "                                             cluster_E_1_scaled.astype(np.float32),\n",
    "                                             cluster_E_2_scaled.astype(np.float32),\n",
    "                                             cluster_E_3_scaled.astype(np.float32),\n",
    "                                             cluster_E_4_scaled.astype(np.float32),\n",
    "                                             cluster_E_5_scaled.astype(np.float32),\n",
    "                                             cluster_E_6_scaled.astype(np.float32),\n",
    "                                             cluster_E_7_scaled.astype(np.float32),\n",
    "                                             cluster_E_8_scaled.astype(np.float32),\n",
    "                                             cluster_E_9_scaled.astype(np.float32),\n",
    "                                             track_pt_scaled.astype(np.float32),\n",
    "                                             track_z0_scaled.astype(np.float32),\n",
    "                                             track_eta_scaled.astype(np.float32),\n",
    "                                             track_phi_scaled.astype(np.float32),\n",
    "                                             ])\n",
    "\n",
    "                    graph = {'nodes': nodes.astype(np.float32),\n",
    "                            'globals': globals_list,\n",
    "                            'senders': senders.astype(np.int32),\n",
    "                            'receivers': receivers.astype(np.int32),\n",
    "                            'edges': edges.astype(np.float32),\n",
    "                            'cluster_calib_E': cluster_calib_E.astype(np.float32),\n",
    "                            'cluster_eta': cluster_eta.astype(np.float32), 'cluster_EM_prob': cluster_EM_prob.astype(np.float32),\n",
    "                            'cluster_E_0': cluster_E_0.astype(np.float32), 'cluster_HAD_WEIGHT': cluster_HAD_WEIGHT.astype(np.float32),\n",
    "                            'truthPartPt': truthPartPt.astype(np.float32), 'truthPartE': truth_particle_E.astype(np.float32),\n",
    "                             'track_pt': track_pt.astype(np.float32),\n",
    "                             'track_eta': track_eta.astype(np.float32)}\n",
    "                    target = np.reshape([truth_particle_E_scaled.astype(np.float32), 1], [1,2])\n",
    "                    preprocessed_data.append((graph, target))\n",
    "\n",
    "#             ### Pi0\n",
    "#             file = self.pi0_file_list[file_num]\n",
    "#             event_data = np.load(file, allow_pickle=True).item()\n",
    "#             num_events = len(event_data[[key for key in event_data.keys()][0]])\n",
    "\n",
    "#             for event_ind in range(num_events):\n",
    "#                 num_clusters = event_data['nCluster'][event_ind]\n",
    "#                 truth_particle_E = np.log10(event_data['truthPartE'][event_ind][0]) # first one is the pion!\n",
    "#                 truthPartPt = event_data['truthPartPt'][event_ind][0]\n",
    "\n",
    "#                 for i in range(num_clusters):\n",
    "#                     cluster_calib_E = self.get_cluster_calib(event_data, event_ind, i)\n",
    "#                     cluster_EM_prob = event_data['cluster_EM_PROBABILITY'][event_ind][i]\n",
    "#                     cluster_E = np.log10(event_data['cluster_E'][event_ind][i])\n",
    "#                     cluster_HAD_WEIGHT = event_data['cluster_HAD_WEIGHT'][event_ind][i]\n",
    "\n",
    "#                     if event_data['dR_pass'][event_ind][i] == False:\n",
    "#                         continue\n",
    "\n",
    "#                     if cluster_calib_E is None:\n",
    "#                         continue\n",
    "\n",
    "#                     cluster_eta = self.get_cluster_eta(event_data, event_ind, i)\n",
    "\n",
    "#                     nodes, global_node, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, i)\n",
    "#                     senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
    "\n",
    "#                     # track section ----------------------------------------------------------------\n",
    "#                     track_nodes = np.empty((0, 11))\n",
    "#                     num_tracks = event_data['nTrack'][event_ind]\n",
    "#                     for track_index in range(num_tracks):\n",
    "#                         np.append(track_nodes, self.get_track_node(event_data, event_ind, track_index).reshape(1, -1), axis=0)\n",
    "#                         track_pt = np.array([np.log10(event_data[\"trackPt\"][event_ind][track_index])])\n",
    "\n",
    "#                     track_senders, track_receivers, track_edge_features = self.get_track_edges(len(track_nodes), cluster_num_nodes)\n",
    "\n",
    "#                     nodes = np.append(nodes, np.array(track_nodes), axis=0)\n",
    "#                     edges = np.append(edges, track_edge_features, axis=0)\n",
    "#                     senders = np.append(senders, track_senders, axis=0)\n",
    "#                     receivers = np.append(receivers, track_receivers, axis=0)\n",
    "\n",
    "#                     # end track section ----------------------------------------------------------------\n",
    "\n",
    "#                     graph = {'nodes': nodes.astype(np.float32),\n",
    "# #                              'globals': global_node.astype(np.float32),\n",
    "#                              'globals': track_pt.astype(np.float32),\n",
    "#                         'senders': senders.astype(np.int32), 'receivers': receivers.astype(np.int32),\n",
    "#                         'edges': edges.astype(np.float32), 'cluster_calib_E': cluster_calib_E.astype(np.float32),\n",
    "#                         'cluster_eta': cluster_eta.astype(np.float32), 'cluster_EM_prob': cluster_EM_prob.astype(np.float32),\n",
    "#                         'cluster_E': cluster_E.astype(np.float32), 'cluster_HAD_WEIGHT': cluster_HAD_WEIGHT.astype(np.float32),\n",
    "#                         'truthPartPt': truthPartPt.astype(np.float32), 'track_pt': track_pt.astype(np.float32)}\n",
    "#                     target = np.reshape([truth_particle_E.astype(np.float32), 0], [1,2])\n",
    "\n",
    "#                     preprocessed_data.append((graph, target))\n",
    "\n",
    "            random.shuffle(preprocessed_data)\n",
    "\n",
    "            pickle.dump(preprocessed_data, open(self.output_dir + f'data_{file_num:03d}.p', 'wb'), compression='gzip')\n",
    "\n",
    "            print(f\"Finished processing {file_num} files\")\n",
    "            file_num += self.num_procs\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        print('\\nPreprocessing and saving data to {}'.format(self.output_dir))\n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.preprocessor, args=(i,), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "\n",
    "        for p in self.procs:\n",
    "            p.join()\n",
    "\n",
    "        self.file_list = [self.output_dir + f'data_{i:03d}.p' for i in range(self.num_files)]\n",
    "\n",
    "    def preprocessed_worker(self, worker_id, batch_queue):\n",
    "        batch_graphs = []\n",
    "        batch_targets = []\n",
    "\n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
    "\n",
    "            for i in range(len(file_data)):\n",
    "                batch_graphs.append(file_data[i][0])\n",
    "                batch_targets.append(file_data[i][1])\n",
    "\n",
    "                if len(batch_graphs) == self.batch_size:\n",
    "                    batch_targets = np.reshape(np.array(batch_targets), [-1,2]).astype(np.float32)\n",
    "\n",
    "                    batch_queue.put((batch_graphs, batch_targets))\n",
    "\n",
    "                    batch_graphs = []\n",
    "                    batch_targets = []\n",
    "\n",
    "            file_num += self.num_procs\n",
    "\n",
    "        if len(batch_graphs) > 0:\n",
    "            batch_targets = np.reshape(np.array(batch_targets), [-1,2]).astype(np.float32)\n",
    "            batch_queue.put((batch_graphs, batch_targets))\n",
    "\n",
    "    def worker(self, worker_id, batch_queue):\n",
    "        if self.preprocess:\n",
    "            self.preprocessed_worker(worker_id, batch_queue)\n",
    "        else:\n",
    "            raise Exception('Preprocessing is required for combined classification/regression models.')\n",
    "\n",
    "    def check_procs(self):\n",
    "        for p in self.procs:\n",
    "            if p.is_alive(): return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def kill_procs(self):\n",
    "        for p in self.procs:\n",
    "            p.kill()\n",
    "\n",
    "        self.procs = []\n",
    "\n",
    "    def generator(self):\n",
    "        \"\"\"\n",
    "        Generator that returns processed batches during training\n",
    "        \"\"\"\n",
    "        batch_queue = Queue(2 * self.num_procs)\n",
    "\n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.worker, args=(i, batch_queue), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "\n",
    "        while self.check_procs() or not batch_queue.empty():\n",
    "            try:\n",
    "                batch = batch_queue.get(True, 0.0001)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            yield batch\n",
    "\n",
    "        for p in self.procs:\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the data generation step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/clusterfs/ml4hep/mpettee/ml4pions/data/onetrack_multicluster/'\n",
    "pi0_files = [] #np.sort(glob.glob(data_dir+'pi0_files/*.npy'))[:1]\n",
    "pion_files = np.sort(glob.glob(data_dir+'pion_files/*.npy'))[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = GraphDataGenerator(pion_file_list=pion_files, \n",
    "                              pi0_file_list=pi0_files,\n",
    "                              cellGeo_file='/clusterfs/ml4hep/mpettee/ml4pions/data/cell_geo.root',\n",
    "                              batch_size=32,\n",
    "                              shuffle=False,\n",
    "                              num_procs=32,\n",
    "                              preprocess=True,\n",
    "                              output_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from graph_nets.graphs import GraphsTuple\n",
    "\n",
    "# Cell\n",
    "def convert_to_tuple(graphs):\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    global_nodes = []\n",
    "    senders = []\n",
    "    receivers = []\n",
    "    n_node = []\n",
    "    n_edge = []\n",
    "    offset = 0\n",
    "    cluster_energies = []\n",
    "    cluster_etas = []\n",
    "    cluster_EM_probs = []\n",
    "    cluster_calib_Es = []\n",
    "    cluster_had_weights = []\n",
    "    truth_particle_es = []\n",
    "    truth_particle_pts = []\n",
    "    track_pts = []\n",
    "    track_etas = []\n",
    "\n",
    "    for graph in graphs:\n",
    "        nodes.append(graph['nodes'])\n",
    "        edges.append(graph['edges'])\n",
    "        global_nodes.append([graph['globals']])\n",
    "        senders.append(graph['senders'])\n",
    "        receivers.append(graph['receivers'])\n",
    "        senders.append(graph['senders'] + offset)\n",
    "        receivers.append(graph['receivers'] + offset)\n",
    "        n_node.append(graph['nodes'].shape[:1])\n",
    "        n_edge.append(graph['edges'].shape[:1])\n",
    "        cluster_energies.append(graph['cluster_E'])\n",
    "        cluster_etas.append(graph['cluster_eta'])\n",
    "        cluster_EM_probs.append(graph['cluster_EM_prob'])\n",
    "        cluster_calib_Es.append(graph['cluster_calib_E'])\n",
    "        cluster_had_weights.append(graph['cluster_HAD_WEIGHT'])\n",
    "        truth_particle_es.append(graph['truthPartE'])\n",
    "        truth_particle_pts.append(graph['truthPartPt'])\n",
    "        track_pts.append(graph['track_pt'])\n",
    "        track_etas.append(graph['track_eta'])\n",
    "\n",
    "        offset += len(graph['nodes'])\n",
    "\n",
    "    nodes = tf.convert_to_tensor(np.concatenate(nodes))\n",
    "    edges = tf.convert_to_tensor(np.concatenate(edges))\n",
    "    global_nodes = tf.convert_to_tensor(np.concatenate(global_nodes))\n",
    "    senders = tf.convert_to_tensor(np.concatenate(senders))\n",
    "    receivers = tf.convert_to_tensor(np.concatenate(receivers))\n",
    "    n_node = tf.convert_to_tensor(np.concatenate(n_node))\n",
    "    n_edge = tf.convert_to_tensor(np.concatenate(n_edge))\n",
    "\n",
    "    graph = GraphsTuple(\n",
    "            nodes=nodes,\n",
    "            edges=edges,\n",
    "            globals=global_nodes,\n",
    "            senders=senders,\n",
    "            receivers=receivers,\n",
    "            n_node=n_node,\n",
    "            n_edge=n_edge\n",
    "        )\n",
    "\n",
    "    return graph, cluster_energies, cluster_etas, cluster_EM_probs, cluster_calib_Es, cluster_had_weights, truth_particle_es, truth_particle_pts, track_pts, track_etas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get batch of data\n",
    "def get_batch(data_iter):\n",
    "    for graphs, targets in data_iter:\n",
    "        targets = tf.convert_to_tensor(targets)\n",
    "        graphs, energies, etas, em_probs, cluster_calib_es, cluster_had_weights, truth_particle_es, truth_particle_pts, track_pts, track_etas = convert_to_tuple(graphs)\n",
    "        yield graphs, targets, energies, etas, em_probs, cluster_calib_es, cluster_had_weights, truth_particle_es, truth_particle_pts, track_pts, track_etas\n",
    "\n",
    "samp_graph, samp_target, _, _, _, _, _, _, _, _, _ = next(get_batch(data_gen.generator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "nbdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
