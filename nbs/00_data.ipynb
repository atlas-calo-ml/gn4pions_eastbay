{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modules.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import uproot as ur\n",
    "import time\n",
    "from multiprocessing import Process, Queue, set_start_method\n",
    "import compress_pickle as pickle\n",
    "from scipy.stats import circmean\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphDataGenerator\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GraphDataGenerator:\n",
    "    \"\"\"\n",
    "    DataGenerator class for extracting and formating data from list of root files\n",
    "    This data generator uses the cell_geo file to create the input graph structure\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 pi0_file_list: list,\n",
    "                 pion_file_list: list,\n",
    "                 cellGeo_file: str,\n",
    "                 batch_size: int,\n",
    "                 shuffle: bool = True,\n",
    "                 num_procs = 32,\n",
    "                 preprocess = False,\n",
    "                 output_dir = None):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            self.pi0_file_list = pi0_file_list\n",
    "            self.pion_file_list = pion_file_list\n",
    "            assert len(pi0_file_list) == len(pion_file_list)\n",
    "            self.num_files = len(self.pi0_file_list)\n",
    "        else:\n",
    "            self.file_list = pi0_file_list\n",
    "            self.num_files = len(self.file_list)\n",
    "        \n",
    "        self.cellGeo_file = cellGeo_file\n",
    "        \n",
    "        self.cellGeo_data = ur.open(self.cellGeo_file)['CellGeo']\n",
    "        self.geoFeatureNames = self.cellGeo_data.keys()[1:9]\n",
    "        self.nodeFeatureNames = ['cluster_cell_E', *self.geoFeatureNames[:-2]]\n",
    "        self.edgeFeatureNames = self.cellGeo_data.keys()[9:]\n",
    "        self.num_nodeFeatures = len(self.nodeFeatureNames)\n",
    "        self.num_edgeFeatures = len(self.edgeFeatureNames)\n",
    "\n",
    "        self.cellGeo_data = self.cellGeo_data.arrays(library='np')\n",
    "        self.cellGeo_ID = self.cellGeo_data['cell_geo_ID'][0]\n",
    "        self.sorter = np.argsort(self.cellGeo_ID)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        if self.shuffle: np.random.shuffle(self.file_list)\n",
    "        \n",
    "        self.num_procs = np.min([num_procs, self.num_files])\n",
    "        self.procs = []\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            os.makedirs(self.output_dir, exist_ok=True)\n",
    "            self.preprocess_data()\n",
    "\n",
    "    def get_cluster_calib(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading cluster calibration energy \"\"\" \n",
    "            \n",
    "        cluster_calib_E = event_data['cluster_ENG_CALIB_TOT'][event_ind][cluster_ind]\n",
    "\n",
    "        if cluster_calib_E <= 0:\n",
    "            return None\n",
    "\n",
    "        return np.log10(cluster_calib_E)\n",
    "            \n",
    "    def get_nodes(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading Node features \"\"\" \n",
    "\n",
    "        cell_IDs = event_data['cluster_cell_ID'][event_ind][cluster_ind]\n",
    "        cell_IDmap = self.sorter[np.searchsorted(self.cellGeo_ID, cell_IDs, sorter=self.sorter)]\n",
    "        \n",
    "        nodes = np.log10(event_data['cluster_cell_E'][event_ind][cluster_ind])\n",
    "        global_node = np.log10(event_data['cluster_E'][event_ind][cluster_ind])\n",
    "        \n",
    "        # Scaling the cell_geo_sampling by 28\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_sampling'][0][cell_IDmap]/28.)\n",
    "        for f in self.nodeFeatureNames[2:4]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "        # Scaling the cell_geo_rPerp by 3000\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_rPerp'][0][cell_IDmap]/3000.)\n",
    "        for f in self.nodeFeatureNames[5:]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "\n",
    "        nodes = np.reshape(nodes, (len(self.nodeFeatureNames), -1)).T\n",
    "        cluster_num_nodes = len(nodes)\n",
    "        \n",
    "        return nodes, np.array([global_node]), cluster_num_nodes, cell_IDmap\n",
    "    \n",
    "    def get_edges(self, cluster_num_nodes, cell_IDmap):\n",
    "        \"\"\" \n",
    "        Reading edge features \n",
    "        Resturns senders, receivers, and edges    \n",
    "        \"\"\" \n",
    "        \n",
    "        edge_inds = np.zeros((cluster_num_nodes, self.num_edgeFeatures))\n",
    "        for i, f in enumerate(self.edgeFeatureNames):\n",
    "            edge_inds[:, i] = self.cellGeo_data[f][0][cell_IDmap]\n",
    "        edge_inds[np.logical_not(np.isin(edge_inds, cell_IDmap))] = np.nan\n",
    "        \n",
    "        senders, edge_on_inds = np.isin(edge_inds, cell_IDmap).nonzero()\n",
    "        cluster_num_edges = len(senders)\n",
    "        edges = np.zeros((cluster_num_edges, self.num_edgeFeatures))\n",
    "        edges[np.arange(cluster_num_edges), edge_on_inds] = 1\n",
    "        \n",
    "        cell_IDmap_sorter = np.argsort(cell_IDmap)\n",
    "        rank = np.searchsorted(cell_IDmap, edge_inds , sorter=cell_IDmap_sorter)\n",
    "        receivers = cell_IDmap_sorter[rank[rank!=cluster_num_nodes]]\n",
    "        \n",
    "        return senders, receivers, edges\n",
    "\n",
    "    def preprocessor(self, worker_id):\n",
    "        \"\"\"\n",
    "        Prerocessing root file data for faster data \n",
    "        generation during multiple training epochs\n",
    "        \"\"\"\n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            print(f\"Proceesing file number {file_num}\")\n",
    "            file = self.pion_file_list[file_num]\n",
    "            event_tree = ur.open(file)['EventTree']\n",
    "            num_events = event_tree.num_entries\n",
    "\n",
    "            event_data = event_tree.arrays(library='np')\n",
    "\n",
    "            preprocessed_data = []\n",
    "\n",
    "            for event_ind in range(num_events):\n",
    "                num_clusters = event_data['nCluster'][event_ind]\n",
    "                \n",
    "                for i in range(num_clusters):\n",
    "                    cluster_calib_E = self.get_cluster_calib(event_data, event_ind, i)\n",
    "                    \n",
    "                    if cluster_calib_E is None:\n",
    "                        continue\n",
    "                        \n",
    "                    nodes, global_node, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, i)\n",
    "                    senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
    "\n",
    "                    graph = {'nodes': nodes.astype(np.float32), 'globals': global_node.astype(np.float32),\n",
    "                        'senders': senders.astype(np.int32), 'receivers': receivers.astype(np.int32),\n",
    "                        'edges': edges.astype(np.float32)}\n",
    "                    target = np.reshape([cluster_calib_E.astype(np.float32), 1], [1,2])\n",
    "\n",
    "                    preprocessed_data.append((graph, target))\n",
    "\n",
    "            file = self.pi0_file_list[file_num]\n",
    "            event_tree = ur.open(file)['EventTree']\n",
    "            num_events = event_tree.num_entries\n",
    "\n",
    "            event_data = event_tree.arrays(library='np')\n",
    "\n",
    "            for event_ind in range(num_events):\n",
    "                num_clusters = event_data['nCluster'][event_ind]\n",
    "                \n",
    "                for i in range(num_clusters):\n",
    "                    cluster_calib_E = self.get_cluster_calib(event_data, event_ind, i)\n",
    "                    \n",
    "                    if cluster_calib_E is None:\n",
    "                        continue\n",
    "                        \n",
    "                    nodes, global_node, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, i)\n",
    "                    senders, receivers, edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
    "                    \n",
    "                    graph = {'nodes': nodes.astype(np.float32), 'globals': global_node.astype(np.float32),\n",
    "                        'senders': senders.astype(np.int32), 'receivers': receivers.astype(np.int32),\n",
    "                        'edges': edges.astype(np.float32)}\n",
    "                    target = np.reshape([cluster_calib_E.astype(np.float32), 0], [1,2])\n",
    "\n",
    "                    preprocessed_data.append((graph, target))\n",
    "\n",
    "            random.shuffle(preprocessed_data)\n",
    "\n",
    "            pickle.dump(preprocessed_data, open(self.output_dir + f'data_{file_num:03d}.p', 'wb'), compression='gzip')\n",
    "            \n",
    "            print(f\"Finished processing {file_num} files\")\n",
    "            file_num += self.num_procs\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        print('\\nPreprocessing and saving data to {}'.format(self.output_dir))\n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.preprocessor, args=(i,), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()\n",
    "\n",
    "        self.file_list = [self.output_dir + f'data_{i:03d}.p' for i in range(self.num_files)]\n",
    "\n",
    "    def preprocessed_worker(self, worker_id, batch_queue):\n",
    "        batch_graphs = []\n",
    "        batch_targets = []\n",
    "\n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
    "\n",
    "            for i in range(len(file_data)):\n",
    "                batch_graphs.append(file_data[i][0])\n",
    "                batch_targets.append(file_data[i][1])\n",
    "                    \n",
    "                if len(batch_graphs) == self.batch_size:\n",
    "                    batch_targets = np.reshape(np.array(batch_targets), [-1,2]).astype(np.float32)\n",
    "                    \n",
    "                    batch_queue.put((batch_graphs, batch_targets))\n",
    "                    \n",
    "                    batch_graphs = []\n",
    "                    batch_targets = []\n",
    "\n",
    "            file_num += self.num_procs\n",
    "                    \n",
    "        if len(batch_graphs) > 0:\n",
    "            batch_targets = np.reshape(np.array(batch_targets), [-1,2]).astype(np.float32)\n",
    "            \n",
    "            batch_queue.put((batch_graphs, batch_targets))\n",
    "\n",
    "    def worker(self, worker_id, batch_queue):\n",
    "        if self.preprocess:\n",
    "            self.preprocessed_worker(worker_id, batch_queue)\n",
    "        else:\n",
    "            raise Exception('Preprocessing is required for combined classification/regression models.')\n",
    "        \n",
    "    def check_procs(self):\n",
    "        for p in self.procs:\n",
    "            if p.is_alive(): return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def kill_procs(self):\n",
    "        for p in self.procs:\n",
    "            p.kill()\n",
    "\n",
    "        self.procs = []\n",
    "    \n",
    "    def generator(self):\n",
    "        \"\"\"\n",
    "        Generator that returns processed batches during training\n",
    "        \"\"\"\n",
    "        batch_queue = Queue(2 * self.num_procs)\n",
    "            \n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.worker, args=(i, batch_queue), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        while self.check_procs() or not batch_queue.empty():\n",
    "            try:\n",
    "                batch = batch_queue.get(True, 0.0001)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            yield batch\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/usr/workspace/hip/ML4Jets/regression_images/'\n",
    "out_dir = '/p/vast1/karande1/heavyIon/data/preprocessed_data/gn4pions/geo/train/'\n",
    "pi0_files = np.sort(glob.glob(data_dir+'graphs.v01-45-gaa27bcb/'+'*pi0*/*.root'))[10:20]\n",
    "pion_files = np.sort(glob.glob(data_dir+'graphs.v01-45-gaa27bcb/'+'*pion*/*.root'))[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing and saving data to /p/vast1/karande1/heavyIon/data/preprocessed_data/gn4pions/geo/train/\n",
      "Proceesing file number 0\n",
      "Proceesing file number 1\n",
      "Proceesing file number 2\n",
      "Proceesing file number 3\n",
      "Proceesing file number 4\n",
      "Proceesing file number 5\n",
      "Proceesing file number 6\n",
      "Proceesing file number 7\n",
      "Proceesing file number 8\n",
      "Proceesing file number 9\n",
      "Finished processing 38 files\n",
      "Finished processing 35 files\n",
      "Finished processing 32 files\n",
      "Finished processing 34 files\n",
      "Finished processing 41 files\n",
      "Finished processing 36 files\n",
      "Finished processing 33 files\n",
      "Finished processing 40 files\n",
      "Finished processing 37 files\n",
      "Finished processing 39 files\n"
     ]
    }
   ],
   "source": [
    "data_gen = GraphDataGenerator(pion_file_list=pion_files, \n",
    "                              pi0_file_list=pi0_files,\n",
    "                              cellGeo_file=data_dir+'graph_examples/cell_geo.root',\n",
    "                              batch_size=32,\n",
    "                              shuffle=False,\n",
    "                              num_procs=32,\n",
    "                              preprocess=True,\n",
    "                              output_dir=out_dir)\n",
    "\n",
    "# gen = data_gen.generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing and saving data to /p/vast1/karande1/heavyIon/data/preprocessed_data/gn4pions/geo/val/\n",
      "Proceesing file number 0\n",
      "Proceesing file number 1\n",
      "Proceesing file number 2\n",
      "Proceesing file number 3\n",
      "Proceesing file number 4\n",
      "Proceesing file number 5\n",
      "Proceesing file number 6\n",
      "Proceesing file number 7\n",
      "Proceesing file number 8\n",
      "Proceesing file number 9\n",
      "Finished processing 16 files\n",
      "Finished processing 15 files\n",
      "Finished processing 18 files\n",
      "Finished processing 19 files\n",
      "Finished processing 10 files\n",
      "Finished processing 14 files\n",
      "Finished processing 13 files\n",
      "Finished processing 12 files\n",
      "Finished processing 17 files\n",
      "Finished processing 11 files\n"
     ]
    }
   ],
   "source": [
    "out_dir = '/p/vast1/karande1/heavyIon/data/preprocessed_data/gn4pions/geo/val/'\n",
    "pi0_files = np.sort(glob.glob(data_dir+'graphs.v01-45-gaa27bcb/'+'*pi0*/*.root'))[20:30]\n",
    "pion_files = np.sort(glob.glob(data_dir+'graphs.v01-45-gaa27bcb/'+'*pion*/*.root'))[20:30]\n",
    "data_gen_test = GraphDataGenerator(pion_file_list=pion_files, \n",
    "                                   pi0_file_list=pi0_files,\n",
    "                                   cellGeo_file=data_dir+'graph_examples/cell_geo.root',\n",
    "                                   batch_size=32,\n",
    "                                   shuffle=False,\n",
    "                                   num_procs=32,\n",
    "                                   preprocess=True,\n",
    "                                   output_dir=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opence",
   "language": "python",
   "name": "opence"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
