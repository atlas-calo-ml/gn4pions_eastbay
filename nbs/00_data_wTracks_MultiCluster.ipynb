{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modules.data_trackMultiCalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import uproot as ur\n",
    "import time\n",
    "from multiprocessing import Process, Queue, set_start_method\n",
    "import compress_pickle as pickle\n",
    "from scipy.stats import circmean\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphDataGenerator\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiCaloTrackDataGenerator:\n",
    "    \"\"\"DataGenerator class for extracting and formating data from list of root files\"\"\"\n",
    "    def __init__(self,\n",
    "                 pion_file_list: list,\n",
    "                 cellGeo_file: str,\n",
    "                 batch_size: int,\n",
    "                 n_clusters: int,\n",
    "                 shuffle: bool = True,\n",
    "                 num_procs: int = 32,\n",
    "                 preprocess: bool = False,\n",
    "                 output_dir: str = None):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            self.pion_file_list = pion_file_list\n",
    "            self.num_files = len(self.pion_file_list)\n",
    "        else:\n",
    "            self.file_list = pion_file_list\n",
    "            self.num_files = len(self.file_list)\n",
    "        \n",
    "        self.cellGeo_file = cellGeo_file\n",
    "        \n",
    "        self.cellGeo_data = ur.open(self.cellGeo_file)['CellGeo']\n",
    "        self.geoFeatureNames = self.cellGeo_data.keys()[1:9]\n",
    "        self.nodeFeatureNames = ['cluster_cell_E', *self.geoFeatureNames[:-2], 'cluster_E']\n",
    "        self.num_nodeFeatures = len(self.nodeFeatureNames)\n",
    "\n",
    "        self.edgeFeatureNames = self.cellGeo_data.keys()[9:]\n",
    "        self.num_edgeFeatures = len(self.edgeFeatureNames)\n",
    "\n",
    "        self.cellGeo_data = self.cellGeo_data.arrays(library='np')\n",
    "        self.cellGeo_ID = self.cellGeo_data['cell_geo_ID'][0]\n",
    "        self.sorter = np.argsort(self.cellGeo_ID)\n",
    "        \n",
    "        self.track_feature_names = ['trackPt','trackD0','trackZ0', 'trackEta_EMB2','trackPhi_EMB2',\n",
    "                                    'trackEta','trackPhi','truthPartE', 'truthPartPt', 'num_clusters']\n",
    "        self.cluster_feature_names = ['cluster_E', 'cluster_Eta', 'cluster_Phi', 'cluster_ENG_CALIB_TOT', \n",
    "                                      'cluster_EM_PROBABILITY','cluster_E_LCCalib','cluster_HAD_WEIGHT', 'dR']\n",
    "        \n",
    "        self.dr_thresh = 1.2\n",
    "        self.clusterThresh = .5\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "        if self.shuffle: np.random.shuffle(self.file_list)\n",
    "        \n",
    "        self.num_procs = num_procs\n",
    "        self.procs = []\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            os.makedirs(self.output_dir, exist_ok=True)\n",
    "            self.preprocess_data()\n",
    "    \n",
    "    def get_cluster_inds(self, event_data, event_ind):\n",
    "        \n",
    "        if self.n_clusters==-1:   # get all nodes satisfying dR criterion\n",
    "            c_inds = range(event_data['nCluster'][event_ind])\n",
    "            c_inds = [c for c in c_inds if (event_data['dR'][event_ind][c]<self.dr_thresh) and \n",
    "                      (event_data['cluster_E'][event_ind][c]>self.clusterThresh)]\n",
    "        else:                # get n leading nodes satisfying dR criterion\n",
    "            c_inds = np.argsort(event_data['cluster_E'][event_ind])[::-1]\n",
    "            c_inds = [c for c in c_inds if (event_data['dR'][event_ind][c]<self.dr_thresh) and \n",
    "                      (event_data['cluster_E'][event_ind][c]>self.clusterThresh)]\n",
    "            c_inds = c_inds[:self.n_clusters]\n",
    "        \n",
    "        return c_inds\n",
    "    \n",
    "    def get_meta(self, event_data, event_ind, c_inds):\n",
    "        \"\"\" \n",
    "        Reading meta data\n",
    "        \"\"\"  \n",
    "        track_meta_data = []\n",
    "        for f in self.track_feature_names[:-1]:\n",
    "            track_meta_data.append(event_data[f][event_ind])\n",
    "        track_meta_data.append(len(c_inds))\n",
    "        \n",
    "        cluster_meta_data = []\n",
    "        for c in c_inds:\n",
    "            curr_meta = []\n",
    "            \n",
    "            for f in self.cluster_feature_names:\n",
    "                curr_meta.append(event_data[f][event_ind][c])\n",
    "            \n",
    "            cluster_meta_data.append(curr_meta)\n",
    "            \n",
    "        return np.array(track_meta_data, dtype=np.float32), np.array(cluster_meta_data, dtype=np.float32)\n",
    "    \n",
    "    def get_nodes(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading Node features \"\"\" \n",
    "\n",
    "        cell_IDs = event_data['cluster_cell_ID'][event_ind][cluster_ind]\n",
    "        cell_IDmap = self.sorter[np.searchsorted(self.cellGeo_ID, cell_IDs, sorter=self.sorter)]\n",
    "        \n",
    "        nodes = np.log10(event_data['cluster_cell_E'][event_ind][cluster_ind])\n",
    "        \n",
    "        # Scaling the cell_geo_sampling by 28\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_sampling'][0][cell_IDmap]/28.)\n",
    "        for f in self.nodeFeatureNames[2:4]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "        # Scaling the cell_geo_rPerp by 3000\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_rPerp'][0][cell_IDmap]/3000.)\n",
    "        for f in self.nodeFeatureNames[5:-1]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "        \n",
    "        nodes = np.reshape(nodes, (len(self.nodeFeatureNames)-1, -1)).T\n",
    "        cluster_E = np.log10(event_data['cluster_E'][event_ind][cluster_ind])\n",
    "        cluster_E = np.repeat([[cluster_E]], len(nodes), axis=0)\n",
    "        nodes = np.hstack([nodes, cluster_E])\n",
    "\n",
    "        cluster_num_nodes = len(nodes)\n",
    "        \n",
    "        return nodes, cluster_num_nodes, cell_IDmap\n",
    "                     \n",
    "    def get_edges(self, cluster_num_nodes, cell_IDmap):\n",
    "        \"\"\" \n",
    "        Reading edge features \n",
    "        Resturns senders, receivers, and edges    \n",
    "        \"\"\" \n",
    "        \n",
    "        edge_inds = np.zeros((cluster_num_nodes, self.num_edgeFeatures))\n",
    "        for i, f in enumerate(self.edgeFeatureNames):\n",
    "            edge_inds[:, i] = self.cellGeo_data[f][0][cell_IDmap]\n",
    "        edge_inds[np.logical_not(np.isin(edge_inds, cell_IDmap))] = np.nan\n",
    "        \n",
    "        senders, edge_on_inds = np.isin(edge_inds, cell_IDmap).nonzero()\n",
    "        cluster_num_edges = len(senders)\n",
    "        edges = np.zeros((cluster_num_edges, self.num_edgeFeatures))\n",
    "        edges[np.arange(cluster_num_edges), edge_on_inds] = 1\n",
    "        \n",
    "        cell_IDmap_sorter = np.argsort(cell_IDmap)\n",
    "        rank = np.searchsorted(cell_IDmap, edge_inds , sorter=cell_IDmap_sorter)\n",
    "        receivers = cell_IDmap_sorter[rank[rank!=cluster_num_nodes]]\n",
    "        \n",
    "        return senders, receivers, edges\n",
    "\n",
    "    def preprocessor(self, worker_id):\n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            print(f\"Proceesing file {os.path.basename(self.pion_file_list[file_num])}\")\n",
    "            file = self.pion_file_list[file_num]\n",
    "            event_data = np.load(file, allow_pickle=True).item()\n",
    "            num_events = len(event_data[[key for key in event_data.keys()][0]])\n",
    "\n",
    "            preprocessed_data = []\n",
    "\n",
    "            for event_ind in range(num_events):\n",
    "                truth_particle_E = np.log10(event_data['truthPartE'][event_ind][0]) # first one is the pion! \n",
    "                trackPt = event_data['trackPt'][event_ind][0]\n",
    "                if trackPt>5000:\n",
    "                    continue\n",
    "                    \n",
    "                trackEta = event_data['trackEta'][event_ind][0]\n",
    "                global_node = np.array([np.log10(trackPt), trackEta], dtype=np.float32)\n",
    "        \n",
    "                c_inds = self.get_cluster_inds(event_data, event_ind)\n",
    "                if not len(c_inds):\n",
    "                    continue\n",
    "                \n",
    "                nodes = []\n",
    "                senders = []\n",
    "                receivers = []\n",
    "                edges = []\n",
    "                offset = 0\n",
    "                for c in c_inds:\n",
    "                    curr_nodes, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, c)\n",
    "                    curr_senders, curr_receivers, curr_edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
    "                    \n",
    "                    nodes.append(curr_nodes)\n",
    "                    edges.append(curr_edges)\n",
    "                    senders.append(curr_senders + offset)\n",
    "                    receivers.append(curr_receivers + offset)\n",
    "                    \n",
    "                    offset += len(curr_nodes)\n",
    "                \n",
    "                nodes = np.concatenate(nodes)\n",
    "                edges = np.concatenate(edges)\n",
    "                senders = np.concatenate(senders)\n",
    "                receivers = np.concatenate(receivers)\n",
    "                \n",
    "                track_meta_data, cluster_meta_data = self.get_meta(event_data, event_ind, c_inds)\n",
    "                \n",
    "                graph = {'nodes': nodes.astype(np.float32), \n",
    "                         'globals': global_node.astype(np.float32),\n",
    "                         'senders': senders.astype(np.int32), \n",
    "                         'receivers': receivers.astype(np.int32),\n",
    "                         'edges': edges.astype(np.float32)}\n",
    "                target = truth_particle_E.astype(np.float32)\n",
    "\n",
    "                preprocessed_data.append((graph, target, track_meta_data, cluster_meta_data))\n",
    "\n",
    "            random.shuffle(preprocessed_data)\n",
    "\n",
    "            pickle.dump(preprocessed_data, open(self.output_dir + f'data_{file_num:03d}.p', 'wb'), compression='gzip')\n",
    "\n",
    "            print(f\"Finished processing {file_num} files\")\n",
    "            file_num += self.num_procs\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        print('\\nPreprocessing and saving data to {}'.format(self.output_dir))\n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.preprocessor, args=(i,), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()\n",
    "\n",
    "        self.file_list = [self.output_dir + f'data_{i:03d}.p' for i in range(self.num_files)]\n",
    "\n",
    "    def preprocessed_worker(self, worker_id, batch_queue):\n",
    "        batch_graphs = []\n",
    "        batch_targets = []\n",
    "        batch_track_meta = []\n",
    "        batch_cluster_meta = []\n",
    "        \n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
    "\n",
    "            for i in range(len(file_data)):\n",
    "                batch_graphs.append(file_data[i][0])\n",
    "                batch_targets.append(file_data[i][1])\n",
    "                batch_track_meta.append(file_data[i][2])\n",
    "                batch_cluster_meta.append(file_data[i][3])\n",
    "                    \n",
    "                if len(batch_graphs) == self.batch_size:\n",
    "                    batch_targets = np.array(batch_targets).astype(np.float32)\n",
    "                    batch_queue.put((batch_graphs, batch_targets, batch_track_meta, batch_cluster_meta))\n",
    "                    \n",
    "                    batch_graphs = []\n",
    "                    batch_targets = []\n",
    "                    batch_track_meta = []\n",
    "                    batch_cluster_meta = []\n",
    "\n",
    "            file_num += self.num_procs\n",
    "                    \n",
    "        if len(batch_graphs) > 0:\n",
    "            batch_targets = np.array(batch_targets).astype(np.float32)\n",
    "            batch_queue.put((batch_graphs, batch_targets, batch_track_meta, batch_cluster_meta))\n",
    "\n",
    "    def worker(self, worker_id, batch_queue):\n",
    "        if self.preprocess:\n",
    "            self.preprocessed_worker(worker_id, batch_queue)\n",
    "        else:\n",
    "            raise Exception('Preprocessing is required for combined classification/regression models.')\n",
    "        \n",
    "    def check_procs(self):\n",
    "        for p in self.procs:\n",
    "            if p.is_alive(): return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def kill_procs(self):\n",
    "        for p in self.procs:\n",
    "            p.kill()\n",
    "\n",
    "        self.procs = []\n",
    "    \n",
    "    def generator(self):\n",
    "        # for file in self.file_list:\n",
    "        batch_queue = Queue(2 * self.num_procs)\n",
    "            \n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.worker, args=(i, batch_queue), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        while self.check_procs() or not batch_queue.empty():\n",
    "            try:\n",
    "                batch = batch_queue.get(True, 0.0001)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            yield batch\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the data generation step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pion_dir = '/usr/workspace/hip/ML4Jets/regression_images/graphs.v01-45-gaa27bcb/onetrack_multicluster/pion_files/'\n",
    "pion_files = np.sort(glob.glob(pion_dir+\"*.npy\"))\n",
    "n_files = 3\n",
    "\n",
    "cell_geo_file = '/usr/workspace/hip/ML4Jets/regression_images/graph_examples/cell_geo.root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing and saving data to ./\n",
      "Proceesing file 001.npy\n",
      "Proceesing file 002.npy\n",
      "Proceesing file 003.npy\n",
      "Finished processing 2 files\n",
      "Finished processing 1 files\n",
      "Finished processing 0 files\n"
     ]
    }
   ],
   "source": [
    "data_gen = MultiCaloTrackDataGenerator(pion_file_list=pion_files[:n_files],\n",
    "                                       cellGeo_file=cell_geo_file,\n",
    "                                       batch_size=32,\n",
    "                                       n_clusters=-1,\n",
    "                                       shuffle=False,\n",
    "                                       num_procs=32,\n",
    "                                       preprocess=True,\n",
    "                                       output_dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, target, track_meta_data, cluster_meta_data = next(data_gen.generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.471,  0.429, -0.85 , -2.209,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-0.404,  0.429, -0.85 , -2.307,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-0.436,  0.429, -0.85 , -2.111,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-0.44 ,  0.429, -0.95 , -2.307,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.6  ,  0.429, -0.75 , -2.307,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-0.908,  0.429, -0.95 , -2.111,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.046,  0.429, -0.75 , -2.111,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-2.133,  0.464, -0.85 , -2.209,  0.932,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.457,  0.107, -0.873, -2.168,  0.645,  0.05 ,  0.025,  0.69 ],\n",
       "       [-1.572,  0.429, -0.85 , -2.405,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.289,  0.464, -0.75 , -2.307,  1.007,  0.1  ,  0.098,  0.69 ],\n",
       "       [-2.061,  0.464, -0.85 , -2.307,  0.932,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.7  ,  0.107, -0.823, -2.34 ,  0.649,  0.05 ,  0.025,  0.69 ],\n",
       "       [-1.508,  0.107, -0.823, -2.291,  0.649,  0.05 ,  0.025,  0.69 ],\n",
       "       [-1.679,  0.107, -0.823, -2.266,  0.649,  0.05 ,  0.025,  0.69 ],\n",
       "       [-1.564,  0.429, -0.85 , -2.013,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-0.795,  0.429, -0.95 , -2.013,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-2.208,  0.464, -0.85 , -2.111,  0.932,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.744,  0.107, -0.823, -2.144,  0.649,  0.05 ,  0.025,  0.69 ],\n",
       "       [-1.822,  0.107, -0.823, -2.07 ,  0.649,  0.05 ,  0.025,  0.69 ],\n",
       "       [-1.407,  0.107, -0.873, -2.144,  0.645,  0.05 ,  0.025,  0.69 ],\n",
       "       [-2.156,  0.107, -0.873, -2.119,  0.645,  0.05 ,  0.025,  0.69 ],\n",
       "       [-1.917,  0.607, -1.157, -2.307,  0.826,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.628,  0.607, -1.157, -2.405,  0.826,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.977,  0.607, -1.057, -2.307,  0.936,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.921,  0.107, -0.974, -2.315,  0.641,  0.05 ,  0.025,  0.69 ],\n",
       "       [-2.156,  0.107, -0.974, -2.291,  0.641,  0.05 ,  0.025,  0.69 ],\n",
       "       [-2.25 ,  0.607, -1.057, -2.209,  0.936,  0.1  ,  0.098,  0.69 ],\n",
       "       [-0.374,  0.429, -0.95 , -2.209,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [ 0.122,  0.107, -0.923, -2.217,  0.643,  0.05 ,  0.025,  0.446],\n",
       "       [-1.794,  0.107, -0.923, -2.242,  0.643,  0.05 ,  0.025,  0.446],\n",
       "       [-0.101,  0.107, -0.923, -2.193,  0.643,  0.05 ,  0.025,  0.446],\n",
       "       [-1.347,  0.107, -0.973, -2.217,  0.641,  0.05 ,  0.025,  0.446],\n",
       "       [-1.538,  0.107, -0.973, -2.193,  0.641,  0.05 ,  0.025,  0.446],\n",
       "       [-0.545,  0.071, -0.911, -2.217,  0.575,  0.025,  0.025,  0.446],\n",
       "       [-1.421,  0.071, -0.936, -2.217,  0.574,  0.025,  0.025,  0.446],\n",
       "       [-2.298,  0.107, -0.973, -2.168,  0.641,  0.05 ,  0.025,  0.446],\n",
       "       [-0.954,  0.071, -0.911, -2.193,  0.575,  0.025,  0.025,  0.446],\n",
       "       [-1.337,  0.071, -0.936, -2.193,  0.574,  0.025,  0.025,  0.446],\n",
       "       [-1.317,  0.071, -0.911, -2.242,  0.575,  0.025,  0.025,  0.446],\n",
       "       [-1.744,  0.071, -0.886, -2.217,  0.577,  0.025,  0.025,  0.446],\n",
       "       [-2.   ,  0.071, -0.886, -2.242,  0.577,  0.025,  0.025,  0.446],\n",
       "       [-2.156,  0.036, -0.902, -2.205,  0.509,  0.003,  0.098,  0.446],\n",
       "       [-1.267,  0.036, -0.909, -2.205,  0.509,  0.003,  0.098,  0.446],\n",
       "       [-1.286,  0.071, -0.911, -2.168,  0.575,  0.025,  0.025,  0.446],\n",
       "       [-1.679,  0.107, -0.873, -2.242,  0.645,  0.05 ,  0.025,  0.446],\n",
       "       [-1.744,  0.107, -0.873, -2.193,  0.645,  0.05 ,  0.025,  0.446],\n",
       "       [-1.957,  0.107, -0.873, -2.217,  0.645,  0.05 ,  0.025,  0.446],\n",
       "       [-1.679,  0.107, -0.923, -2.168,  0.643,  0.05 ,  0.025,  0.446],\n",
       "       [-2.156,  0.107, -0.873, -2.266,  0.645,  0.05 ,  0.025,  0.446]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph[ind]['nodes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 3., 4., 3., 4., 2., 1., 1., 2., 4., 2., 4., 3., 4., 1., 2.,\n",
       "       1., 3., 2., 2., 5., 1., 3., 4., 1., 3., 2., 4., 3., 1., 6.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusts = np.array(track_meta_data).squeeze()\n",
    "num_clusts[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    num_diff_clustE = len(np.unique(graph[i]['nodes'][:, -1]))\n",
    "    assert(num_diff_clustE==num_clusts[i, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "ind = 6\n",
    "row = graph[ind]['senders']\n",
    "col = graph[ind]['receivers']\n",
    "values = np.ones(len(row))\n",
    "num_nodes = graph[ind]['nodes'].shape[0]\n",
    "out = sparse.coo_matrix((values, (row, col)), (num_nodes, num_nodes))\n",
    "out.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(out.A, cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 8)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = graph[0]['nodes']\n",
    "fake_E = 3.4\n",
    "fake_E = np.repeat([[fake_E]], len(temp), axis=0)\n",
    "temp = np.hstack([temp, fake_E])\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_E.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiCaloTrackInferData:\n",
    "    \"\"\"DataGenerator class for extracting and formating data from list of root files\"\"\"\n",
    "    def __init__(self,\n",
    "                 pion_file: str,\n",
    "                 cellGeo_file: str,\n",
    "                 batch_size: int,\n",
    "                 n_clusters: int,\n",
    "                 shuffle: bool = True,\n",
    "                 num_procs: int = 32,\n",
    "                 preprocess: bool = False,\n",
    "                 output_dir: str = None):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        self.pion_file = pion_file\n",
    "        self.num_files = 1\n",
    "        \n",
    "        self.cellGeo_file = cellGeo_file\n",
    "        \n",
    "        self.cellGeo_data = ur.open(self.cellGeo_file)['CellGeo']\n",
    "        self.geoFeatureNames = self.cellGeo_data.keys()[1:9]\n",
    "        self.nodeFeatureNames = ['cluster_cell_E', *self.geoFeatureNames[:-2], 'cluster_E']\n",
    "        self.num_nodeFeatures = len(self.nodeFeatureNames)\n",
    "\n",
    "        self.edgeFeatureNames = self.cellGeo_data.keys()[9:]\n",
    "        self.num_edgeFeatures = len(self.edgeFeatureNames)\n",
    "\n",
    "        self.cellGeo_data = self.cellGeo_data.arrays(library='np')\n",
    "        self.cellGeo_ID = self.cellGeo_data['cell_geo_ID'][0]\n",
    "        self.sorter = np.argsort(self.cellGeo_ID)\n",
    "        \n",
    "        self.track_feature_names = ['trackPt','trackD0','trackZ0', 'trackEta_EMB2','trackPhi_EMB2',\n",
    "                                    'trackEta','trackPhi','truthPartE', 'truthPartPt', 'num_clusters']\n",
    "        self.cluster_feature_names = ['cluster_E', 'cluster_Eta', 'cluster_Phi', 'cluster_ENG_CALIB_TOT', \n",
    "                                      'cluster_EM_PROBABILITY','cluster_E_LCCalib','cluster_HAD_WEIGHT', 'deltaR']\n",
    "        \n",
    "        self.dr_thresh = 1.2\n",
    "        self.clusterThresh = .5\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "        if self.shuffle: np.random.shuffle(self.file_list)\n",
    "        \n",
    "        self.num_procs = num_procs\n",
    "        self.procs = []\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            os.makedirs(self.output_dir, exist_ok=True)\n",
    "            self.preprocess_data()\n",
    "    \n",
    "    def get_cluster_inds(self, event_data, event_ind):\n",
    "        \n",
    "        if self.n_clusters==-1:   # get all nodes satisfying dR criterion\n",
    "            c_inds = range(event_data['nCluster'][event_ind])\n",
    "            c_inds = [c for c in c_inds if (event_data['deltaR'][event_ind][c]<self.dr_thresh) and \n",
    "                      (event_data['cluster_E'][event_ind][c]>self.clusterThresh)]\n",
    "        else:                # get n leading nodes satisfying dR criterion\n",
    "            c_inds = np.argsort(event_data['cluster_E'][event_ind])[::-1]\n",
    "            c_inds = [c for c in c_inds if (event_data['deltaR'][event_ind][c]<self.dr_thresh) and \n",
    "                      (event_data['cluster_E'][event_ind][c]>self.clusterThresh)]\n",
    "            c_inds = c_inds[:self.n_clusters]\n",
    "        \n",
    "        return c_inds\n",
    "    \n",
    "    def get_meta(self, event_data, event_ind, c_inds):\n",
    "        \"\"\" \n",
    "        Reading meta data\n",
    "        \"\"\"  \n",
    "        track_meta_data = []\n",
    "        for f in self.track_feature_names[:-1]:\n",
    "            track_meta_data.append(event_data[f][event_ind])\n",
    "        track_meta_data.append(len(c_inds))\n",
    "        \n",
    "        cluster_meta_data = []\n",
    "        for c in c_inds:\n",
    "            curr_meta = []\n",
    "            \n",
    "            for f in self.cluster_feature_names:\n",
    "                curr_meta.append(event_data[f][event_ind][c])\n",
    "            \n",
    "            cluster_meta_data.append(curr_meta)\n",
    "            \n",
    "        return np.array(track_meta_data, dtype=np.float32), np.array(cluster_meta_data, dtype=np.float32)\n",
    "    \n",
    "    def get_nodes(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading Node features \"\"\" \n",
    "\n",
    "        cell_IDs = event_data['cluster_cell_ID'][event_ind][cluster_ind]\n",
    "        cell_IDmap = self.sorter[np.searchsorted(self.cellGeo_ID, cell_IDs, sorter=self.sorter)]\n",
    "        \n",
    "        nodes = np.log10(event_data['cluster_cell_E'][event_ind][cluster_ind])\n",
    "        \n",
    "        # Scaling the cell_geo_sampling by 28\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_sampling'][0][cell_IDmap]/28.)\n",
    "        for f in self.nodeFeatureNames[2:4]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "        # Scaling the cell_geo_rPerp by 3000\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_rPerp'][0][cell_IDmap]/3000.)\n",
    "        for f in self.nodeFeatureNames[5:-1]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "        \n",
    "        nodes = np.reshape(nodes, (len(self.nodeFeatureNames)-1, -1)).T\n",
    "        cluster_E = np.log10(event_data['cluster_E'][event_ind][cluster_ind])\n",
    "        cluster_E = np.repeat([[cluster_E]], len(nodes), axis=0)\n",
    "        nodes = np.hstack([nodes, cluster_E])\n",
    "\n",
    "        cluster_num_nodes = len(nodes)\n",
    "        \n",
    "        return nodes, cluster_num_nodes, cell_IDmap\n",
    "                     \n",
    "    def get_edges(self, cluster_num_nodes, cell_IDmap):\n",
    "        \"\"\" \n",
    "        Reading edge features \n",
    "        Resturns senders, receivers, and edges    \n",
    "        \"\"\" \n",
    "        \n",
    "        edge_inds = np.zeros((cluster_num_nodes, self.num_edgeFeatures))\n",
    "        for i, f in enumerate(self.edgeFeatureNames):\n",
    "            edge_inds[:, i] = self.cellGeo_data[f][0][cell_IDmap]\n",
    "        edge_inds[np.logical_not(np.isin(edge_inds, cell_IDmap))] = np.nan\n",
    "        \n",
    "        senders, edge_on_inds = np.isin(edge_inds, cell_IDmap).nonzero()\n",
    "        cluster_num_edges = len(senders)\n",
    "        edges = np.zeros((cluster_num_edges, self.num_edgeFeatures))\n",
    "        edges[np.arange(cluster_num_edges), edge_on_inds] = 1\n",
    "        \n",
    "        cell_IDmap_sorter = np.argsort(cell_IDmap)\n",
    "        rank = np.searchsorted(cell_IDmap, edge_inds , sorter=cell_IDmap_sorter)\n",
    "        receivers = cell_IDmap_sorter[rank[rank!=cluster_num_nodes]]\n",
    "        \n",
    "        return senders, receivers, edges\n",
    "\n",
    "    def preprocessor(self, worker_id):\n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            print(f\"Proceesing file {os.path.basename(self.pion_file)}\")\n",
    "            file = self.pion_file\n",
    "            event_data = pd.read_pickle(file)\n",
    "            num_events = len(event_data)\n",
    "\n",
    "            preprocessed_data = []\n",
    "\n",
    "            for event_ind in event_data.index:\n",
    "                truth_particle_E = np.log10(event_data['truthPartE'][event_ind][0]) # first one is the pion! \n",
    "                trackPt = event_data['trackPt'][event_ind][0]\n",
    "                if trackPt>5000:\n",
    "                    continue\n",
    "                    \n",
    "                trackEta = event_data['trackEta'][event_ind][0]\n",
    "                global_node = np.array([np.log10(trackPt), trackEta], dtype=np.float32)\n",
    "        \n",
    "                c_inds = self.get_cluster_inds(event_data, event_ind)\n",
    "                if not len(c_inds):\n",
    "                    continue\n",
    "                \n",
    "                nodes = []\n",
    "                senders = []\n",
    "                receivers = []\n",
    "                edges = []\n",
    "                offset = 0\n",
    "                for c in c_inds:\n",
    "                    curr_nodes, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, c)\n",
    "                    curr_senders, curr_receivers, curr_edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
    "                    \n",
    "                    nodes.append(curr_nodes)\n",
    "                    edges.append(curr_edges)\n",
    "                    senders.append(curr_senders + offset)\n",
    "                    receivers.append(curr_receivers + offset)\n",
    "                    \n",
    "                    offset += len(curr_nodes)\n",
    "                \n",
    "                nodes = np.concatenate(nodes)\n",
    "                edges = np.concatenate(edges)\n",
    "                senders = np.concatenate(senders)\n",
    "                receivers = np.concatenate(receivers)\n",
    "                \n",
    "                track_meta_data, cluster_meta_data = self.get_meta(event_data, event_ind, c_inds)\n",
    "                \n",
    "                graph = {'nodes': nodes.astype(np.float32), \n",
    "                         'globals': global_node.astype(np.float32),\n",
    "                         'senders': senders.astype(np.int32), \n",
    "                         'receivers': receivers.astype(np.int32),\n",
    "                         'edges': edges.astype(np.float32)}\n",
    "                target = truth_particle_E.astype(np.float32)\n",
    "\n",
    "                preprocessed_data.append((graph, target, track_meta_data, cluster_meta_data))\n",
    "\n",
    "            random.shuffle(preprocessed_data)\n",
    "\n",
    "            pickle.dump(preprocessed_data, open(self.output_dir + f'data_{file_num:03d}.p', 'wb'), compression='gzip')\n",
    "\n",
    "            print(f\"Finished processing {file_num} files\")\n",
    "            file_num += self.num_procs\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        print('\\nPreprocessing and saving data to {}'.format(self.output_dir))\n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.preprocessor, args=(i,), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()\n",
    "\n",
    "        self.file_list = [self.output_dir + f'data_{i:03d}.p' for i in range(self.num_files)]\n",
    "\n",
    "    def preprocessed_worker(self, worker_id, batch_queue):\n",
    "        batch_graphs = []\n",
    "        batch_targets = []\n",
    "        batch_track_meta = []\n",
    "        batch_cluster_meta = []\n",
    "        \n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
    "\n",
    "            for i in range(len(file_data)):\n",
    "                batch_graphs.append(file_data[i][0])\n",
    "                batch_targets.append(file_data[i][1])\n",
    "                batch_track_meta.append(file_data[i][2])\n",
    "                batch_cluster_meta.append(file_data[i][3])\n",
    "                    \n",
    "                if len(batch_graphs) == self.batch_size:\n",
    "                    batch_targets = np.array(batch_targets).astype(np.float32)\n",
    "                    batch_queue.put((batch_graphs, batch_targets, batch_track_meta, batch_cluster_meta))\n",
    "                    \n",
    "                    batch_graphs = []\n",
    "                    batch_targets = []\n",
    "                    batch_track_meta = []\n",
    "                    batch_cluster_meta = []\n",
    "\n",
    "            file_num += self.num_procs\n",
    "                    \n",
    "        if len(batch_graphs) > 0:\n",
    "            batch_targets = np.array(batch_targets).astype(np.float32)\n",
    "            batch_queue.put((batch_graphs, batch_targets, batch_track_meta, batch_cluster_meta))\n",
    "\n",
    "    def worker(self, worker_id, batch_queue):\n",
    "        if self.preprocess:\n",
    "            self.preprocessed_worker(worker_id, batch_queue)\n",
    "        else:\n",
    "            raise Exception('Preprocessing is required for combined classification/regression models.')\n",
    "        \n",
    "    def check_procs(self):\n",
    "        for p in self.procs:\n",
    "            if p.is_alive(): return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def kill_procs(self):\n",
    "        for p in self.procs:\n",
    "            p.kill()\n",
    "\n",
    "        self.procs = []\n",
    "    \n",
    "    def generator(self):\n",
    "        # for file in self.file_list:\n",
    "        batch_queue = Queue(2 * self.num_procs)\n",
    "            \n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.worker, args=(i, batch_queue), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        while self.check_procs() or not batch_queue.empty():\n",
    "            try:\n",
    "                batch = batch_queue.get(True, 0.0001)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            yield batch\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing and saving data to ./\n",
      "Proceesing file df_transformer_debug.pkl\n",
      "Finished processing 0 files\n"
     ]
    }
   ],
   "source": [
    "pion_file = '../df_transformer_debug.pkl'\n",
    "cell_geo_file = '/usr/workspace/hip/ML4Jets/regression_images/graph_examples/cell_geo.root'\n",
    "\n",
    "data_gen = MultiCaloTrackInferData(pion_file=pion_file,\n",
    "                                   cellGeo_file=cell_geo_file,\n",
    "                                   batch_size=32,\n",
    "                                   n_clusters=-1,\n",
    "                                   shuffle=False,\n",
    "                                   num_procs=32,\n",
    "                                   preprocess=True,\n",
    "                                   output_dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, target, track_meta_data, cluster_meta_data = next(data_gen.generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.59222865,  6.6680737 , 27.934269  , -0.6480477 ,  0.8865697 ,\n",
       "       -0.628881  , -0.4802343 ,  9.119415  ,  1.8925352 ,  1.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_meta_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trackPt',\n",
       " 'trackD0',\n",
       " 'trackZ0',\n",
       " 'trackEta_EMB2',\n",
       " 'trackPhi_EMB2',\n",
       " 'trackEta',\n",
       " 'trackPhi',\n",
       " 'truthPartE',\n",
       " 'truthPartPt',\n",
       " 'num_clusters']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gen.track_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../df_transformer_debug.npz', allow_pickle=True)\n",
    "event_data = data['data']\n",
    "cols = data['columns']\n",
    "index = data['index']\n",
    "# num_events = len(event_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(data=event_data, columns=cols, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.to_pickle('../df_transformer_debug.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_pickle('../df_transformer_debug.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'runNumber', 'eventNumber', 'lumiBlock', 'coreFlags',\n",
       "       'mcEventNumber', 'mcChannelNumber', 'mcEventWeight', 'nTruthPart',\n",
       "       'G4PreCalo_n_EM',\n",
       "       ...\n",
       "       'cluster_CENTER_LAMBDA', 'cluster_ISOLATION',\n",
       "       'cluster_ENERGY_DigiHSTruth', 'cluster_cell_ID', 'cluster_cell_E',\n",
       "       'cluster_hitsTruthIndex', 'cluster_hitsTruthE', 'deltaR', 'dR_pass',\n",
       "       'event_number'],\n",
       "      dtype='object', length=129)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trackPt</th>\n",
       "      <th>trackD0</th>\n",
       "      <th>trackZ0</th>\n",
       "      <th>trackEta_EMB2</th>\n",
       "      <th>trackPhi_EMB2</th>\n",
       "      <th>trackEta</th>\n",
       "      <th>trackPhi</th>\n",
       "      <th>truthPartE</th>\n",
       "      <th>truthPartPt</th>\n",
       "      <th>cluster_E</th>\n",
       "      <th>cluster_Eta</th>\n",
       "      <th>cluster_Phi</th>\n",
       "      <th>deltaR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4109</th>\n",
       "      <td>[0.56499124]</td>\n",
       "      <td>[-7.4432144]</td>\n",
       "      <td>[87.2737]</td>\n",
       "      <td>[-0.292916]</td>\n",
       "      <td>[-0.09242209]</td>\n",
       "      <td>[-0.2805243]</td>\n",
       "      <td>[-1.5919197]</td>\n",
       "      <td>[1.638227]</td>\n",
       "      <td>[0.6287291]</td>\n",
       "      <td>[0.5934837]</td>\n",
       "      <td>[-0.25973687]</td>\n",
       "      <td>[-0.74060816]</td>\n",
       "      <td>[0.8515652624760468]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9082</th>\n",
       "      <td>[0.59222865]</td>\n",
       "      <td>[6.6680737]</td>\n",
       "      <td>[27.934269]</td>\n",
       "      <td>[-0.6480477]</td>\n",
       "      <td>[0.8865697]</td>\n",
       "      <td>[-0.628881]</td>\n",
       "      <td>[-0.4802343]</td>\n",
       "      <td>[9.119415]</td>\n",
       "      <td>[1.8925352]</td>\n",
       "      <td>[0.82536364]</td>\n",
       "      <td>[-0.6838584]</td>\n",
       "      <td>[0.3976988]</td>\n",
       "      <td>[0.8796527826676044]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3985</th>\n",
       "      <td>[0.7130422]</td>\n",
       "      <td>[-3.952645]</td>\n",
       "      <td>[22.175701]</td>\n",
       "      <td>[-1000000000.0]</td>\n",
       "      <td>[-1000000000.0]</td>\n",
       "      <td>[-1.9086738]</td>\n",
       "      <td>[2.9050717]</td>\n",
       "      <td>[29.691723]</td>\n",
       "      <td>[8.562422]</td>\n",
       "      <td>[0.66128916]</td>\n",
       "      <td>[-1.5136156]</td>\n",
       "      <td>[3.1018682]</td>\n",
       "      <td>[0.4413612757326127]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8254</th>\n",
       "      <td>[436.06116]</td>\n",
       "      <td>[0.0017354734]</td>\n",
       "      <td>[25.76988]</td>\n",
       "      <td>[0.01986047]</td>\n",
       "      <td>[-1.1808629]</td>\n",
       "      <td>[0.019860815]</td>\n",
       "      <td>[-1.1825681]</td>\n",
       "      <td>[499.59772]</td>\n",
       "      <td>[499.4997]</td>\n",
       "      <td>[131.32864, 42.44467, 5.466173, 3.823332, 2.44...</td>\n",
       "      <td>[0.02523974, 0.032032937, 0.013351178, 0.01600...</td>\n",
       "      <td>[-1.1544378, -1.1846708, -1.2848923, -0.943424...</td>\n",
       "      <td>[0.028639940728819663, 0.012352409796186377, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592</th>\n",
       "      <td>[1.1556271]</td>\n",
       "      <td>[9.772532]</td>\n",
       "      <td>[-44.83223]</td>\n",
       "      <td>[-1000000000.0]</td>\n",
       "      <td>[-1000000000.0]</td>\n",
       "      <td>[-2.182635]</td>\n",
       "      <td>[0.7438593]</td>\n",
       "      <td>[100.29169]</td>\n",
       "      <td>[10.957157]</td>\n",
       "      <td>[1.7634326]</td>\n",
       "      <td>[-1.6830969]</td>\n",
       "      <td>[-0.09693346]</td>\n",
       "      <td>[0.9779932768951003]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8425</th>\n",
       "      <td>[1118.2161]</td>\n",
       "      <td>[-0.004356428]</td>\n",
       "      <td>[-1.2269732]</td>\n",
       "      <td>[-0.9787819]</td>\n",
       "      <td>[-0.9380014]</td>\n",
       "      <td>[-0.97878176]</td>\n",
       "      <td>[-0.9386091]</td>\n",
       "      <td>[995.07166]</td>\n",
       "      <td>[655.1402]</td>\n",
       "      <td>[119.363686, 7.9105496, 5.7105308, 3.256291]</td>\n",
       "      <td>[-1.0072887, -1.0556288, -1.0082572, -1.1504852]</td>\n",
       "      <td>[-0.9378377, -0.9186243, -0.77520394, -1.1300832]</td>\n",
       "      <td>[0.028517369896244267, 0.07940313518030324, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           trackPt         trackD0       trackZ0    trackEta_EMB2  \\\n",
       "4109  [0.56499124]    [-7.4432144]     [87.2737]      [-0.292916]   \n",
       "9082  [0.59222865]     [6.6680737]   [27.934269]     [-0.6480477]   \n",
       "3985   [0.7130422]     [-3.952645]   [22.175701]  [-1000000000.0]   \n",
       "8254   [436.06116]  [0.0017354734]    [25.76988]     [0.01986047]   \n",
       "3592   [1.1556271]      [9.772532]   [-44.83223]  [-1000000000.0]   \n",
       "8425   [1118.2161]  [-0.004356428]  [-1.2269732]     [-0.9787819]   \n",
       "\n",
       "        trackPhi_EMB2       trackEta      trackPhi   truthPartE  truthPartPt  \\\n",
       "4109    [-0.09242209]   [-0.2805243]  [-1.5919197]   [1.638227]  [0.6287291]   \n",
       "9082      [0.8865697]    [-0.628881]  [-0.4802343]   [9.119415]  [1.8925352]   \n",
       "3985  [-1000000000.0]   [-1.9086738]   [2.9050717]  [29.691723]   [8.562422]   \n",
       "8254     [-1.1808629]  [0.019860815]  [-1.1825681]  [499.59772]   [499.4997]   \n",
       "3592  [-1000000000.0]    [-2.182635]   [0.7438593]  [100.29169]  [10.957157]   \n",
       "8425     [-0.9380014]  [-0.97878176]  [-0.9386091]  [995.07166]   [655.1402]   \n",
       "\n",
       "                                              cluster_E  \\\n",
       "4109                                        [0.5934837]   \n",
       "9082                                       [0.82536364]   \n",
       "3985                                       [0.66128916]   \n",
       "8254  [131.32864, 42.44467, 5.466173, 3.823332, 2.44...   \n",
       "3592                                        [1.7634326]   \n",
       "8425       [119.363686, 7.9105496, 5.7105308, 3.256291]   \n",
       "\n",
       "                                            cluster_Eta  \\\n",
       "4109                                      [-0.25973687]   \n",
       "9082                                       [-0.6838584]   \n",
       "3985                                       [-1.5136156]   \n",
       "8254  [0.02523974, 0.032032937, 0.013351178, 0.01600...   \n",
       "3592                                       [-1.6830969]   \n",
       "8425   [-1.0072887, -1.0556288, -1.0082572, -1.1504852]   \n",
       "\n",
       "                                            cluster_Phi  \\\n",
       "4109                                      [-0.74060816]   \n",
       "9082                                        [0.3976988]   \n",
       "3985                                        [3.1018682]   \n",
       "8254  [-1.1544378, -1.1846708, -1.2848923, -0.943424...   \n",
       "3592                                      [-0.09693346]   \n",
       "8425  [-0.9378377, -0.9186243, -0.77520394, -1.1300832]   \n",
       "\n",
       "                                                 deltaR  \n",
       "4109                               [0.8515652624760468]  \n",
       "9082                               [0.8796527826676044]  \n",
       "3985                               [0.4413612757326127]  \n",
       "8254  [0.028639940728819663, 0.012352409796186377, 0...  \n",
       "3592                               [0.9779932768951003]  \n",
       "8425  [0.028517369896244267, 0.07940313518030324, 0....  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[['trackPt','trackD0','trackZ0', 'trackEta_EMB2','trackPhi_EMB2',\n",
    "     'trackEta','trackPhi','truthPartE', 'truthPartPt', 'cluster_E', \n",
    "     'cluster_Eta', 'cluster_Phi', 'deltaR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventNumber</th>\n",
       "      <th>truthPartEta</th>\n",
       "      <th>truthPartPhi</th>\n",
       "      <th>trackEta</th>\n",
       "      <th>trackPhi</th>\n",
       "      <th>cluster_Eta</th>\n",
       "      <th>cluster_Phi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4109</th>\n",
       "      <td>8805468</td>\n",
       "      <td>[1.607805]</td>\n",
       "      <td>[-1.7966828]</td>\n",
       "      <td>[-0.2805243]</td>\n",
       "      <td>[-1.5919197]</td>\n",
       "      <td>[-0.25973687]</td>\n",
       "      <td>[-0.74060816]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9082</th>\n",
       "      <td>1744200</td>\n",
       "      <td>[-2.25457]</td>\n",
       "      <td>[-0.19429713]</td>\n",
       "      <td>[-0.628881]</td>\n",
       "      <td>[-0.4802343]</td>\n",
       "      <td>[-0.6838584]</td>\n",
       "      <td>[0.3976988]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3985</th>\n",
       "      <td>6740038</td>\n",
       "      <td>[-1.9151504]</td>\n",
       "      <td>[2.8025804]</td>\n",
       "      <td>[-1.9086738]</td>\n",
       "      <td>[2.9050717]</td>\n",
       "      <td>[-1.5136156]</td>\n",
       "      <td>[3.1018682]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8254</th>\n",
       "      <td>9453782</td>\n",
       "      <td>[0.019807491]</td>\n",
       "      <td>[-1.1825013]</td>\n",
       "      <td>[0.019860815]</td>\n",
       "      <td>[-1.1825681]</td>\n",
       "      <td>[0.02523974, 0.032032937, 0.013351178, 0.01600...</td>\n",
       "      <td>[-1.1544378, -1.1846708, -1.2848923, -0.943424...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592</th>\n",
       "      <td>6070536</td>\n",
       "      <td>[-2.9042387]</td>\n",
       "      <td>[1.1400995]</td>\n",
       "      <td>[-2.182635]</td>\n",
       "      <td>[0.7438593]</td>\n",
       "      <td>[-1.6830969]</td>\n",
       "      <td>[-0.09693346]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8425</th>\n",
       "      <td>2149922</td>\n",
       "      <td>[-0.9791123]</td>\n",
       "      <td>[-0.9387261]</td>\n",
       "      <td>[-0.97878176]</td>\n",
       "      <td>[-0.9386091]</td>\n",
       "      <td>[-1.0072887, -1.0556288, -1.0082572, -1.1504852]</td>\n",
       "      <td>[-0.9378377, -0.9186243, -0.77520394, -1.1300832]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     eventNumber   truthPartEta   truthPartPhi       trackEta      trackPhi  \\\n",
       "4109     8805468     [1.607805]   [-1.7966828]   [-0.2805243]  [-1.5919197]   \n",
       "9082     1744200     [-2.25457]  [-0.19429713]    [-0.628881]  [-0.4802343]   \n",
       "3985     6740038   [-1.9151504]    [2.8025804]   [-1.9086738]   [2.9050717]   \n",
       "8254     9453782  [0.019807491]   [-1.1825013]  [0.019860815]  [-1.1825681]   \n",
       "3592     6070536   [-2.9042387]    [1.1400995]    [-2.182635]   [0.7438593]   \n",
       "8425     2149922   [-0.9791123]   [-0.9387261]  [-0.97878176]  [-0.9386091]   \n",
       "\n",
       "                                            cluster_Eta  \\\n",
       "4109                                      [-0.25973687]   \n",
       "9082                                       [-0.6838584]   \n",
       "3985                                       [-1.5136156]   \n",
       "8254  [0.02523974, 0.032032937, 0.013351178, 0.01600...   \n",
       "3592                                       [-1.6830969]   \n",
       "8425   [-1.0072887, -1.0556288, -1.0082572, -1.1504852]   \n",
       "\n",
       "                                            cluster_Phi  \n",
       "4109                                      [-0.74060816]  \n",
       "9082                                        [0.3976988]  \n",
       "3985                                        [3.1018682]  \n",
       "8254  [-1.1544378, -1.1846708, -1.2848923, -0.943424...  \n",
       "3592                                      [-0.09693346]  \n",
       "8425  [-0.9378377, -0.9186243, -0.77520394, -1.1300832]  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[['eventNumber', 'truthPartEta', 'truthPartPhi', 'trackEta', 'trackPhi', 'cluster_Eta', 'cluster_Phi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4109                                         [0.66069525]\n",
       "9082                                          [1.7295669]\n",
       "3985                                          [1.7862197]\n",
       "8254    [164.663, 47.580204, 11.02904, 6.647328, 4.434...\n",
       "3592                                          [5.3970976]\n",
       "8425          [144.80768, 10.055521, 8.0233965, 4.009662]\n",
       "Name: cluster_E_LCCalib, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp['cluste']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>GNN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.638227</td>\n",
       "      <td>0.609559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.119415</td>\n",
       "      <td>0.731308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.691723</td>\n",
       "      <td>2.518506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.291718</td>\n",
       "      <td>5.359109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>499.597565</td>\n",
       "      <td>405.521271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>995.071350</td>\n",
       "      <td>1319.857422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Target          GNN\n",
       "0    1.638227     0.609559\n",
       "1    9.119415     0.731308\n",
       "2   29.691723     2.518506\n",
       "3  100.291718     5.359109\n",
       "4  499.597565   405.521271\n",
       "5  995.071350  1319.857422"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = np.load('../results/onetrack_multicluster/Block_20220613_2046_trackMultiCalo_regress/inference_predictions_debug_GNN.npz')\n",
    "ind = np.argsort(res['targets'])\n",
    "pd.DataFrame(list(zip(res['targets'][ind], res['outputs'].squeeze()[ind])), columns=['Target', 'GNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.638227, 0.59623784),\n",
       " (9.119415, 0.726395),\n",
       " (29.691723, 2.5295691),\n",
       " (100.29172, 5.16362),\n",
       " (499.59756, 437.69775),\n",
       " (995.07135, 1242.4777)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_deep = np.load('../results/onetrack_multicluster/Block_20220711_1016_trackMultiCalo_regress_deepset/inference_predictions_debug_Deepsets.npz')\n",
    "ind_deep = np.argsort(res_deep['targets'])\n",
    "list(zip(res_deep['targets'][ind_deep], res_deep['outputs'].squeeze()[ind_deep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_res = np.vstack([res['targets'][ind], res['outputs'].squeeze()[ind], res_deep['outputs'].squeeze()[ind_deep]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>GNN</th>\n",
       "      <th>Deepsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.638227</td>\n",
       "      <td>0.609559</td>\n",
       "      <td>0.596238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.119415</td>\n",
       "      <td>0.731308</td>\n",
       "      <td>0.726395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.691723</td>\n",
       "      <td>2.518506</td>\n",
       "      <td>2.529569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.291718</td>\n",
       "      <td>5.359109</td>\n",
       "      <td>5.163620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>499.597565</td>\n",
       "      <td>405.521271</td>\n",
       "      <td>437.697754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>995.071350</td>\n",
       "      <td>1319.857422</td>\n",
       "      <td>1242.477661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Target          GNN     Deepsets\n",
       "0    1.638227     0.609559     0.596238\n",
       "1    9.119415     0.731308     0.726395\n",
       "2   29.691723     2.518506     2.529569\n",
       "3  100.291718     5.359109     5.163620\n",
       "4  499.597565   405.521271   437.697754\n",
       "5  995.071350  1319.857422  1242.477661"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=stacked_res.T, columns=['Target', 'GNN', 'Deepsets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.59223,  6.66807, 27.93427, -0.64805,  0.88657, -0.62888,\n",
       "        -0.48023,  9.11942,  1.89254,  1.     ], dtype=float32),\n",
       " array([ 0.56499, -7.44321, 87.2737 , -0.29292, -0.09242, -0.28052,\n",
       "        -1.59192,  1.63823,  0.62873,  1.     ], dtype=float32),\n",
       " array([1118.2161 ,   -0.00436,   -1.22697,   -0.97878,   -0.938  ,\n",
       "          -0.97878,   -0.93861,  995.07166,  655.1402 ,    4.     ],\n",
       "       dtype=float32),\n",
       " array([436.06116,   0.00174,  25.76988,   0.01986,  -1.18086,   0.01986,\n",
       "         -1.18257, 499.59772, 499.4997 ,   5.     ], dtype=float32),\n",
       " array([ 7.13042e-01, -3.95265e+00,  2.21757e+01, -1.00000e+09,\n",
       "        -1.00000e+09, -1.90867e+00,  2.90507e+00,  2.96917e+01,\n",
       "         8.56242e+00,  1.00000e+00], dtype=float32),\n",
       " array([ 1.15563e+00,  9.77253e+00, -4.48322e+01, -1.00000e+09,\n",
       "        -1.00000e+09, -2.18264e+00,  7.43859e-01,  1.00292e+02,\n",
       "         1.09572e+01,  1.00000e+00], dtype=float32)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track_meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opence",
   "language": "python",
   "name": "opence"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
