{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp modules.data_trackMultiCalo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import uproot as ur\n",
    "import time\n",
    "from multiprocessing import Process, Queue, set_start_method\n",
    "import compress_pickle as pickle\n",
    "from scipy.stats import circmean\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphDataGenerator\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiCaloTrackDataGenerator:\n",
    "    \"\"\"DataGenerator class for extracting and formating data from list of root files\"\"\"\n",
    "    def __init__(self,\n",
    "                 pion_file_list: list,\n",
    "                 cellGeo_file: str,\n",
    "                 batch_size: int,\n",
    "                 n_clusters: int,\n",
    "                 shuffle: bool = True,\n",
    "                 num_procs: int = 32,\n",
    "                 preprocess: bool = False,\n",
    "                 output_dir: str = None):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "\n",
    "        self.preprocess = preprocess\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            self.pion_file_list = pion_file_list\n",
    "            self.num_files = len(self.pion_file_list)\n",
    "        else:\n",
    "            self.file_list = pion_file_list\n",
    "            self.num_files = len(self.file_list)\n",
    "        \n",
    "        self.cellGeo_file = cellGeo_file\n",
    "        \n",
    "        self.cellGeo_data = ur.open(self.cellGeo_file)['CellGeo']\n",
    "        self.geoFeatureNames = self.cellGeo_data.keys()[1:9]\n",
    "        self.nodeFeatureNames = ['cluster_cell_E', *self.geoFeatureNames[:-2], 'cluster_E']\n",
    "        self.num_nodeFeatures = len(self.nodeFeatureNames)\n",
    "\n",
    "        self.edgeFeatureNames = self.cellGeo_data.keys()[9:]\n",
    "        self.num_edgeFeatures = len(self.edgeFeatureNames)\n",
    "\n",
    "        self.cellGeo_data = self.cellGeo_data.arrays(library='np')\n",
    "        self.cellGeo_ID = self.cellGeo_data['cell_geo_ID'][0]\n",
    "        self.sorter = np.argsort(self.cellGeo_ID)\n",
    "        \n",
    "        self.track_feature_names = ['trackPt','trackD0','trackZ0', 'trackEta_EMB2','trackPhi_EMB2',\n",
    "                                    'trackEta','trackPhi','truthPartE', 'truthPartPt', 'num_clusters']\n",
    "        self.cluster_feature_names = ['cluster_E', 'cluster_Eta', 'cluster_Phi', 'cluster_ENG_CALIB_TOT', \n",
    "                                      'cluster_EM_PROBABILITY','cluster_E_LCCalib','cluster_HAD_WEIGHT', 'dR']\n",
    "        \n",
    "        self.dr_thresh = 1.2\n",
    "        self.clusterThresh = .5\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_clusters = n_clusters\n",
    "        \n",
    "        if self.shuffle: np.random.shuffle(self.file_list)\n",
    "        \n",
    "        self.num_procs = num_procs\n",
    "        self.procs = []\n",
    "\n",
    "        if self.preprocess and self.output_dir is not None:\n",
    "            os.makedirs(self.output_dir, exist_ok=True)\n",
    "            self.preprocess_data()\n",
    "    \n",
    "    def get_cluster_inds(self, event_data, event_ind):\n",
    "        \n",
    "        if self.n_clusters==-1:   # get all nodes satisfying dR criterion\n",
    "            c_inds = range(event_data['nCluster'][event_ind])\n",
    "            c_inds = [c for c in c_inds if (event_data['dR'][event_ind][c]<self.dr_thresh) and \n",
    "                      (event_data['cluster_E'][event_ind][c]>self.clusterThresh)]\n",
    "        else:                # get n leading nodes satisfying dR criterion\n",
    "            c_inds = np.argsort(event_data['cluster_E'][event_ind])[::-1]\n",
    "            c_inds = [c for c in c_inds if (event_data['dR'][event_ind][c]<self.dr_thresh) and \n",
    "                      (event_data['cluster_E'][event_ind][c]>self.clusterThresh)]\n",
    "            c_inds = c_inds[:self.n_clusters]\n",
    "        \n",
    "        return c_inds\n",
    "    \n",
    "    def get_meta(self, event_data, event_ind, c_inds):\n",
    "        \"\"\" \n",
    "        Reading meta data\n",
    "        \"\"\"  \n",
    "        track_meta_data = []\n",
    "        for f in self.track_feature_names[:-1]:\n",
    "            track_meta_data.append(event_data[f][event_ind])\n",
    "        track_meta_data.append(len(c_inds))\n",
    "        \n",
    "        cluster_meta_data = []\n",
    "        for c in c_inds:\n",
    "            curr_meta = []\n",
    "            \n",
    "            for f in self.cluster_feature_names:\n",
    "                curr_meta.append(event_data[f][event_ind][c])\n",
    "            \n",
    "            cluster_meta_data.append(curr_meta)\n",
    "            \n",
    "        return np.array(track_meta_data, dtype=np.float32), np.array(cluster_meta_data, dtype=np.float32)\n",
    "    \n",
    "    def get_nodes(self, event_data, event_ind, cluster_ind):\n",
    "        \"\"\" Reading Node features \"\"\" \n",
    "\n",
    "        cell_IDs = event_data['cluster_cell_ID'][event_ind][cluster_ind]\n",
    "        cell_IDmap = self.sorter[np.searchsorted(self.cellGeo_ID, cell_IDs, sorter=self.sorter)]\n",
    "        \n",
    "        nodes = np.log10(event_data['cluster_cell_E'][event_ind][cluster_ind])\n",
    "        \n",
    "        # Scaling the cell_geo_sampling by 28\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_sampling'][0][cell_IDmap]/28.)\n",
    "        for f in self.nodeFeatureNames[2:4]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "        # Scaling the cell_geo_rPerp by 3000\n",
    "        nodes = np.append(nodes, self.cellGeo_data['cell_geo_rPerp'][0][cell_IDmap]/3000.)\n",
    "        for f in self.nodeFeatureNames[5:-1]:\n",
    "            nodes = np.append(nodes, self.cellGeo_data[f][0][cell_IDmap])\n",
    "        \n",
    "        nodes = np.reshape(nodes, (len(self.nodeFeatureNames)-1, -1)).T\n",
    "        cluster_E = np.log10(event_data['cluster_E'][event_ind][cluster_ind])\n",
    "        cluster_E = np.repeat([[cluster_E]], len(nodes), axis=0)\n",
    "        nodes = np.hstack([nodes, cluster_E])\n",
    "\n",
    "        cluster_num_nodes = len(nodes)\n",
    "        \n",
    "        return nodes, cluster_num_nodes, cell_IDmap\n",
    "                     \n",
    "    def get_edges(self, cluster_num_nodes, cell_IDmap):\n",
    "        \"\"\" \n",
    "        Reading edge features \n",
    "        Resturns senders, receivers, and edges    \n",
    "        \"\"\" \n",
    "        \n",
    "        edge_inds = np.zeros((cluster_num_nodes, self.num_edgeFeatures))\n",
    "        for i, f in enumerate(self.edgeFeatureNames):\n",
    "            edge_inds[:, i] = self.cellGeo_data[f][0][cell_IDmap]\n",
    "        edge_inds[np.logical_not(np.isin(edge_inds, cell_IDmap))] = np.nan\n",
    "        \n",
    "        senders, edge_on_inds = np.isin(edge_inds, cell_IDmap).nonzero()\n",
    "        cluster_num_edges = len(senders)\n",
    "        edges = np.zeros((cluster_num_edges, self.num_edgeFeatures))\n",
    "        edges[np.arange(cluster_num_edges), edge_on_inds] = 1\n",
    "        \n",
    "        cell_IDmap_sorter = np.argsort(cell_IDmap)\n",
    "        rank = np.searchsorted(cell_IDmap, edge_inds , sorter=cell_IDmap_sorter)\n",
    "        receivers = cell_IDmap_sorter[rank[rank!=cluster_num_nodes]]\n",
    "        \n",
    "        return senders, receivers, edges\n",
    "\n",
    "    def preprocessor(self, worker_id):\n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            print(f\"Proceesing file {os.path.basename(self.pion_file_list[file_num])}\")\n",
    "            file = self.pion_file_list[file_num]\n",
    "            event_data = np.load(file, allow_pickle=True).item()\n",
    "            num_events = len(event_data[[key for key in event_data.keys()][0]])\n",
    "\n",
    "            preprocessed_data = []\n",
    "\n",
    "            for event_ind in range(num_events):\n",
    "                truth_particle_E = np.log10(event_data['truthPartE'][event_ind][0]) # first one is the pion! \n",
    "                trackPt = event_data['trackPt'][event_ind][0]\n",
    "                if trackPt>5000:\n",
    "                    continue\n",
    "                    \n",
    "                trackEta = event_data['trackEta'][event_ind][0]\n",
    "                global_node = np.array([np.log10(trackPt), trackEta], dtype=np.float32)\n",
    "        \n",
    "                c_inds = self.get_cluster_inds(event_data, event_ind)\n",
    "                if not len(c_inds):\n",
    "                    continue\n",
    "                \n",
    "                nodes = []\n",
    "                senders = []\n",
    "                receivers = []\n",
    "                edges = []\n",
    "                offset = 0\n",
    "                for c in c_inds:\n",
    "                    curr_nodes, cluster_num_nodes, cell_IDmap = self.get_nodes(event_data, event_ind, c)\n",
    "                    curr_senders, curr_receivers, curr_edges = self.get_edges(cluster_num_nodes, cell_IDmap)\n",
    "                    \n",
    "                    nodes.append(curr_nodes)\n",
    "                    edges.append(curr_edges)\n",
    "                    senders.append(curr_senders + offset)\n",
    "                    receivers.append(curr_receivers + offset)\n",
    "                    \n",
    "                    offset += len(curr_nodes)\n",
    "                \n",
    "                nodes = np.concatenate(nodes)\n",
    "                edges = np.concatenate(edges)\n",
    "                senders = np.concatenate(senders)\n",
    "                receivers = np.concatenate(receivers)\n",
    "                \n",
    "                track_meta_data, cluster_meta_data = self.get_meta(event_data, event_ind, c_inds)\n",
    "                \n",
    "                graph = {'nodes': nodes.astype(np.float32), \n",
    "                         'globals': global_node.astype(np.float32),\n",
    "                         'senders': senders.astype(np.int32), \n",
    "                         'receivers': receivers.astype(np.int32),\n",
    "                         'edges': edges.astype(np.float32)}\n",
    "                target = truth_particle_E.astype(np.float32)\n",
    "\n",
    "                preprocessed_data.append((graph, target, track_meta_data, cluster_meta_data))\n",
    "\n",
    "            random.shuffle(preprocessed_data)\n",
    "\n",
    "            pickle.dump(preprocessed_data, open(self.output_dir + f'data_{file_num:03d}.p', 'wb'), compression='gzip')\n",
    "\n",
    "            print(f\"Finished processing {file_num} files\")\n",
    "            file_num += self.num_procs\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        print('\\nPreprocessing and saving data to {}'.format(self.output_dir))\n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.preprocessor, args=(i,), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()\n",
    "\n",
    "        self.file_list = [self.output_dir + f'data_{i:03d}.p' for i in range(self.num_files)]\n",
    "\n",
    "    def preprocessed_worker(self, worker_id, batch_queue):\n",
    "        batch_graphs = []\n",
    "        batch_targets = []\n",
    "        batch_track_meta = []\n",
    "        batch_cluster_meta = []\n",
    "        \n",
    "        file_num = worker_id\n",
    "        while file_num < self.num_files:\n",
    "            file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
    "\n",
    "            for i in range(len(file_data)):\n",
    "                batch_graphs.append(file_data[i][0])\n",
    "                batch_targets.append(file_data[i][1])\n",
    "                batch_track_meta.append(file_data[i][2])\n",
    "                batch_cluster_meta.append(file_data[i][3])\n",
    "                    \n",
    "                if len(batch_graphs) == self.batch_size:\n",
    "                    batch_targets = np.array(batch_targets).astype(np.float32)\n",
    "                    batch_queue.put((batch_graphs, batch_targets, batch_track_meta, batch_cluster_meta))\n",
    "                    \n",
    "                    batch_graphs = []\n",
    "                    batch_targets = []\n",
    "                    batch_track_meta = []\n",
    "                    batch_cluster_meta = []\n",
    "\n",
    "            file_num += self.num_procs\n",
    "                    \n",
    "        if len(batch_graphs) > 0:\n",
    "            batch_targets = np.array(batch_targets).astype(np.float32)\n",
    "            batch_queue.put((batch_graphs, batch_targets, batch_track_meta, batch_cluster_meta))\n",
    "\n",
    "    def worker(self, worker_id, batch_queue):\n",
    "        if self.preprocess:\n",
    "            self.preprocessed_worker(worker_id, batch_queue)\n",
    "        else:\n",
    "            raise Exception('Preprocessing is required for combined classification/regression models.')\n",
    "        \n",
    "    def check_procs(self):\n",
    "        for p in self.procs:\n",
    "            if p.is_alive(): return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def kill_procs(self):\n",
    "        for p in self.procs:\n",
    "            p.kill()\n",
    "\n",
    "        self.procs = []\n",
    "    \n",
    "    def generator(self):\n",
    "        # for file in self.file_list:\n",
    "        batch_queue = Queue(2 * self.num_procs)\n",
    "            \n",
    "        for i in range(self.num_procs):\n",
    "            p = Process(target=self.worker, args=(i, batch_queue), daemon=True)\n",
    "            p.start()\n",
    "            self.procs.append(p)\n",
    "        \n",
    "        while self.check_procs() or not batch_queue.empty():\n",
    "            try:\n",
    "                batch = batch_queue.get(True, 0.0001)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            yield batch\n",
    "        \n",
    "        for p in self.procs:\n",
    "            p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the data generation step..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pion_dir = '/usr/workspace/hip/ML4Jets/regression_images/graphs.v01-45-gaa27bcb/onetrack_multicluster/pion_files/'\n",
    "pion_files = np.sort(glob.glob(pion_dir+\"*.npy\"))\n",
    "n_files = 3\n",
    "\n",
    "cell_geo_file = '/usr/workspace/hip/ML4Jets/regression_images/graph_examples/cell_geo.root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing and saving data to ./\n",
      "Proceesing file 001.npy\n",
      "Proceesing file 002.npy\n",
      "Proceesing file 003.npy\n",
      "Finished processing 2 files\n",
      "Finished processing 1 files\n",
      "Finished processing 0 files\n"
     ]
    }
   ],
   "source": [
    "data_gen = MultiCaloTrackDataGenerator(pion_file_list=pion_files[:n_files],\n",
    "                                       cellGeo_file=cell_geo_file,\n",
    "                                       batch_size=32,\n",
    "                                       n_clusters=-1,\n",
    "                                       shuffle=False,\n",
    "                                       num_procs=32,\n",
    "                                       preprocess=True,\n",
    "                                       output_dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, target, track_meta_data, cluster_meta_data = next(data_gen.generator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.471,  0.429, -0.85 , -2.209,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-0.404,  0.429, -0.85 , -2.307,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-0.436,  0.429, -0.85 , -2.111,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-0.44 ,  0.429, -0.95 , -2.307,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.6  ,  0.429, -0.75 , -2.307,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-0.908,  0.429, -0.95 , -2.111,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.046,  0.429, -0.75 , -2.111,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-2.133,  0.464, -0.85 , -2.209,  0.932,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.457,  0.107, -0.873, -2.168,  0.645,  0.05 ,  0.025,  0.69 ],\n",
       "       [-1.572,  0.429, -0.85 , -2.405,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.289,  0.464, -0.75 , -2.307,  1.007,  0.1  ,  0.098,  0.69 ],\n",
       "       [-2.061,  0.464, -0.85 , -2.307,  0.932,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.7  ,  0.107, -0.823, -2.34 ,  0.649,  0.05 ,  0.025,  0.69 ],\n",
       "       [-1.508,  0.107, -0.823, -2.291,  0.649,  0.05 ,  0.025,  0.69 ],\n",
       "       [-1.679,  0.107, -0.823, -2.266,  0.649,  0.05 ,  0.025,  0.69 ],\n",
       "       [-1.564,  0.429, -0.85 , -2.013,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-0.795,  0.429, -0.95 , -2.013,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [-2.208,  0.464, -0.85 , -2.111,  0.932,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.744,  0.107, -0.823, -2.144,  0.649,  0.05 ,  0.025,  0.69 ],\n",
       "       [-1.822,  0.107, -0.823, -2.07 ,  0.649,  0.05 ,  0.025,  0.69 ],\n",
       "       [-1.407,  0.107, -0.873, -2.144,  0.645,  0.05 ,  0.025,  0.69 ],\n",
       "       [-2.156,  0.107, -0.873, -2.119,  0.645,  0.05 ,  0.025,  0.69 ],\n",
       "       [-1.917,  0.607, -1.157, -2.307,  0.826,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.628,  0.607, -1.157, -2.405,  0.826,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.977,  0.607, -1.057, -2.307,  0.936,  0.1  ,  0.098,  0.69 ],\n",
       "       [-1.921,  0.107, -0.974, -2.315,  0.641,  0.05 ,  0.025,  0.69 ],\n",
       "       [-2.156,  0.107, -0.974, -2.291,  0.641,  0.05 ,  0.025,  0.69 ],\n",
       "       [-2.25 ,  0.607, -1.057, -2.209,  0.936,  0.1  ,  0.098,  0.69 ],\n",
       "       [-0.374,  0.429, -0.95 , -2.209,  0.817,  0.1  ,  0.098,  0.69 ],\n",
       "       [ 0.122,  0.107, -0.923, -2.217,  0.643,  0.05 ,  0.025,  0.446],\n",
       "       [-1.794,  0.107, -0.923, -2.242,  0.643,  0.05 ,  0.025,  0.446],\n",
       "       [-0.101,  0.107, -0.923, -2.193,  0.643,  0.05 ,  0.025,  0.446],\n",
       "       [-1.347,  0.107, -0.973, -2.217,  0.641,  0.05 ,  0.025,  0.446],\n",
       "       [-1.538,  0.107, -0.973, -2.193,  0.641,  0.05 ,  0.025,  0.446],\n",
       "       [-0.545,  0.071, -0.911, -2.217,  0.575,  0.025,  0.025,  0.446],\n",
       "       [-1.421,  0.071, -0.936, -2.217,  0.574,  0.025,  0.025,  0.446],\n",
       "       [-2.298,  0.107, -0.973, -2.168,  0.641,  0.05 ,  0.025,  0.446],\n",
       "       [-0.954,  0.071, -0.911, -2.193,  0.575,  0.025,  0.025,  0.446],\n",
       "       [-1.337,  0.071, -0.936, -2.193,  0.574,  0.025,  0.025,  0.446],\n",
       "       [-1.317,  0.071, -0.911, -2.242,  0.575,  0.025,  0.025,  0.446],\n",
       "       [-1.744,  0.071, -0.886, -2.217,  0.577,  0.025,  0.025,  0.446],\n",
       "       [-2.   ,  0.071, -0.886, -2.242,  0.577,  0.025,  0.025,  0.446],\n",
       "       [-2.156,  0.036, -0.902, -2.205,  0.509,  0.003,  0.098,  0.446],\n",
       "       [-1.267,  0.036, -0.909, -2.205,  0.509,  0.003,  0.098,  0.446],\n",
       "       [-1.286,  0.071, -0.911, -2.168,  0.575,  0.025,  0.025,  0.446],\n",
       "       [-1.679,  0.107, -0.873, -2.242,  0.645,  0.05 ,  0.025,  0.446],\n",
       "       [-1.744,  0.107, -0.873, -2.193,  0.645,  0.05 ,  0.025,  0.446],\n",
       "       [-1.957,  0.107, -0.873, -2.217,  0.645,  0.05 ,  0.025,  0.446],\n",
       "       [-1.679,  0.107, -0.923, -2.168,  0.643,  0.05 ,  0.025,  0.446],\n",
       "       [-2.156,  0.107, -0.873, -2.266,  0.645,  0.05 ,  0.025,  0.446]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph[ind]['nodes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 3., 4., 3., 4., 2., 1., 1., 2., 4., 2., 4., 3., 4., 1., 2.,\n",
       "       1., 3., 2., 2., 5., 1., 3., 4., 1., 3., 2., 4., 3., 1., 6.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusts = np.array(track_meta_data).squeeze()\n",
    "num_clusts[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    num_diff_clustE = len(np.unique(graph[i]['nodes'][:, -1]))\n",
    "    assert(num_diff_clustE==num_clusts[i, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "ind = 6\n",
    "row = graph[ind]['senders']\n",
    "col = graph[ind]['receivers']\n",
    "values = np.ones(len(row))\n",
    "num_nodes = graph[ind]['nodes'].shape[0]\n",
    "out = sparse.coo_matrix((values, (row, col)), (num_nodes, num_nodes))\n",
    "out.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(out.A, cmap='Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 8)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = graph[0]['nodes']\n",
    "fake_E = 3.4\n",
    "fake_E = np.repeat([[fake_E]], len(temp), axis=0)\n",
    "temp = np.hstack([temp, fake_E])\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72, 1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_E.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opence",
   "language": "python",
   "name": "opence"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
