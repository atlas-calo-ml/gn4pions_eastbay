{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import uproot as ur\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets.graphs import GraphsTuple\n",
    "import sonnet as snt\n",
    "import argparse\n",
    "import yaml\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "from gn4pions.modules.data import GraphDataGenerator\n",
    "from gn4pions.modules.models import MultiOutWeightedRegressModel\n",
    "from gn4pions.modules.utils import convert_to_tuple\n",
    "\n",
    "sns.set_context('poster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model config\n",
    "config_file = 'gn4pions/configs/test.yaml' # for a quick run of the notebook\n",
    "# config_file = 'gn4pions/configs/baseline.yaml' # for actual training\n",
    "config = yaml.load(open(config_file), Loader=yaml.FullLoader)\n",
    "\n",
    "# Data config\n",
    "data_config = config['data']\n",
    "\n",
    "data_dir = data_config['data_dir']\n",
    "num_train_files = data_config['num_train_files']\n",
    "num_val_files = data_config['num_val_files']\n",
    "batch_size = data_config['batch_size']\n",
    "shuffle = data_config['shuffle']\n",
    "num_procs = data_config['num_procs']\n",
    "preprocess = data_config['preprocess']\n",
    "output_dir = data_config['output_dir']\n",
    "already_preprocessed = data_config['already_preprocessed']  # Set to false when running training for first time\n",
    "\n",
    "# Model Config\n",
    "model_config = config['model']\n",
    "\n",
    "concat_input = model_config['concat_input']\n",
    "\n",
    "\n",
    "# Traning Config\n",
    "train_config = config['training']\n",
    "\n",
    "epochs = train_config['epochs']\n",
    "learning_rate = train_config['learning_rate']\n",
    "alpha = train_config['alpha']\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(train_config['gpu'])\n",
    "log_freq = train_config['log_freq']\n",
    "save_dir = train_config['save_dir'] + config_file.replace('.yaml','').split('/')[-1] + '_' + time.strftime(\"%Y%m%d\")\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "yaml.dump(config, open(save_dir + '/config.yaml', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and create data generators\n",
    "\n",
    "pi0_files = np.sort(glob.glob(data_dir+'*pi0*/*root'))\n",
    "pion_files = np.sort(glob.glob(data_dir+'*pion*/*root'))\n",
    "\n",
    "train_start = 10\n",
    "train_end = train_start + num_train_files\n",
    "val_end = train_end + num_val_files\n",
    "\n",
    "pi0_train_files = pi0_files[train_start:train_end]\n",
    "pi0_val_files = pi0_files[train_end:val_end]\n",
    "pion_train_files = pion_files[train_start:train_end]\n",
    "pion_val_files = pion_files[train_end:val_end]\n",
    "\n",
    "train_output_dir = None\n",
    "val_output_dir = None\n",
    "\n",
    "# Get Data\n",
    "if preprocess:\n",
    "    train_output_dir = output_dir + '/train/'\n",
    "    val_output_dir = output_dir + '/val/'\n",
    "\n",
    "    if already_preprocessed:\n",
    "        train_files = np.sort(glob.glob(train_output_dir+'*.p'))[:num_train_files]\n",
    "        val_files = np.sort(glob.glob(val_output_dir+'*.p'))[:num_val_files]\n",
    "\n",
    "        pi0_train_files = train_files\n",
    "        pi0_val_files = val_files\n",
    "        pion_train_files = None\n",
    "        pion_val_files = None\n",
    "\n",
    "        train_output_dir = None\n",
    "        val_output_dir = None\n",
    "\n",
    "# Traning Data Generator\n",
    "# Will preprocess data if it doesnt find pickled files\n",
    "data_gen_train = GraphDataGenerator(pi0_file_list=pi0_train_files,\n",
    "                                    pion_file_list=pion_train_files,\n",
    "                                    cellGeo_file=data_dir+'cell_geo.root',\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=shuffle,\n",
    "                                    num_procs=num_procs,\n",
    "                                    preprocess=preprocess,\n",
    "                                    output_dir=train_output_dir)\n",
    "\n",
    "# Validation Data generator\n",
    "# Will preprocess data if it doesnt find pickled files\n",
    "data_gen_val = GraphDataGenerator(pi0_file_list=pi0_val_files,\n",
    "                                  pion_file_list=pion_val_files,\n",
    "                                  cellGeo_file=data_dir+'cell_geo.root',\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=shuffle,\n",
    "                                  num_procs=num_procs,\n",
    "                                  preprocess=preprocess,\n",
    "                                  output_dir=val_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get batch of data\n",
    "def get_batch(data_iter):\n",
    "    for graphs, targets in data_iter:\n",
    "        graphs = convert_to_tuple(graphs)\n",
    "        targets = tf.convert_to_tensor(targets)\n",
    "        yield graphs, targets\n",
    "        \n",
    "# Define loss function        \n",
    "mae_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def loss_fn(targets, regress_preds, class_preds):\n",
    "    regress_loss = mae_loss(targets[:,:1], regress_preds)\n",
    "    class_loss = bce_loss(targets[:,1:], class_preds)\n",
    "    combined_loss = alpha*regress_loss + (1 - alpha)*class_loss \n",
    "    return regress_loss, class_loss, combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 12:53:31.550685: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-10 12:53:32.063874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22834 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:41:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Get a sample graph for tf.function decorator\n",
    "samp_graph, samp_target = next(get_batch(data_gen_train.generator()))\n",
    "data_gen_train.kill_procs()\n",
    "graph_spec = utils_tf.specs_from_graphs_tuple(samp_graph, True, True, True)\n",
    "\n",
    "# Training set\n",
    "@tf.function(input_signature=[graph_spec, tf.TensorSpec(shape=[None,2], dtype=tf.float32)])\n",
    "def train_step(graphs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        regress_output, class_output = model(graphs)\n",
    "        regress_preds = regress_output.globals\n",
    "        class_preds = class_output.globals\n",
    "        regress_loss, class_loss, loss = loss_fn(targets, regress_preds, class_preds)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return regress_loss, class_loss, loss\n",
    "\n",
    "# Validation Step\n",
    "@tf.function(input_signature=[graph_spec, tf.TensorSpec(shape=[None,2], dtype=tf.float32)])\n",
    "def val_step(graphs, targets):\n",
    "    regress_output, class_output = model(graphs)\n",
    "    regress_preds = regress_output.globals\n",
    "    class_preds = class_output.globals\n",
    "    regress_loss, class_loss, loss = loss_fn(targets, regress_preds, class_preds)\n",
    "    return regress_loss, class_loss, loss, regress_preds, class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "model = MultiOutWeightedRegressModel(global_output_size=1, num_outputs=2, model_config=model_config)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Average epoch losses\n",
    "training_loss_epoch = []\n",
    "training_loss_regress_epoch = []\n",
    "training_loss_class_epoch = []\n",
    "val_loss_epoch = []\n",
    "val_loss_regress_epoch = []\n",
    "val_loss_class_epoch = []\n",
    "\n",
    "# Model checkpointing, load latest model if available\n",
    "checkpoint = tf.train.Checkpoint(module=model)\n",
    "checkpoint_prefix = os.path.join(save_dir, 'latest_model')\n",
    "latest = tf.train.latest_checkpoint(save_dir)\n",
    "if latest is not None:\n",
    "    checkpoint.restore(latest)\n",
    "else:\n",
    "    checkpoint.save(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting epoch: 0\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 12:53:38.849761: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 71), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 71), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2022-01-10 12:53:40.359374: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0001, Tr_loss_mean: 1.6351, Tr_loss_rg_mean: 1.8948, Tr_loss_cl_mean: 0.8559, Took 12.0675secs\n",
      "Iter: 0101, Tr_loss_mean: 0.3988, Tr_loss_rg_mean: 0.3203, Tr_loss_cl_mean: 0.6343, Took 16.9372secs\n",
      "Iter: 0201, Tr_loss_mean: 0.3479, Tr_loss_rg_mean: 0.2548, Tr_loss_cl_mean: 0.6271, Took 17.2365secs\n",
      "Iter: 0301, Tr_loss_mean: 0.3135, Tr_loss_rg_mean: 0.2117, Tr_loss_cl_mean: 0.6191, Took 17.6188secs\n",
      "Iter: 0401, Tr_loss_mean: 0.2886, Tr_loss_rg_mean: 0.1883, Tr_loss_cl_mean: 0.5896, Took 17.5053secs\n",
      "Iter: 0501, Tr_loss_mean: 0.2685, Tr_loss_rg_mean: 0.1729, Tr_loss_cl_mean: 0.5555, Took 16.5376secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1729, Val_loss_rg_mean: 0.1041, Val_loss_cl_mean: 0.3792, Took 3.8297secs\n",
      "Iter: 0101, Val_loss_mean: 0.1734, Val_loss_rg_mean: 0.1031, Val_loss_cl_mean: 0.3843, Took 10.2481secs\n",
      "Iter: 0201, Val_loss_mean: 0.1729, Val_loss_rg_mean: 0.1029, Val_loss_cl_mean: 0.3827, Took 9.5328secs\n",
      "Iter: 0301, Val_loss_mean: 0.1730, Val_loss_rg_mean: 0.1030, Val_loss_cl_mean: 0.3829, Took 9.4813secs\n",
      "Iter: 0401, Val_loss_mean: 0.1730, Val_loss_rg_mean: 0.1030, Val_loss_cl_mean: 0.3830, Took 9.5413secs\n",
      "Iter: 0501, Val_loss_mean: 0.1730, Val_loss_rg_mean: 0.1030, Val_loss_cl_mean: 0.3832, Took 9.5143secs\n",
      "\n",
      "Epoch 0 ended\n",
      "Training:  1:45\n",
      "Validation:  0:56\n",
      "Loss decreased from 100000.0000 to 0.1728\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_20220110\n",
      "\n",
      "\n",
      "Starting epoch: 1\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1727, Tr_loss_rg_mean: 0.1071, Tr_loss_cl_mean: 0.3694, Took 2.9100secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1714, Tr_loss_rg_mean: 0.1021, Tr_loss_cl_mean: 0.3793, Took 16.3316secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1744, Tr_loss_rg_mean: 0.1091, Tr_loss_cl_mean: 0.3704, Took 16.4419secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1742, Tr_loss_rg_mean: 0.1100, Tr_loss_cl_mean: 0.3665, Took 16.4571secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1713, Tr_loss_rg_mean: 0.1077, Tr_loss_cl_mean: 0.3622, Took 16.3881secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1683, Tr_loss_rg_mean: 0.1055, Tr_loss_cl_mean: 0.3565, Took 16.2672secs\n",
      "\n",
      "Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-50:\n",
      "Process Process-49:\n",
      "Process Process-46:\n",
      "Process Process-48:\n",
      "Traceback (most recent call last):\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 233, in worker\n",
      "    self.preprocessed_worker(worker_id, batch_queue)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 210, in preprocessed_worker\n",
      "    file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-45:\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/compress_pickle.py\", line 272, in load\n",
      "    output = uncompress_and_unpickle(\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-47:\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/functools.py\", line 877, in wrapper\n",
      "    return dispatch(args[0].__class__)(*args, **kw)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 233, in worker\n",
      "    self.preprocessed_worker(worker_id, batch_queue)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/io/base.py\", line 99, in default_uncompress_and_unpickle\n",
      "    return pickler.load(stream=compresser.get_stream(), **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-44:\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 233, in worker\n",
      "    self.preprocessed_worker(worker_id, batch_queue)\n",
      "Process Process-43:\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 210, in preprocessed_worker\n",
      "    file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/picklers/pickle.py\", line 45, in load\n",
      "    return pickle.load(stream, **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 233, in worker\n",
      "    self.preprocessed_worker(worker_id, batch_queue)\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 210, in preprocessed_worker\n",
      "    file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
      "Traceback (most recent call last):\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/compress_pickle.py\", line 272, in load\n",
      "    output = uncompress_and_unpickle(\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 300, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/compress_pickle.py\", line 272, in load\n",
      "    output = uncompress_and_unpickle(\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 210, in preprocessed_worker\n",
      "    file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
      "Traceback (most recent call last):\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/functools.py\", line 877, in wrapper\n",
      "    return dispatch(args[0].__class__)(*args, **kw)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/functools.py\", line 877, in wrapper\n",
      "    return dispatch(args[0].__class__)(*args, **kw)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/io/base.py\", line 99, in default_uncompress_and_unpickle\n",
      "    return pickler.load(stream=compresser.get_stream(), **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/compress_pickle.py\", line 272, in load\n",
      "    output = uncompress_and_unpickle(\n",
      "Traceback (most recent call last):\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 509, in read\n",
      "    self._add_read_data( uncompress )\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/io/base.py\", line 99, in default_uncompress_and_unpickle\n",
      "    return pickler.load(stream=compresser.get_stream(), **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/picklers/pickle.py\", line 45, in load\n",
      "    return pickle.load(stream, **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 233, in worker\n",
      "    self.preprocessed_worker(worker_id, batch_queue)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/functools.py\", line 877, in wrapper\n",
      "    return dispatch(args[0].__class__)(*args, **kw)\n",
      "Process Process-41:\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 514, in _add_read_data\n",
      "    self._crc = zlib.crc32(data, self._crc)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/picklers/pickle.py\", line 45, in load\n",
      "    return pickle.load(stream, **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 320, in peek\n",
      "    return self._buffer.peek(n)\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 210, in preprocessed_worker\n",
      "    file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 300, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/io/base.py\", line 99, in default_uncompress_and_unpickle\n",
      "    return pickler.load(stream=compresser.get_stream(), **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 233, in worker\n",
      "    self.preprocessed_worker(worker_id, batch_queue)\n",
      "Process Process-42:\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 233, in worker\n",
      "    self.preprocessed_worker(worker_id, batch_queue)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/compress_pickle.py\", line 272, in load\n",
      "    output = uncompress_and_unpickle(\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 233, in worker\n",
      "    self.preprocessed_worker(worker_id, batch_queue)\n",
      "KeyboardInterrupt\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 210, in preprocessed_worker\n",
      "    file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/picklers/pickle.py\", line 45, in load\n",
      "    return pickle.load(stream, **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/functools.py\", line 877, in wrapper\n",
      "    return dispatch(args[0].__class__)(*args, **kw)\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 210, in preprocessed_worker\n",
      "    file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 210, in preprocessed_worker\n",
      "    file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
      "Traceback (most recent call last):\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 509, in read\n",
      "    self._add_read_data( uncompress )\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/compress_pickle.py\", line 272, in load\n",
      "    output = uncompress_and_unpickle(\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/io/base.py\", line 99, in default_uncompress_and_unpickle\n",
      "    return pickler.load(stream=compresser.get_stream(), **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/compress_pickle.py\", line 272, in load\n",
      "    output = uncompress_and_unpickle(\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 300, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/compress_pickle.py\", line 272, in load\n",
      "    output = uncompress_and_unpickle(\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 514, in _add_read_data\n",
      "    self._crc = zlib.crc32(data, self._crc)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/functools.py\", line 877, in wrapper\n",
      "    return dispatch(args[0].__class__)(*args, **kw)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/picklers/pickle.py\", line 45, in load\n",
      "    return pickle.load(stream, **kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/functools.py\", line 877, in wrapper\n",
      "    return dispatch(args[0].__class__)(*args, **kw)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/_compression.py\", line 69, in readinto\n",
      "    byte_view[:len(data)] = data\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/functools.py\", line 877, in wrapper\n",
      "    return dispatch(args[0].__class__)(*args, **kw)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 300, in read\n",
      "    return self._buffer.read(size)\n",
      "KeyboardInterrupt\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/io/base.py\", line 99, in default_uncompress_and_unpickle\n",
      "    return pickler.load(stream=compresser.get_stream(), **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/io/base.py\", line 99, in default_uncompress_and_unpickle\n",
      "    return pickler.load(stream=compresser.get_stream(), **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 233, in worker\n",
      "    self.preprocessed_worker(worker_id, batch_queue)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/io/base.py\", line 99, in default_uncompress_and_unpickle\n",
      "    return pickler.load(stream=compresser.get_stream(), **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/picklers/pickle.py\", line 45, in load\n",
      "    return pickle.load(stream, **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 210, in preprocessed_worker\n",
      "    file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
      "KeyboardInterrupt\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 509, in read\n",
      "    self._add_read_data( uncompress )\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/picklers/pickle.py\", line 45, in load\n",
      "    return pickle.load(stream, **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/picklers/pickle.py\", line 45, in load\n",
      "    return pickle.load(stream, **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 300, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 233, in worker\n",
      "    self.preprocessed_worker(worker_id, batch_queue)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/compress_pickle.py\", line 272, in load\n",
      "    output = uncompress_and_unpickle(\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 514, in _add_read_data\n",
      "    self._crc = zlib.crc32(data, self._crc)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 300, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 300, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"/clusterfs/ml4hep/mpettee/ml4pions/gn4pions_eastbay/gn4pions/modules/data.py\", line 210, in preprocessed_worker\n",
      "    file_data = pickle.load(open(self.file_list[file_num], 'rb'), compression='gzip')\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/functools.py\", line 877, in wrapper\n",
      "    return dispatch(args[0].__class__)(*args, **kw)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "KeyboardInterrupt\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 509, in read\n",
      "    self._add_read_data( uncompress )\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/compress_pickle.py\", line 272, in load\n",
      "    output = uncompress_and_unpickle(\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/io/base.py\", line 99, in default_uncompress_and_unpickle\n",
      "    return pickler.load(stream=compresser.get_stream(), **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 503, in read\n",
      "    if uncompress != b\"\":\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 514, in _add_read_data\n",
      "    self._crc = zlib.crc32(data, self._crc)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/functools.py\", line 877, in wrapper\n",
      "    return dispatch(args[0].__class__)(*args, **kw)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/picklers/pickle.py\", line 45, in load\n",
      "    return pickle.load(stream, **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 495, in read\n",
      "    uncompress = self._decompressor.decompress(buf, size)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 296, in read\n",
      "    self._check_not_closed()\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/io/base.py\", line 99, in default_uncompress_and_unpickle\n",
      "    return pickler.load(stream=compresser.get_stream(), **kwargs)\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/_compression.py\", line 13, in _check_not_closed\n",
      "    if self.closed:\n",
      "KeyboardInterrupt\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/compress_pickle/picklers/pickle.py\", line 45, in load\n",
      "    return pickle.load(stream, **kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/gzip.py\", line 315, in peek\n",
      "    def peek(self, n):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15545/2565898385.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mepoch_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mall_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mall_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "curr_loss = 1e5\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    print(f'\\n\\nStarting epoch: {e}')\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Batchwise losses\n",
    "    training_loss = []\n",
    "    training_loss_regress = []\n",
    "    training_loss_class = []\n",
    "    val_loss = []\n",
    "    val_loss_regress = []\n",
    "    val_loss_class = []\n",
    "\n",
    "    # Train\n",
    "    print('Training...')\n",
    "    start = time.time()\n",
    "    for i, (graph_data_tr, targets_tr) in enumerate(get_batch(data_gen_train.generator())):\n",
    "        losses_tr_rg, losses_tr_cl, losses_tr = train_step(graph_data_tr, targets_tr)\n",
    "\n",
    "        training_loss.append(losses_tr.numpy())\n",
    "        training_loss_regress.append(losses_tr_rg.numpy())\n",
    "        training_loss_class.append(losses_tr_cl.numpy())\n",
    "\n",
    "        if not (i-1)%log_freq:\n",
    "            end = time.time()\n",
    "            print(f'Iter: {i:04d}, ', end='')\n",
    "            print(f'Tr_loss_mean: {np.mean(training_loss):.4f}, ', end='')\n",
    "            print(f'Tr_loss_rg_mean: {np.mean(training_loss_regress):.4f}, ', end='') \n",
    "            print(f'Tr_loss_cl_mean: {np.mean(training_loss_class):.4f}, ', end='') \n",
    "            print(f'Took {end-start:.4f}secs')\n",
    "            start = time.time()\n",
    "                  \n",
    "    training_loss_epoch.append(training_loss)\n",
    "    training_loss_regress_epoch.append(training_loss_regress)\n",
    "    training_loss_class_epoch.append(training_loss_class)\n",
    "    training_end = time.time()\n",
    "\n",
    "    # validate\n",
    "    print('\\nValidation...')\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    start = time.time()\n",
    "    for i, (graph_data_val, targets_val) in enumerate(get_batch(data_gen_val.generator())):\n",
    "        losses_val_rg, losses_val_cl, losses_val, regress_vals, class_vals = val_step(graph_data_val, targets_val)\n",
    "\n",
    "        targets_val = targets_val.numpy()\n",
    "        regress_vals = regress_vals.numpy()\n",
    "        class_vals = class_vals.numpy()\n",
    "\n",
    "        targets_val[:,0] = 10**targets_val[:,0]\n",
    "        regress_vals = 10**regress_vals\n",
    "        class_vals =  tf.math.sigmoid(class_vals)\n",
    "\n",
    "        output_vals = np.hstack([regress_vals, class_vals])\n",
    "\n",
    "        val_loss.append(losses_val.numpy())\n",
    "        val_loss_regress.append(losses_val_rg.numpy())\n",
    "        val_loss_class.append(losses_val_cl.numpy())\n",
    "\n",
    "        all_targets.append(targets_val)\n",
    "        all_outputs.append(output_vals)\n",
    "\n",
    "        if not (i-1)%log_freq:\n",
    "            end = time.time()\n",
    "            print(f'Iter: {i:04d}, ', end='')\n",
    "            print(f'Val_loss_mean: {np.mean(val_loss):.4f}, ', end='')\n",
    "            print(f'Val_loss_rg_mean: {np.mean(val_loss_regress):.4f}, ', end='') \n",
    "            print(f'Val_loss_cl_mean: {np.mean(val_loss_class):.4f}, ', end='') \n",
    "            print(f'Took {end-start:.4f}secs')\n",
    "            start = time.time()\n",
    "\n",
    "    epoch_end = time.time()\n",
    "\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "\n",
    "    val_loss_epoch.append(val_loss)\n",
    "    val_loss_regress_epoch.append(val_loss_regress)\n",
    "    val_loss_class_epoch.append(val_loss_class)\n",
    "\n",
    "    \n",
    "    # Book keeping\n",
    "    val_mins = int((epoch_end - training_end)/60)\n",
    "    val_secs = int((epoch_end - training_end)%60)\n",
    "    training_mins = int((training_end - epoch_start)/60)\n",
    "    training_secs = int((training_end - epoch_start)%60)\n",
    "    print(f'\\nEpoch {e} ended')\n",
    "    print(f'Training: {training_mins:2d}:{training_secs:02d}')\n",
    "    print(f'Validation: {val_mins:2d}:{val_secs:02d}')\n",
    "    \n",
    "    \n",
    "    # Save losses\n",
    "    np.savez(save_dir+'/losses', \n",
    "            training=training_loss_epoch, validation=val_loss_epoch,\n",
    "            training_regress=training_loss_regress_epoch, validation_regress=val_loss_regress_epoch,\n",
    "            training_class=training_loss_class_epoch, validation_class=val_loss_class_epoch,\n",
    "            )\n",
    "\n",
    "    \n",
    "    # Checkpoint if validation loss improved\n",
    "    if np.mean(val_loss)<curr_loss:\n",
    "        print(f'Loss decreased from {curr_loss:.4f} to {np.mean(val_loss):.4f}')\n",
    "        print(f'Checkpointing and saving predictions to:\\n{save_dir}')\n",
    "        curr_loss = np.mean(val_loss)\n",
    "        np.savez(save_dir+'/predictions', \n",
    "                targets=all_targets, \n",
    "                outputs=all_outputs)\n",
    "        checkpoint.save(checkpoint_prefix)\n",
    "    else: \n",
    "        print(f'Loss didnt decrease from {curr_loss:.4f}')\n",
    "    \n",
    "    \n",
    "    # Decrease learning rate every few epochs\n",
    "    if not (e+1)%2:   #%20:\n",
    "        optimizer.learning_rate = optimizer.learning_rate/10\n",
    "        print(f'Learning rate decreased to: {optimizer.learning_rate.value():.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "nbdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
