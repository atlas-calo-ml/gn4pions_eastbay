{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import uproot as ur\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets.graphs import GraphsTuple\n",
    "import sonnet as snt\n",
    "import argparse\n",
    "import yaml\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gn4pions.modules.data import GraphDataGenerator\n",
    "from gn4pions.modules.models import MultiOutWeightedRegressModel\n",
    "from gn4pions.modules.utils import convert_to_tuple\n",
    "\n",
    "sns.set_context('poster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model config\n",
    "# config_file = 'gn4pions/configs/test.yaml' # for a quick run of the notebook\n",
    "# config_file = 'gn4pions/configs/test-nosampling.yaml' # no cell_geo_sampling input\n",
    "# config_file = 'gn4pions/configs/baseline.yaml' # for actual training\n",
    "config_file = 'gn4pions/configs/baseline-nosampling.yaml' # no cell_geo_sampling input\n",
    "config = yaml.load(open(config_file), Loader=yaml.FullLoader)\n",
    "\n",
    "# Data config\n",
    "data_config = config['data']\n",
    "\n",
    "data_dir = data_config['data_dir']\n",
    "num_train_files = data_config['num_train_files']\n",
    "num_val_files = data_config['num_val_files']\n",
    "batch_size = data_config['batch_size']\n",
    "shuffle = data_config['shuffle']\n",
    "num_procs = data_config['num_procs']\n",
    "preprocess = data_config['preprocess']\n",
    "output_dir = data_config['output_dir']\n",
    "already_preprocessed = data_config['already_preprocessed']  # Set to false when running training for first time\n",
    "\n",
    "# Model Config\n",
    "model_config = config['model']\n",
    "concat_input = model_config['concat_input']\n",
    "\n",
    "# Traning Config\n",
    "train_config = config['training']\n",
    "\n",
    "epochs = train_config['epochs']\n",
    "learning_rate = train_config['learning_rate']\n",
    "alpha = train_config['alpha']\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(train_config['gpu'])\n",
    "log_freq = train_config['log_freq']\n",
    "save_dir = train_config['save_dir'] + config_file.replace('.yaml','').split('/')[-1] + '_' + time.strftime(\"%Y%m%d\")\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "yaml.dump(config, open(save_dir + '/config.yaml', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files: ['/clusterfs/ml4hep/mpettee/ml4pions/data//train_nosampling/data_000.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//train_nosampling/data_001.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//train_nosampling/data_002.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//train_nosampling/data_003.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//train_nosampling/data_004.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//train_nosampling/data_005.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//train_nosampling/data_006.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//train_nosampling/data_007.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//train_nosampling/data_008.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//train_nosampling/data_009.p']\n",
      "Validation files: ['/clusterfs/ml4hep/mpettee/ml4pions/data//val_nosampling/data_000.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//val_nosampling/data_001.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//val_nosampling/data_002.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//val_nosampling/data_003.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//val_nosampling/data_004.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//val_nosampling/data_005.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//val_nosampling/data_006.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//val_nosampling/data_007.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//val_nosampling/data_008.p'\n",
      " '/clusterfs/ml4hep/mpettee/ml4pions/data//val_nosampling/data_009.p']\n"
     ]
    }
   ],
   "source": [
    "# Read data and create data generators\n",
    "\n",
    "pi0_files = np.sort(glob.glob(data_dir+'*pi0*/*root'))\n",
    "pion_files = np.sort(glob.glob(data_dir+'*pion*/*root'))\n",
    "\n",
    "train_start = 0\n",
    "train_end = train_start + num_train_files\n",
    "val_end = train_end + num_val_files\n",
    "\n",
    "pi0_train_files = pi0_files[train_start:train_end]\n",
    "pi0_val_files = pi0_files[train_end:val_end]\n",
    "pion_train_files = pion_files[train_start:train_end]\n",
    "pion_val_files = pion_files[train_end:val_end]\n",
    "\n",
    "train_output_dir = None\n",
    "val_output_dir = None\n",
    "\n",
    "print(\"Pi0 training files:\", pi0_train_files)\n",
    "print(\"Pi0 validation files:\", pi0_val_files)\n",
    "print(\"Pion training files:\", pion_train_files)\n",
    "print(\"Pion validation files:\", pion_val_files)\n",
    "\n",
    "# Get Data\n",
    "if preprocess:\n",
    "    train_output_dir = output_dir + '/train_nosampling/'\n",
    "    val_output_dir = output_dir + '/val_nosampling/'\n",
    "\n",
    "    if already_preprocessed:\n",
    "        train_files = np.sort(glob.glob(train_output_dir+'*.p'))[:num_train_files]\n",
    "        val_files = np.sort(glob.glob(val_output_dir+'*.p'))[:num_val_files]\n",
    "        \n",
    "        pi0_train_files = train_files\n",
    "        pi0_val_files = val_files\n",
    "        pion_train_files = None\n",
    "        pion_val_files = None\n",
    "\n",
    "        train_output_dir = None\n",
    "        val_output_dir = None\n",
    "\n",
    "# Traning Data Generator\n",
    "# Will preprocess data if it doesnt find pickled files\n",
    "data_gen_train = GraphDataGenerator(pi0_file_list=pi0_train_files,\n",
    "                                    pion_file_list=pion_train_files,\n",
    "                                    cellGeo_file=data_dir+'cell_geo.root',\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=shuffle,\n",
    "                                    num_procs=num_procs,\n",
    "                                    preprocess=preprocess,\n",
    "                                    output_dir=train_output_dir)\n",
    "\n",
    "# Validation Data generator\n",
    "# Will preprocess data if it doesnt find pickled files\n",
    "data_gen_val = GraphDataGenerator(pi0_file_list=pi0_val_files,\n",
    "                                  pion_file_list=pion_val_files,\n",
    "                                  cellGeo_file=data_dir+'cell_geo.root',\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=shuffle,\n",
    "                                  num_procs=num_procs,\n",
    "                                  preprocess=preprocess,\n",
    "                                  output_dir=val_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get batch of data\n",
    "def get_batch(data_iter):\n",
    "    for graphs, targets in data_iter:\n",
    "        graphs = convert_to_tuple(graphs)\n",
    "        targets = tf.convert_to_tensor(targets)\n",
    "        yield graphs, targets\n",
    "        \n",
    "# Define loss function        \n",
    "mae_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def loss_fn(targets, regress_preds, class_preds):\n",
    "    regress_loss = mae_loss(targets[:,:1], regress_preds)\n",
    "    class_loss = bce_loss(targets[:,1:], class_preds)\n",
    "    combined_loss = alpha*regress_loss + (1 - alpha)*class_loss \n",
    "    return regress_loss, class_loss, combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 15:52:23.442342: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-14 15:52:23.894895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43662 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:41:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Get a sample graph for tf.function decorator\n",
    "samp_graph, samp_target = next(get_batch(data_gen_train.generator()))\n",
    "data_gen_train.kill_procs()\n",
    "graph_spec = utils_tf.specs_from_graphs_tuple(samp_graph, True, True, True)\n",
    "\n",
    "# Training set\n",
    "@tf.function(input_signature=[graph_spec, tf.TensorSpec(shape=[None,2], dtype=tf.float32)])\n",
    "def train_step(graphs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        regress_output, class_output = model(graphs)\n",
    "        regress_preds = regress_output.globals\n",
    "        class_preds = class_output.globals\n",
    "        regress_loss, class_loss, loss = loss_fn(targets, regress_preds, class_preds)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return regress_loss, class_loss, loss\n",
    "\n",
    "# Validation Step\n",
    "@tf.function(input_signature=[graph_spec, tf.TensorSpec(shape=[None,2], dtype=tf.float32)])\n",
    "def val_step(graphs, targets):\n",
    "    regress_output, class_output = model(graphs)\n",
    "    regress_preds = regress_output.globals\n",
    "    class_preds = class_output.globals\n",
    "    regress_loss, class_loss, loss = loss_fn(targets, regress_preds, class_preds)\n",
    "    return regress_loss, class_loss, loss, regress_preds, class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "model = MultiOutWeightedRegressModel(global_output_size=1, num_outputs=2, model_config=model_config)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Average epoch losses\n",
    "training_loss_epoch = []\n",
    "training_loss_regress_epoch = []\n",
    "training_loss_class_epoch = []\n",
    "val_loss_epoch = []\n",
    "val_loss_regress_epoch = []\n",
    "val_loss_class_epoch = []\n",
    "\n",
    "# Model checkpointing, load latest model if available\n",
    "checkpoint = tf.train.Checkpoint(module=model)\n",
    "checkpoint_prefix = os.path.join(save_dir, 'latest_model')\n",
    "latest = tf.train.latest_checkpoint(save_dir)\n",
    "if latest is not None:\n",
    "    checkpoint.restore(latest)\n",
    "else:\n",
    "    checkpoint.save(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (graph_data_tr, targets_tr) in enumerate(get_batch(data_gen_train.generator())):\n",
    "#     print(\"Targets: \",targets_tr[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting epoch: 0\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 15:52:30.844257: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 70), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 70), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2022-02-14 15:52:32.203210: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-02-14 15:52:35.737127: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0001, Tr_loss_mean: 0.4698, Tr_loss_rg_mean: 0.4317, Tr_loss_cl_mean: 0.5841, Took 11.8219secs\n",
      "Iter: 0101, Tr_loss_mean: 0.2706, Tr_loss_rg_mean: 0.1653, Tr_loss_cl_mean: 0.5866, Took 15.7886secs\n",
      "Iter: 0201, Tr_loss_mean: 0.2574, Tr_loss_rg_mean: 0.1531, Tr_loss_cl_mean: 0.5701, Took 15.7367secs\n",
      "Iter: 0301, Tr_loss_mean: 0.2520, Tr_loss_rg_mean: 0.1479, Tr_loss_cl_mean: 0.5643, Took 15.6917secs\n",
      "Iter: 0401, Tr_loss_mean: 0.2475, Tr_loss_rg_mean: 0.1439, Tr_loss_cl_mean: 0.5586, Took 15.6518secs\n",
      "Iter: 0501, Tr_loss_mean: 0.2430, Tr_loss_rg_mean: 0.1404, Tr_loss_cl_mean: 0.5510, Took 15.4332secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.2067, Val_loss_rg_mean: 0.1136, Val_loss_cl_mean: 0.4860, Took 3.7583secs\n",
      "Iter: 0101, Val_loss_mean: 0.2176, Val_loss_rg_mean: 0.1226, Val_loss_cl_mean: 0.5025, Took 8.8506secs\n",
      "Iter: 0201, Val_loss_mean: 0.2172, Val_loss_rg_mean: 0.1221, Val_loss_cl_mean: 0.5024, Took 8.6588secs\n",
      "Iter: 0301, Val_loss_mean: 0.2176, Val_loss_rg_mean: 0.1227, Val_loss_cl_mean: 0.5021, Took 8.6079secs\n",
      "Iter: 0401, Val_loss_mean: 0.2174, Val_loss_rg_mean: 0.1227, Val_loss_cl_mean: 0.5018, Took 8.7256secs\n",
      "Iter: 0501, Val_loss_mean: 0.2175, Val_loss_rg_mean: 0.1228, Val_loss_cl_mean: 0.5017, Took 8.9594secs\n",
      "\n",
      "Epoch 0 ended\n",
      "Training:  1:37\n",
      "Validation:  0:51\n",
      "Loss decreased from 100000.0000 to 0.2176\n",
      "Checkpointing and saving predictions to:\n",
      "results/test-nosampling_20220214\n",
      "\n",
      "\n",
      "Starting epoch: 1\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.2270, Tr_loss_rg_mean: 0.1270, Tr_loss_cl_mean: 0.5270, Took 2.7775secs\n",
      "Iter: 0101, Tr_loss_mean: 0.2183, Tr_loss_rg_mean: 0.1233, Tr_loss_cl_mean: 0.5035, Took 15.6596secs\n",
      "Iter: 0201, Tr_loss_mean: 0.2163, Tr_loss_rg_mean: 0.1228, Tr_loss_cl_mean: 0.4965, Took 15.7239secs\n",
      "Iter: 0301, Tr_loss_mean: 0.2159, Tr_loss_rg_mean: 0.1234, Tr_loss_cl_mean: 0.4935, Took 15.8965secs\n",
      "Iter: 0401, Tr_loss_mean: 0.2161, Tr_loss_rg_mean: 0.1236, Tr_loss_cl_mean: 0.4937, Took 15.7026secs\n",
      "Iter: 0501, Tr_loss_mean: 0.2166, Tr_loss_rg_mean: 0.1239, Tr_loss_cl_mean: 0.4947, Took 15.5678secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.2158, Val_loss_rg_mean: 0.1191, Val_loss_cl_mean: 0.5061, Took 2.6182secs\n",
      "Iter: 0101, Val_loss_mean: 0.2170, Val_loss_rg_mean: 0.1209, Val_loss_cl_mean: 0.5054, Took 8.4783secs\n",
      "Iter: 0201, Val_loss_mean: 0.2166, Val_loss_rg_mean: 0.1204, Val_loss_cl_mean: 0.5052, Took 8.6936secs\n",
      "Iter: 0301, Val_loss_mean: 0.2172, Val_loss_rg_mean: 0.1212, Val_loss_cl_mean: 0.5051, Took 8.5314secs\n",
      "Iter: 0401, Val_loss_mean: 0.2173, Val_loss_rg_mean: 0.1213, Val_loss_cl_mean: 0.5054, Took 8.5581secs\n",
      "Iter: 0501, Val_loss_mean: 0.2173, Val_loss_rg_mean: 0.1213, Val_loss_cl_mean: 0.5053, Took 8.6128secs\n",
      "\n",
      "Epoch 1 ended\n",
      "Training:  1:28\n",
      "Validation:  0:49\n",
      "Loss decreased from 0.2176 to 0.2173\n",
      "Checkpointing and saving predictions to:\n",
      "results/test-nosampling_20220214\n",
      "Learning rate decreased to: 1.000e-04\n",
      "\n",
      "\n",
      "Starting epoch: 2\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.2229, Tr_loss_rg_mean: 0.1249, Tr_loss_cl_mean: 0.5170, Took 2.8016secs\n",
      "Iter: 0101, Tr_loss_mean: 0.2058, Tr_loss_rg_mean: 0.1152, Tr_loss_cl_mean: 0.4776, Took 15.7817secs\n",
      "Iter: 0201, Tr_loss_mean: 0.2044, Tr_loss_rg_mean: 0.1144, Tr_loss_cl_mean: 0.4746, Took 15.7478secs\n",
      "Iter: 0301, Tr_loss_mean: 0.2041, Tr_loss_rg_mean: 0.1142, Tr_loss_cl_mean: 0.4737, Took 15.5102secs\n",
      "Iter: 0401, Tr_loss_mean: 0.2039, Tr_loss_rg_mean: 0.1142, Tr_loss_cl_mean: 0.4729, Took 15.7424secs\n",
      "Iter: 0501, Tr_loss_mean: 0.2037, Tr_loss_rg_mean: 0.1141, Tr_loss_cl_mean: 0.4724, Took 15.3750secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1937, Val_loss_rg_mean: 0.1044, Val_loss_cl_mean: 0.4618, Took 2.8444secs\n",
      "Iter: 0101, Val_loss_mean: 0.2019, Val_loss_rg_mean: 0.1127, Val_loss_cl_mean: 0.4694, Took 8.6700secs\n",
      "Iter: 0201, Val_loss_mean: 0.2012, Val_loss_rg_mean: 0.1119, Val_loss_cl_mean: 0.4689, Took 8.7528secs\n",
      "Iter: 0301, Val_loss_mean: 0.2017, Val_loss_rg_mean: 0.1128, Val_loss_cl_mean: 0.4685, Took 8.7794secs\n",
      "Iter: 0401, Val_loss_mean: 0.2016, Val_loss_rg_mean: 0.1128, Val_loss_cl_mean: 0.4679, Took 8.6437secs\n",
      "Iter: 0501, Val_loss_mean: 0.2015, Val_loss_rg_mean: 0.1128, Val_loss_cl_mean: 0.4676, Took 8.6913secs\n",
      "\n",
      "Epoch 2 ended\n",
      "Training:  1:28\n",
      "Validation:  0:50\n",
      "Loss decreased from 0.2173 to 0.2017\n",
      "Checkpointing and saving predictions to:\n",
      "results/test-nosampling_20220214\n",
      "\n",
      "\n",
      "Starting epoch: 3\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.2117, Tr_loss_rg_mean: 0.1251, Tr_loss_cl_mean: 0.4717, Took 2.9710secs\n",
      "Iter: 0101, Tr_loss_mean: 0.2018, Tr_loss_rg_mean: 0.1134, Tr_loss_cl_mean: 0.4669, Took 15.9448secs\n",
      "Iter: 0201, Tr_loss_mean: 0.2008, Tr_loss_rg_mean: 0.1126, Tr_loss_cl_mean: 0.4656, Took 15.8253secs\n",
      "Iter: 0301, Tr_loss_mean: 0.2006, Tr_loss_rg_mean: 0.1124, Tr_loss_cl_mean: 0.4651, Took 15.5373secs\n",
      "Iter: 0401, Tr_loss_mean: 0.2006, Tr_loss_rg_mean: 0.1125, Tr_loss_cl_mean: 0.4650, Took 15.5645secs\n",
      "Iter: 0501, Tr_loss_mean: 0.2005, Tr_loss_rg_mean: 0.1126, Tr_loss_cl_mean: 0.4642, Took 15.6724secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1900, Val_loss_rg_mean: 0.0985, Val_loss_cl_mean: 0.4643, Took 2.6598secs\n",
      "Iter: 0101, Val_loss_mean: 0.2011, Val_loss_rg_mean: 0.1115, Val_loss_cl_mean: 0.4700, Took 8.7883secs\n",
      "Iter: 0201, Val_loss_mean: 0.2008, Val_loss_rg_mean: 0.1112, Val_loss_cl_mean: 0.4695, Took 8.4537secs\n",
      "Iter: 0301, Val_loss_mean: 0.2013, Val_loss_rg_mean: 0.1121, Val_loss_cl_mean: 0.4691, Took 8.6879secs\n",
      "Iter: 0401, Val_loss_mean: 0.2012, Val_loss_rg_mean: 0.1121, Val_loss_cl_mean: 0.4685, Took 8.5788secs\n",
      "Iter: 0501, Val_loss_mean: 0.2011, Val_loss_rg_mean: 0.1121, Val_loss_cl_mean: 0.4684, Took 8.4062secs\n",
      "\n",
      "Epoch 3 ended\n",
      "Training:  1:28\n",
      "Validation:  0:49\n",
      "Loss decreased from 0.2017 to 0.2013\n",
      "Checkpointing and saving predictions to:\n",
      "results/test-nosampling_20220214\n",
      "Learning rate decreased to: 1.000e-05\n",
      "\n",
      "\n",
      "Starting epoch: 4\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.2090, Tr_loss_rg_mean: 0.1277, Tr_loss_cl_mean: 0.4531, Took 2.7571secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1984, Tr_loss_rg_mean: 0.1115, Tr_loss_cl_mean: 0.4592, Took 15.6405secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1975, Tr_loss_rg_mean: 0.1111, Tr_loss_cl_mean: 0.4569, Took 15.7179secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1973, Tr_loss_rg_mean: 0.1108, Tr_loss_cl_mean: 0.4569, Took 15.8007secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1975, Tr_loss_rg_mean: 0.1111, Tr_loss_cl_mean: 0.4566, Took 15.5730secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1976, Tr_loss_rg_mean: 0.1113, Tr_loss_cl_mean: 0.4565, Took 15.4387secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1857, Val_loss_rg_mean: 0.1015, Val_loss_cl_mean: 0.4383, Took 2.8188secs\n",
      "Iter: 0101, Val_loss_mean: 0.1973, Val_loss_rg_mean: 0.1107, Val_loss_cl_mean: 0.4569, Took 8.6763secs\n",
      "Iter: 0201, Val_loss_mean: 0.1969, Val_loss_rg_mean: 0.1103, Val_loss_cl_mean: 0.4568, Took 8.7286secs\n",
      "Iter: 0301, Val_loss_mean: 0.1973, Val_loss_rg_mean: 0.1109, Val_loss_cl_mean: 0.4566, Took 8.4657secs\n",
      "Iter: 0401, Val_loss_mean: 0.1972, Val_loss_rg_mean: 0.1109, Val_loss_cl_mean: 0.4560, Took 8.4638secs\n",
      "Iter: 0501, Val_loss_mean: 0.1972, Val_loss_rg_mean: 0.1110, Val_loss_cl_mean: 0.4558, Took 8.4579secs\n",
      "\n",
      "Epoch 4 ended\n",
      "Training:  1:28\n",
      "Validation:  0:49\n",
      "Loss decreased from 0.2013 to 0.1973\n",
      "Checkpointing and saving predictions to:\n",
      "results/test-nosampling_20220214\n",
      "\n",
      "\n",
      "Starting epoch: 5\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.2059, Tr_loss_rg_mean: 0.1263, Tr_loss_cl_mean: 0.4447, Took 3.0254secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1975, Tr_loss_rg_mean: 0.1112, Tr_loss_cl_mean: 0.4561, Took 15.6144secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1968, Tr_loss_rg_mean: 0.1107, Tr_loss_cl_mean: 0.4551, Took 15.5712secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1965, Tr_loss_rg_mean: 0.1103, Tr_loss_cl_mean: 0.4549, Took 15.4950secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1967, Tr_loss_rg_mean: 0.1106, Tr_loss_cl_mean: 0.4547, Took 15.6396secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1968, Tr_loss_rg_mean: 0.1108, Tr_loss_cl_mean: 0.4547, Took 15.4436secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1946, Val_loss_rg_mean: 0.1121, Val_loss_cl_mean: 0.4419, Took 2.6731secs\n",
      "Iter: 0101, Val_loss_mean: 0.1966, Val_loss_rg_mean: 0.1103, Val_loss_cl_mean: 0.4554, Took 8.6237secs\n",
      "Iter: 0201, Val_loss_mean: 0.1961, Val_loss_rg_mean: 0.1096, Val_loss_cl_mean: 0.4557, Took 8.2797secs\n",
      "Iter: 0301, Val_loss_mean: 0.1966, Val_loss_rg_mean: 0.1104, Val_loss_cl_mean: 0.4552, Took 8.4745secs\n",
      "Iter: 0401, Val_loss_mean: 0.1965, Val_loss_rg_mean: 0.1104, Val_loss_cl_mean: 0.4549, Took 8.7117secs\n",
      "Iter: 0501, Val_loss_mean: 0.1965, Val_loss_rg_mean: 0.1105, Val_loss_cl_mean: 0.4546, Took 8.6738secs\n",
      "\n",
      "Epoch 5 ended\n",
      "Training:  1:27\n",
      "Validation:  0:49\n",
      "Loss decreased from 0.1973 to 0.1966\n",
      "Checkpointing and saving predictions to:\n",
      "results/test-nosampling_20220214\n",
      "Learning rate decreased to: 1.000e-06\n",
      "\n",
      "\n",
      "Starting epoch: 6\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.2069, Tr_loss_rg_mean: 0.1229, Tr_loss_cl_mean: 0.4591, Took 2.7931secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1967, Tr_loss_rg_mean: 0.1107, Tr_loss_cl_mean: 0.4547, Took 15.6592secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1960, Tr_loss_rg_mean: 0.1102, Tr_loss_cl_mean: 0.4533, Took 15.7305secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1960, Tr_loss_rg_mean: 0.1102, Tr_loss_cl_mean: 0.4534, Took 15.6500secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1961, Tr_loss_rg_mean: 0.1103, Tr_loss_cl_mean: 0.4534, Took 15.4968secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1962, Tr_loss_rg_mean: 0.1104, Tr_loss_cl_mean: 0.4535, Took 15.3695secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1873, Val_loss_rg_mean: 0.1049, Val_loss_cl_mean: 0.4344, Took 2.6193secs\n",
      "Iter: 0101, Val_loss_mean: 0.1961, Val_loss_rg_mean: 0.1101, Val_loss_cl_mean: 0.4540, Took 8.5390secs\n",
      "Iter: 0201, Val_loss_mean: 0.1958, Val_loss_rg_mean: 0.1095, Val_loss_cl_mean: 0.4547, Took 8.5655secs\n",
      "Iter: 0301, Val_loss_mean: 0.1964, Val_loss_rg_mean: 0.1104, Val_loss_cl_mean: 0.4544, Took 8.7415secs\n",
      "Iter: 0401, Val_loss_mean: 0.1962, Val_loss_rg_mean: 0.1103, Val_loss_cl_mean: 0.4540, Took 8.5030secs\n",
      "Iter: 0501, Val_loss_mean: 0.1963, Val_loss_rg_mean: 0.1104, Val_loss_cl_mean: 0.4537, Took 8.4381secs\n",
      "\n",
      "Epoch 6 ended\n",
      "Training:  1:27\n",
      "Validation:  0:49\n",
      "Loss decreased from 0.1966 to 0.1964\n",
      "Checkpointing and saving predictions to:\n",
      "results/test-nosampling_20220214\n",
      "\n",
      "\n",
      "Starting epoch: 7\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1968, Tr_loss_rg_mean: 0.1075, Tr_loss_cl_mean: 0.4648, Took 3.1002secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1965, Tr_loss_rg_mean: 0.1106, Tr_loss_cl_mean: 0.4541, Took 15.5883secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1959, Tr_loss_rg_mean: 0.1102, Tr_loss_cl_mean: 0.4531, Took 15.8179secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1958, Tr_loss_rg_mean: 0.1099, Tr_loss_cl_mean: 0.4534, Took 15.7377secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1960, Tr_loss_rg_mean: 0.1102, Tr_loss_cl_mean: 0.4532, Took 15.6818secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1961, Tr_loss_rg_mean: 0.1104, Tr_loss_cl_mean: 0.4533, Took 15.4638secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1909, Val_loss_rg_mean: 0.1036, Val_loss_cl_mean: 0.4527, Took 2.8420secs\n",
      "Iter: 0101, Val_loss_mean: 0.1964, Val_loss_rg_mean: 0.1099, Val_loss_cl_mean: 0.4556, Took 8.7025secs\n",
      "Iter: 0201, Val_loss_mean: 0.1959, Val_loss_rg_mean: 0.1097, Val_loss_cl_mean: 0.4546, Took 8.5276secs\n",
      "Iter: 0301, Val_loss_mean: 0.1962, Val_loss_rg_mean: 0.1103, Val_loss_cl_mean: 0.4542, Took 8.3664secs\n",
      "Iter: 0401, Val_loss_mean: 0.1962, Val_loss_rg_mean: 0.1104, Val_loss_cl_mean: 0.4538, Took 8.6230secs\n",
      "Iter: 0501, Val_loss_mean: 0.1961, Val_loss_rg_mean: 0.1104, Val_loss_cl_mean: 0.4535, Took 8.8997secs\n",
      "\n",
      "Epoch 7 ended\n",
      "Training:  1:28\n",
      "Validation:  0:49\n",
      "Loss decreased from 0.1964 to 0.1963\n",
      "Checkpointing and saving predictions to:\n",
      "results/test-nosampling_20220214\n",
      "Learning rate decreased to: 1.000e-07\n",
      "\n",
      "\n",
      "Starting epoch: 8\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.2067, Tr_loss_rg_mean: 0.1227, Tr_loss_cl_mean: 0.4586, Took 2.7909secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1965, Tr_loss_rg_mean: 0.1107, Tr_loss_cl_mean: 0.4538, Took 15.6408secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1959, Tr_loss_rg_mean: 0.1102, Tr_loss_cl_mean: 0.4530, Took 15.6614secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1957, Tr_loss_rg_mean: 0.1100, Tr_loss_cl_mean: 0.4529, Took 15.6047secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1959, Tr_loss_rg_mean: 0.1102, Tr_loss_cl_mean: 0.4529, Took 15.7900secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1961, Tr_loss_rg_mean: 0.1104, Tr_loss_cl_mean: 0.4533, Took 15.4445secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1872, Val_loss_rg_mean: 0.1049, Val_loss_cl_mean: 0.4340, Took 2.6442secs\n",
      "Iter: 0101, Val_loss_mean: 0.1961, Val_loss_rg_mean: 0.1100, Val_loss_cl_mean: 0.4543, Took 8.5747secs\n",
      "Iter: 0201, Val_loss_mean: 0.1957, Val_loss_rg_mean: 0.1095, Val_loss_cl_mean: 0.4544, Took 8.4798secs\n",
      "Iter: 0301, Val_loss_mean: 0.1962, Val_loss_rg_mean: 0.1103, Val_loss_cl_mean: 0.4539, Took 8.6729secs\n",
      "Iter: 0401, Val_loss_mean: 0.1961, Val_loss_rg_mean: 0.1103, Val_loss_cl_mean: 0.4536, Took 8.8369secs\n",
      "Iter: 0501, Val_loss_mean: 0.1962, Val_loss_rg_mean: 0.1105, Val_loss_cl_mean: 0.4535, Took 8.4262secs\n",
      "\n",
      "Epoch 8 ended\n",
      "Training:  1:28\n",
      "Validation:  0:49\n",
      "Loss decreased from 0.1963 to 0.1963\n",
      "Checkpointing and saving predictions to:\n",
      "results/test-nosampling_20220214\n",
      "\n",
      "\n",
      "Starting epoch: 9\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.2066, Tr_loss_rg_mean: 0.1226, Tr_loss_cl_mean: 0.4586, Took 2.7655secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1966, Tr_loss_rg_mean: 0.1107, Tr_loss_cl_mean: 0.4543, Took 15.5838secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1959, Tr_loss_rg_mean: 0.1102, Tr_loss_cl_mean: 0.4528, Took 15.5384secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1958, Tr_loss_rg_mean: 0.1100, Tr_loss_cl_mean: 0.4530, Took 15.4445secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1959, Tr_loss_rg_mean: 0.1102, Tr_loss_cl_mean: 0.4530, Took 15.5531secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1961, Tr_loss_rg_mean: 0.1104, Tr_loss_cl_mean: 0.4532, Took 15.4729secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1873, Val_loss_rg_mean: 0.1017, Val_loss_cl_mean: 0.4438, Took 2.6143secs\n",
      "Iter: 0101, Val_loss_mean: 0.1961, Val_loss_rg_mean: 0.1099, Val_loss_cl_mean: 0.4548, Took 8.8012secs\n",
      "Iter: 0201, Val_loss_mean: 0.1958, Val_loss_rg_mean: 0.1095, Val_loss_cl_mean: 0.4547, Took 8.7330secs\n",
      "Iter: 0301, Val_loss_mean: 0.1963, Val_loss_rg_mean: 0.1103, Val_loss_cl_mean: 0.4542, Took 8.7190secs\n",
      "Iter: 0401, Val_loss_mean: 0.1962, Val_loss_rg_mean: 0.1103, Val_loss_cl_mean: 0.4537, Took 8.7908secs\n",
      "Iter: 0501, Val_loss_mean: 0.1961, Val_loss_rg_mean: 0.1103, Val_loss_cl_mean: 0.4535, Took 9.0028secs\n",
      "\n",
      "Epoch 9 ended\n",
      "Training:  1:27\n",
      "Validation:  0:50\n",
      "Loss decreased from 0.1963 to 0.1963\n",
      "Checkpointing and saving predictions to:\n",
      "results/test-nosampling_20220214\n",
      "Learning rate decreased to: 1.000e-08\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "curr_loss = 1e5\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    print(f'\\n\\nStarting epoch: {e}')\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Batchwise losses\n",
    "    training_loss = []\n",
    "    training_loss_regress = []\n",
    "    training_loss_class = []\n",
    "    val_loss = []\n",
    "    val_loss_regress = []\n",
    "    val_loss_class = []\n",
    "\n",
    "    # Train\n",
    "    print('Training...')\n",
    "    start = time.time()\n",
    "    for i, (graph_data_tr, targets_tr) in enumerate(get_batch(data_gen_train.generator())):\n",
    "        losses_tr_rg, losses_tr_cl, losses_tr = train_step(graph_data_tr, targets_tr)\n",
    "\n",
    "        training_loss.append(losses_tr.numpy())\n",
    "        training_loss_regress.append(losses_tr_rg.numpy())\n",
    "        training_loss_class.append(losses_tr_cl.numpy())\n",
    "\n",
    "        if not (i-1)%log_freq:\n",
    "            end = time.time()\n",
    "            print(f'Iter: {i:04d}, ', end='')\n",
    "            print(f'Tr_loss_mean: {np.mean(training_loss):.4f}, ', end='')\n",
    "            print(f'Tr_loss_rg_mean: {np.mean(training_loss_regress):.4f}, ', end='') \n",
    "            print(f'Tr_loss_cl_mean: {np.mean(training_loss_class):.4f}, ', end='') \n",
    "            print(f'Took {end-start:.4f}secs')\n",
    "            start = time.time()\n",
    "                  \n",
    "    training_loss_epoch.append(training_loss)\n",
    "    training_loss_regress_epoch.append(training_loss_regress)\n",
    "    training_loss_class_epoch.append(training_loss_class)\n",
    "    training_end = time.time()\n",
    "\n",
    "    # validate\n",
    "    print('\\nValidation...')\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    all_energies = []\n",
    "    start = time.time()\n",
    "    for i, (graph_data_val, targets_val) in enumerate(get_batch(data_gen_val.generator())):\n",
    "        losses_val_rg, losses_val_cl, losses_val, regress_vals, class_vals = val_step(graph_data_val, targets_val)\n",
    "\n",
    "        targets_val = targets_val.numpy()\n",
    "        regress_vals = regress_vals.numpy()\n",
    "        class_vals = class_vals.numpy()\n",
    "\n",
    "        targets_val[:,0] = 10**targets_val[:,0]\n",
    "        regress_vals = 10**regress_vals\n",
    "        class_vals =  tf.math.sigmoid(class_vals)\n",
    "\n",
    "        output_vals = np.hstack([regress_vals, class_vals])\n",
    "\n",
    "        val_loss.append(losses_val.numpy())\n",
    "        val_loss_regress.append(losses_val_rg.numpy())\n",
    "        val_loss_class.append(losses_val_cl.numpy())\n",
    "\n",
    "        all_targets.append(targets_val)\n",
    "        all_outputs.append(output_vals)\n",
    "\n",
    "        if not (i-1)%log_freq:\n",
    "            end = time.time()\n",
    "            print(f'Iter: {i:04d}, ', end='')\n",
    "            print(f'Val_loss_mean: {np.mean(val_loss):.4f}, ', end='')\n",
    "            print(f'Val_loss_rg_mean: {np.mean(val_loss_regress):.4f}, ', end='') \n",
    "            print(f'Val_loss_cl_mean: {np.mean(val_loss_class):.4f}, ', end='') \n",
    "            print(f'Took {end-start:.4f}secs')\n",
    "            start = time.time()\n",
    "\n",
    "    epoch_end = time.time()\n",
    "\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "\n",
    "    val_loss_epoch.append(val_loss)\n",
    "    val_loss_regress_epoch.append(val_loss_regress)\n",
    "    val_loss_class_epoch.append(val_loss_class)\n",
    "\n",
    "    \n",
    "    # Book keeping\n",
    "    val_mins = int((epoch_end - training_end)/60)\n",
    "    val_secs = int((epoch_end - training_end)%60)\n",
    "    training_mins = int((training_end - epoch_start)/60)\n",
    "    training_secs = int((training_end - epoch_start)%60)\n",
    "    print(f'\\nEpoch {e} ended')\n",
    "    print(f'Training: {training_mins:2d}:{training_secs:02d}')\n",
    "    print(f'Validation: {val_mins:2d}:{val_secs:02d}')\n",
    "    \n",
    "    \n",
    "    # Save losses\n",
    "    np.savez(save_dir+'/losses', \n",
    "            training=training_loss_epoch, validation=val_loss_epoch,\n",
    "            training_regress=training_loss_regress_epoch, validation_regress=val_loss_regress_epoch,\n",
    "            training_class=training_loss_class_epoch, validation_class=val_loss_class_epoch,\n",
    "            )\n",
    "\n",
    "    \n",
    "    # Checkpoint if validation loss improved\n",
    "    if np.mean(val_loss)<curr_loss:\n",
    "        print(f'Loss decreased from {curr_loss:.4f} to {np.mean(val_loss):.4f}')\n",
    "        print(f'Checkpointing and saving predictions to:\\n{save_dir}')\n",
    "        curr_loss = np.mean(val_loss)\n",
    "        np.savez(save_dir+'/predictions', \n",
    "                targets=all_targets, \n",
    "                outputs=all_outputs,\n",
    "                energies=all_energies)\n",
    "        checkpoint.save(checkpoint_prefix)\n",
    "    else: \n",
    "        print(f'Loss didnt decrease from {curr_loss:.4f}')\n",
    "    \n",
    "    \n",
    "    # Decrease learning rate every few epochs\n",
    "    if not (e+1)%2:   #%20:\n",
    "        optimizer.learning_rate = optimizer.learning_rate/10\n",
    "        print(f'Learning rate decreased to: {optimizer.learning_rate.value():.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "nbdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
