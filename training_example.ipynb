{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import uproot as ur\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets.graphs import GraphsTuple\n",
    "import sonnet as snt\n",
    "import argparse\n",
    "import yaml\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gn4pions.modules.data import GraphDataGenerator\n",
    "from gn4pions.modules.models import MultiOutWeightedRegressModel\n",
    "from gn4pions.modules.utils import convert_to_tuple\n",
    "\n",
    "sns.set_context('poster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model config\n",
    "config_file = 'gn4pions/configs/test.yaml' # for a quick run of the notebook\n",
    "# config_file = 'gn4pions/configs/baseline.yaml' # for actual training\n",
    "config = yaml.load(open(config_file), Loader=yaml.FullLoader)\n",
    "\n",
    "# Data config\n",
    "data_config = config['data']\n",
    "\n",
    "data_dir = data_config['data_dir']\n",
    "num_train_files = data_config['num_train_files']\n",
    "num_val_files = data_config['num_val_files']\n",
    "batch_size = data_config['batch_size']\n",
    "shuffle = data_config['shuffle']\n",
    "num_procs = data_config['num_procs']\n",
    "preprocess = data_config['preprocess']\n",
    "output_dir = data_config['output_dir']\n",
    "already_preprocessed = data_config['already_preprocessed']  # Set to false when running training for first time\n",
    "\n",
    "# Model Config\n",
    "model_config = config['model']\n",
    "\n",
    "concat_input = model_config['concat_input']\n",
    "\n",
    "\n",
    "# Traning Config\n",
    "train_config = config['training']\n",
    "\n",
    "epochs = train_config['epochs']\n",
    "learning_rate = train_config['learning_rate']\n",
    "alpha = train_config['alpha']\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(train_config['gpu'])\n",
    "log_freq = train_config['log_freq']\n",
    "save_dir = train_config['save_dir'] + config_file.replace('.yaml','').split('/')[-1] + '_' + time.strftime(\"%Y%m%d\")\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "yaml.dump(config, open(save_dir + '/config.yaml', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and create data generators\n",
    "\n",
    "pi0_files = np.sort(glob.glob(data_dir+'*pi0*/*root'))\n",
    "pion_files = np.sort(glob.glob(data_dir+'*pion*/*root'))\n",
    "\n",
    "train_start = 0\n",
    "train_end = train_start + num_train_files\n",
    "val_end = train_end + num_val_files\n",
    "\n",
    "pi0_train_files = pi0_files[train_start:train_end]\n",
    "pi0_val_files = pi0_files[train_end:val_end]\n",
    "pion_train_files = pion_files[train_start:train_end]\n",
    "pion_val_files = pion_files[train_end:val_end]\n",
    "\n",
    "train_output_dir = None\n",
    "val_output_dir = None\n",
    "\n",
    "# Get Data\n",
    "if preprocess:\n",
    "    train_output_dir = output_dir + '/train/'\n",
    "    val_output_dir = output_dir + '/val/'\n",
    "\n",
    "    if already_preprocessed:\n",
    "        train_files = np.sort(glob.glob(train_output_dir+'*.p'))[:num_train_files]\n",
    "        val_files = np.sort(glob.glob(val_output_dir+'*.p'))[:num_val_files]\n",
    "\n",
    "        pi0_train_files = train_files\n",
    "        pi0_val_files = val_files\n",
    "        pion_train_files = None\n",
    "        pion_val_files = None\n",
    "\n",
    "        train_output_dir = None\n",
    "        val_output_dir = None\n",
    "\n",
    "# Traning Data Generator\n",
    "# Will preprocess data if it doesnt find pickled files\n",
    "data_gen_train = GraphDataGenerator(pi0_file_list=pi0_train_files,\n",
    "                                    pion_file_list=pion_train_files,\n",
    "                                    cellGeo_file=data_dir+'cell_geo.root',\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=shuffle,\n",
    "                                    num_procs=num_procs,\n",
    "                                    preprocess=preprocess,\n",
    "                                    output_dir=train_output_dir)\n",
    "\n",
    "# Validation Data generator\n",
    "# Will preprocess data if it doesnt find pickled files\n",
    "data_gen_val = GraphDataGenerator(pi0_file_list=pi0_val_files,\n",
    "                                  pion_file_list=pion_val_files,\n",
    "                                  cellGeo_file=data_dir+'cell_geo.root',\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=shuffle,\n",
    "                                  num_procs=num_procs,\n",
    "                                  preprocess=preprocess,\n",
    "                                  output_dir=val_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get batch of data\n",
    "def get_batch(data_iter):\n",
    "    for graphs, targets in data_iter:\n",
    "        graphs = convert_to_tuple(graphs)\n",
    "        targets = tf.convert_to_tensor(targets)\n",
    "        yield graphs, targets\n",
    "        \n",
    "# Define loss function        \n",
    "mae_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def loss_fn(targets, regress_preds, class_preds):\n",
    "    regress_loss = mae_loss(targets[:,:1], regress_preds)\n",
    "    class_loss = bce_loss(targets[:,1:], class_preds)\n",
    "    combined_loss = alpha*regress_loss + (1 - alpha)*class_loss \n",
    "    return regress_loss, class_loss, combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 11:01:49.617736: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-31 11:01:50.042622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43662 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:41:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Get a sample graph for tf.function decorator\n",
    "samp_graph, samp_target = next(get_batch(data_gen_train.generator()))\n",
    "data_gen_train.kill_procs()\n",
    "graph_spec = utils_tf.specs_from_graphs_tuple(samp_graph, True, True, True)\n",
    "\n",
    "# Training set\n",
    "@tf.function(input_signature=[graph_spec, tf.TensorSpec(shape=[None,2], dtype=tf.float32)])\n",
    "def train_step(graphs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        regress_output, class_output = model(graphs)\n",
    "        regress_preds = regress_output.globals\n",
    "        class_preds = class_output.globals\n",
    "        regress_loss, class_loss, loss = loss_fn(targets, regress_preds, class_preds)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return regress_loss, class_loss, loss\n",
    "\n",
    "# Validation Step\n",
    "@tf.function(input_signature=[graph_spec, tf.TensorSpec(shape=[None,2], dtype=tf.float32)])\n",
    "def val_step(graphs, targets):\n",
    "    regress_output, class_output = model(graphs)\n",
    "    regress_preds = regress_output.globals\n",
    "    class_preds = class_output.globals\n",
    "    regress_loss, class_loss, loss = loss_fn(targets, regress_preds, class_preds)\n",
    "    return regress_loss, class_loss, loss, regress_preds, class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "model = MultiOutWeightedRegressModel(global_output_size=1, num_outputs=2, model_config=model_config)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Average epoch losses\n",
    "training_loss_epoch = []\n",
    "training_loss_regress_epoch = []\n",
    "training_loss_class_epoch = []\n",
    "val_loss_epoch = []\n",
    "val_loss_regress_epoch = []\n",
    "val_loss_class_epoch = []\n",
    "\n",
    "# Model checkpointing, load latest model if available\n",
    "checkpoint = tf.train.Checkpoint(module=model)\n",
    "checkpoint_prefix = os.path.join(save_dir, 'latest_model')\n",
    "latest = tf.train.latest_checkpoint(save_dir)\n",
    "if latest is not None:\n",
    "    checkpoint.restore(latest)\n",
    "else:\n",
    "    checkpoint.save(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (graph_data_tr, targets_tr) in enumerate(get_batch(data_gen_train.generator())):\n",
    "#     print(\"Targets: \",targets_tr[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting epoch: 0\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 10:04:28.255929: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 71), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 71), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2022-01-31 10:04:29.770201: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-01-31 10:04:33.579173: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0001, Tr_loss_mean: 1.1795, Tr_loss_rg_mean: 1.2666, Tr_loss_cl_mean: 0.9181, Took 12.2805secs\n",
      "Iter: 0101, Tr_loss_mean: 0.4269, Tr_loss_rg_mean: 0.2719, Tr_loss_cl_mean: 0.8921, Took 16.8263secs\n",
      "Iter: 0201, Tr_loss_mean: 0.3463, Tr_loss_rg_mean: 0.2075, Tr_loss_cl_mean: 0.7625, Took 17.1217secs\n",
      "Iter: 0301, Tr_loss_mean: 0.3099, Tr_loss_rg_mean: 0.1820, Tr_loss_cl_mean: 0.6936, Took 17.0343secs\n",
      "Iter: 0401, Tr_loss_mean: 0.2802, Tr_loss_rg_mean: 0.1624, Tr_loss_cl_mean: 0.6335, Took 17.0570secs\n",
      "Iter: 0501, Tr_loss_mean: 0.2596, Tr_loss_rg_mean: 0.1501, Tr_loss_cl_mean: 0.5882, Took 17.1361secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1960, Val_loss_rg_mean: 0.1269, Val_loss_cl_mean: 0.4035, Took 3.7917secs\n",
      "Iter: 0101, Val_loss_mean: 0.1954, Val_loss_rg_mean: 0.1301, Val_loss_cl_mean: 0.3912, Took 9.6076secs\n",
      "Iter: 0201, Val_loss_mean: 0.1954, Val_loss_rg_mean: 0.1303, Val_loss_cl_mean: 0.3909, Took 9.7680secs\n",
      "Iter: 0301, Val_loss_mean: 0.1954, Val_loss_rg_mean: 0.1303, Val_loss_cl_mean: 0.3908, Took 9.7518secs\n",
      "Iter: 0401, Val_loss_mean: 0.1954, Val_loss_rg_mean: 0.1305, Val_loss_cl_mean: 0.3902, Took 9.7175secs\n",
      "Iter: 0501, Val_loss_mean: 0.1952, Val_loss_rg_mean: 0.1305, Val_loss_cl_mean: 0.3895, Took 9.3313secs\n",
      "\n",
      "Epoch 0 ended\n",
      "Training:  1:44\n",
      "Validation:  0:55\n",
      "Loss decreased from 100000.0000 to 0.1953\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_20220131\n",
      "\n",
      "\n",
      "Starting epoch: 1\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.2019, Tr_loss_rg_mean: 0.1283, Tr_loss_cl_mean: 0.4226, Took 3.0823secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1695, Tr_loss_rg_mean: 0.1024, Tr_loss_cl_mean: 0.3710, Took 16.9607secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1642, Tr_loss_rg_mean: 0.0997, Tr_loss_cl_mean: 0.3577, Took 16.8371secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1621, Tr_loss_rg_mean: 0.0988, Tr_loss_cl_mean: 0.3522, Took 17.2184secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1591, Tr_loss_rg_mean: 0.0967, Tr_loss_cl_mean: 0.3465, Took 16.7544secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1569, Tr_loss_rg_mean: 0.0953, Tr_loss_cl_mean: 0.3419, Took 16.6535secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.2115, Val_loss_rg_mean: 0.1354, Val_loss_cl_mean: 0.4399, Took 2.8780secs\n",
      "Iter: 0101, Val_loss_mean: 0.2056, Val_loss_rg_mean: 0.1383, Val_loss_cl_mean: 0.4074, Took 9.8102secs\n",
      "Iter: 0201, Val_loss_mean: 0.2054, Val_loss_rg_mean: 0.1386, Val_loss_cl_mean: 0.4057, Took 10.0231secs\n",
      "Iter: 0301, Val_loss_mean: 0.2054, Val_loss_rg_mean: 0.1386, Val_loss_cl_mean: 0.4060, Took 9.9898secs\n",
      "Iter: 0401, Val_loss_mean: 0.2056, Val_loss_rg_mean: 0.1388, Val_loss_cl_mean: 0.4059, Took 9.3119secs\n",
      "Iter: 0501, Val_loss_mean: 0.2054, Val_loss_rg_mean: 0.1388, Val_loss_cl_mean: 0.4053, Took 9.6796secs\n",
      "\n",
      "Epoch 1 ended\n",
      "Training:  1:35\n",
      "Validation:  0:55\n",
      "Loss didnt decrease from 0.1953\n",
      "Learning rate decreased to: 1.000e-04\n",
      "\n",
      "\n",
      "Starting epoch: 2\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.2055, Tr_loss_rg_mean: 0.1369, Tr_loss_cl_mean: 0.4113, Took 2.8112secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1422, Tr_loss_rg_mean: 0.0825, Tr_loss_cl_mean: 0.3213, Took 16.9577secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1376, Tr_loss_rg_mean: 0.0798, Tr_loss_cl_mean: 0.3112, Took 16.7947secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1361, Tr_loss_rg_mean: 0.0788, Tr_loss_cl_mean: 0.3079, Took 17.0308secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1349, Tr_loss_rg_mean: 0.0780, Tr_loss_cl_mean: 0.3054, Took 16.8541secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1343, Tr_loss_rg_mean: 0.0777, Tr_loss_cl_mean: 0.3041, Took 16.5798secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1326, Val_loss_rg_mean: 0.0731, Val_loss_cl_mean: 0.3110, Took 2.9029secs\n",
      "Iter: 0101, Val_loss_mean: 0.1324, Val_loss_rg_mean: 0.0760, Val_loss_cl_mean: 0.3016, Took 9.7183secs\n",
      "Iter: 0201, Val_loss_mean: 0.1325, Val_loss_rg_mean: 0.0762, Val_loss_cl_mean: 0.3011, Took 10.3078secs\n",
      "Iter: 0301, Val_loss_mean: 0.1322, Val_loss_rg_mean: 0.0761, Val_loss_cl_mean: 0.3005, Took 10.0726secs\n",
      "Iter: 0401, Val_loss_mean: 0.1322, Val_loss_rg_mean: 0.0763, Val_loss_cl_mean: 0.2999, Took 10.0178secs\n",
      "Iter: 0501, Val_loss_mean: 0.1320, Val_loss_rg_mean: 0.0763, Val_loss_cl_mean: 0.2993, Took 9.6791secs\n",
      "\n",
      "Epoch 2 ended\n",
      "Training:  1:34\n",
      "Validation:  0:56\n",
      "Loss decreased from 0.1953 to 0.1322\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_20220131\n",
      "\n",
      "\n",
      "Starting epoch: 3\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1336, Tr_loss_rg_mean: 0.0737, Tr_loss_cl_mean: 0.3135, Took 2.8503secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1319, Tr_loss_rg_mean: 0.0754, Tr_loss_cl_mean: 0.3011, Took 17.1767secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1314, Tr_loss_rg_mean: 0.0757, Tr_loss_cl_mean: 0.2987, Took 16.9399secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1311, Tr_loss_rg_mean: 0.0756, Tr_loss_cl_mean: 0.2976, Took 16.6608secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1308, Tr_loss_rg_mean: 0.0754, Tr_loss_cl_mean: 0.2970, Took 16.7171secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1306, Tr_loss_rg_mean: 0.0754, Tr_loss_cl_mean: 0.2964, Took 16.2931secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1314, Val_loss_rg_mean: 0.0724, Val_loss_cl_mean: 0.3085, Took 2.6837secs\n",
      "Iter: 0101, Val_loss_mean: 0.1311, Val_loss_rg_mean: 0.0753, Val_loss_cl_mean: 0.2988, Took 9.2940secs\n",
      "Iter: 0201, Val_loss_mean: 0.1312, Val_loss_rg_mean: 0.0756, Val_loss_cl_mean: 0.2981, Took 9.7662secs\n",
      "Iter: 0301, Val_loss_mean: 0.1310, Val_loss_rg_mean: 0.0754, Val_loss_cl_mean: 0.2978, Took 9.4190secs\n",
      "Iter: 0401, Val_loss_mean: 0.1310, Val_loss_rg_mean: 0.0756, Val_loss_cl_mean: 0.2971, Took 9.5199secs\n",
      "Iter: 0501, Val_loss_mean: 0.1308, Val_loss_rg_mean: 0.0756, Val_loss_cl_mean: 0.2964, Took 9.7008secs\n",
      "\n",
      "Epoch 3 ended\n",
      "Training:  1:34\n",
      "Validation:  0:54\n",
      "Loss decreased from 0.1322 to 0.1309\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_20220131\n",
      "Learning rate decreased to: 1.000e-05\n",
      "\n",
      "\n",
      "Starting epoch: 4\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1300, Tr_loss_rg_mean: 0.0722, Tr_loss_cl_mean: 0.3033, Took 3.1022secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1297, Tr_loss_rg_mean: 0.0741, Tr_loss_cl_mean: 0.2962, Took 16.6274secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1291, Tr_loss_rg_mean: 0.0744, Tr_loss_cl_mean: 0.2933, Took 15.6045secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1290, Tr_loss_rg_mean: 0.0744, Tr_loss_cl_mean: 0.2928, Took 15.9785secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1287, Tr_loss_rg_mean: 0.0743, Tr_loss_cl_mean: 0.2920, Took 16.5977secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1286, Tr_loss_rg_mean: 0.0743, Tr_loss_cl_mean: 0.2917, Took 16.6533secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1242, Val_loss_rg_mean: 0.0701, Val_loss_cl_mean: 0.2868, Took 2.9091secs\n",
      "Iter: 0101, Val_loss_mean: 0.1292, Val_loss_rg_mean: 0.0742, Val_loss_cl_mean: 0.2943, Took 9.6379secs\n",
      "Iter: 0201, Val_loss_mean: 0.1295, Val_loss_rg_mean: 0.0744, Val_loss_cl_mean: 0.2947, Took 9.8521secs\n",
      "Iter: 0301, Val_loss_mean: 0.1292, Val_loss_rg_mean: 0.0742, Val_loss_cl_mean: 0.2942, Took 10.3268secs\n",
      "Iter: 0401, Val_loss_mean: 0.1292, Val_loss_rg_mean: 0.0744, Val_loss_cl_mean: 0.2935, Took 9.6809secs\n",
      "Iter: 0501, Val_loss_mean: 0.1290, Val_loss_rg_mean: 0.0744, Val_loss_cl_mean: 0.2930, Took 9.5120secs\n",
      "\n",
      "Epoch 4 ended\n",
      "Training:  1:32\n",
      "Validation:  0:56\n",
      "Loss decreased from 0.1309 to 0.1291\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_20220131\n",
      "\n",
      "\n",
      "Starting epoch: 5\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1300, Tr_loss_rg_mean: 0.0754, Tr_loss_cl_mean: 0.2936, Took 2.8295secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1290, Tr_loss_rg_mean: 0.0739, Tr_loss_cl_mean: 0.2944, Took 16.6486secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1287, Tr_loss_rg_mean: 0.0741, Tr_loss_cl_mean: 0.2923, Took 16.8862secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1286, Tr_loss_rg_mean: 0.0742, Tr_loss_cl_mean: 0.2918, Took 17.0420secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1283, Tr_loss_rg_mean: 0.0740, Tr_loss_cl_mean: 0.2909, Took 16.7196secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1282, Tr_loss_rg_mean: 0.0741, Tr_loss_cl_mean: 0.2908, Took 16.4757secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1287, Val_loss_rg_mean: 0.0709, Val_loss_cl_mean: 0.3019, Took 2.6535secs\n",
      "Iter: 0101, Val_loss_mean: 0.1290, Val_loss_rg_mean: 0.0739, Val_loss_cl_mean: 0.2942, Took 9.5368secs\n",
      "Iter: 0201, Val_loss_mean: 0.1291, Val_loss_rg_mean: 0.0742, Val_loss_cl_mean: 0.2936, Took 9.6617secs\n",
      "Iter: 0301, Val_loss_mean: 0.1289, Val_loss_rg_mean: 0.0741, Val_loss_cl_mean: 0.2932, Took 9.7827secs\n",
      "Iter: 0401, Val_loss_mean: 0.1289, Val_loss_rg_mean: 0.0744, Val_loss_cl_mean: 0.2925, Took 9.9421secs\n",
      "Iter: 0501, Val_loss_mean: 0.1288, Val_loss_rg_mean: 0.0744, Val_loss_cl_mean: 0.2920, Took 9.9123secs\n",
      "\n",
      "Epoch 5 ended\n",
      "Training:  1:34\n",
      "Validation:  0:55\n",
      "Loss decreased from 0.1291 to 0.1288\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_20220131\n",
      "Learning rate decreased to: 1.000e-06\n",
      "\n",
      "\n",
      "Starting epoch: 6\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1278, Tr_loss_rg_mean: 0.0711, Tr_loss_cl_mean: 0.2979, Took 2.7987secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1286, Tr_loss_rg_mean: 0.0736, Tr_loss_cl_mean: 0.2937, Took 16.7961secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1283, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2916, Took 16.6974secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1282, Tr_loss_rg_mean: 0.0739, Tr_loss_cl_mean: 0.2910, Took 17.0824secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1280, Tr_loss_rg_mean: 0.0739, Tr_loss_cl_mean: 0.2903, Took 17.3716secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1280, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2903, Took 16.7792secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1285, Val_loss_rg_mean: 0.0707, Val_loss_cl_mean: 0.3018, Took 2.6677secs\n",
      "Iter: 0101, Val_loss_mean: 0.1288, Val_loss_rg_mean: 0.0739, Val_loss_cl_mean: 0.2937, Took 9.7040secs\n",
      "Iter: 0201, Val_loss_mean: 0.1289, Val_loss_rg_mean: 0.0741, Val_loss_cl_mean: 0.2935, Took 9.5058secs\n",
      "Iter: 0301, Val_loss_mean: 0.1287, Val_loss_rg_mean: 0.0740, Val_loss_cl_mean: 0.2929, Took 10.1541secs\n",
      "Iter: 0401, Val_loss_mean: 0.1286, Val_loss_rg_mean: 0.0741, Val_loss_cl_mean: 0.2921, Took 9.6702secs\n",
      "Iter: 0501, Val_loss_mean: 0.1286, Val_loss_rg_mean: 0.0742, Val_loss_cl_mean: 0.2916, Took 9.1934secs\n",
      "\n",
      "Epoch 6 ended\n",
      "Training:  1:34\n",
      "Validation:  0:54\n",
      "Loss decreased from 0.1288 to 0.1287\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_20220131\n",
      "\n",
      "\n",
      "Starting epoch: 7\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1295, Tr_loss_rg_mean: 0.0753, Tr_loss_cl_mean: 0.2921, Took 3.0446secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1286, Tr_loss_rg_mean: 0.0736, Tr_loss_cl_mean: 0.2936, Took 16.8898secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1282, Tr_loss_rg_mean: 0.0739, Tr_loss_cl_mean: 0.2913, Took 16.9665secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1282, Tr_loss_rg_mean: 0.0739, Tr_loss_cl_mean: 0.2908, Took 17.1630secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1278, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2901, Took 17.3941secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1278, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2898, Took 16.9568secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1285, Val_loss_rg_mean: 0.0706, Val_loss_cl_mean: 0.3019, Took 2.8844secs\n",
      "Iter: 0101, Val_loss_mean: 0.1287, Val_loss_rg_mean: 0.0737, Val_loss_cl_mean: 0.2936, Took 9.4414secs\n",
      "Iter: 0201, Val_loss_mean: 0.1289, Val_loss_rg_mean: 0.0740, Val_loss_cl_mean: 0.2933, Took 9.8524secs\n",
      "Iter: 0301, Val_loss_mean: 0.1287, Val_loss_rg_mean: 0.0740, Val_loss_cl_mean: 0.2929, Took 10.0063secs\n",
      "Iter: 0401, Val_loss_mean: 0.1287, Val_loss_rg_mean: 0.0742, Val_loss_cl_mean: 0.2922, Took 9.5322secs\n",
      "Iter: 0501, Val_loss_mean: 0.1286, Val_loss_rg_mean: 0.0742, Val_loss_cl_mean: 0.2917, Took 9.4529secs\n",
      "\n",
      "Epoch 7 ended\n",
      "Training:  1:35\n",
      "Validation:  0:55\n",
      "Loss decreased from 0.1287 to 0.1286\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_20220131\n",
      "Learning rate decreased to: 1.000e-07\n",
      "\n",
      "\n",
      "Starting epoch: 8\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1295, Tr_loss_rg_mean: 0.0709, Tr_loss_cl_mean: 0.3053, Took 2.8783secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1284, Tr_loss_rg_mean: 0.0733, Tr_loss_cl_mean: 0.2938, Took 16.6013secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1282, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2914, Took 16.5300secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1281, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2910, Took 16.7541secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1278, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2900, Took 16.4347secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1278, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2899, Took 16.6002secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1288, Val_loss_rg_mean: 0.0707, Val_loss_cl_mean: 0.3033, Took 2.7803secs\n",
      "Iter: 0101, Val_loss_mean: 0.1285, Val_loss_rg_mean: 0.0739, Val_loss_cl_mean: 0.2923, Took 9.4044secs\n",
      "Iter: 0201, Val_loss_mean: 0.1289, Val_loss_rg_mean: 0.0742, Val_loss_cl_mean: 0.2932, Took 9.0385secs\n",
      "Iter: 0301, Val_loss_mean: 0.1287, Val_loss_rg_mean: 0.0739, Val_loss_cl_mean: 0.2931, Took 9.0341secs\n",
      "Iter: 0401, Val_loss_mean: 0.1287, Val_loss_rg_mean: 0.0741, Val_loss_cl_mean: 0.2923, Took 9.2940secs\n",
      "Iter: 0501, Val_loss_mean: 0.1285, Val_loss_rg_mean: 0.0742, Val_loss_cl_mean: 0.2917, Took 8.8195secs\n",
      "\n",
      "Epoch 8 ended\n",
      "Training:  1:33\n",
      "Validation:  0:52\n",
      "Loss decreased from 0.1286 to 0.1286\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_20220131\n",
      "\n",
      "\n",
      "Starting epoch: 9\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1294, Tr_loss_rg_mean: 0.0752, Tr_loss_cl_mean: 0.2919, Took 2.8728secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1286, Tr_loss_rg_mean: 0.0736, Tr_loss_cl_mean: 0.2935, Took 16.6786secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1281, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2911, Took 16.6932secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1281, Tr_loss_rg_mean: 0.0739, Tr_loss_cl_mean: 0.2906, Took 16.9164secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1278, Tr_loss_rg_mean: 0.0737, Tr_loss_cl_mean: 0.2900, Took 16.5447secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1278, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2898, Took 17.0954secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1284, Val_loss_rg_mean: 0.0706, Val_loss_cl_mean: 0.3016, Took 2.7295secs\n",
      "Iter: 0101, Val_loss_mean: 0.1288, Val_loss_rg_mean: 0.0739, Val_loss_cl_mean: 0.2935, Took 9.1811secs\n",
      "Iter: 0201, Val_loss_mean: 0.1289, Val_loss_rg_mean: 0.0741, Val_loss_cl_mean: 0.2932, Took 9.1637secs\n",
      "Iter: 0301, Val_loss_mean: 0.1286, Val_loss_rg_mean: 0.0739, Val_loss_cl_mean: 0.2928, Took 9.3814secs\n",
      "Iter: 0401, Val_loss_mean: 0.1286, Val_loss_rg_mean: 0.0742, Val_loss_cl_mean: 0.2921, Took 9.3282secs\n",
      "Iter: 0501, Val_loss_mean: 0.1285, Val_loss_rg_mean: 0.0742, Val_loss_cl_mean: 0.2916, Took 8.9851secs\n",
      "\n",
      "Epoch 9 ended\n",
      "Training:  1:34\n",
      "Validation:  0:52\n",
      "Loss decreased from 0.1286 to 0.1286\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_20220131\n",
      "Learning rate decreased to: 1.000e-08\n",
      "\n",
      "\n",
      "Starting epoch: 10\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1290, Tr_loss_rg_mean: 0.0759, Tr_loss_cl_mean: 0.2883, Took 3.0694secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1286, Tr_loss_rg_mean: 0.0736, Tr_loss_cl_mean: 0.2936, Took 17.1048secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1281, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2909, Took 16.7260secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1280, Tr_loss_rg_mean: 0.0739, Tr_loss_cl_mean: 0.2905, Took 17.0076secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1279, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2900, Took 16.6907secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1278, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2899, Took 16.6735secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1288, Val_loss_rg_mean: 0.0707, Val_loss_cl_mean: 0.3033, Took 2.8437secs\n",
      "Iter: 0101, Val_loss_mean: 0.1287, Val_loss_rg_mean: 0.0738, Val_loss_cl_mean: 0.2932, Took 9.0549secs\n",
      "Iter: 0201, Val_loss_mean: 0.1289, Val_loss_rg_mean: 0.0741, Val_loss_cl_mean: 0.2932, Took 9.3125secs\n",
      "Iter: 0301, Val_loss_mean: 0.1287, Val_loss_rg_mean: 0.0739, Val_loss_cl_mean: 0.2930, Took 9.2453secs\n",
      "Iter: 0401, Val_loss_mean: 0.1286, Val_loss_rg_mean: 0.0741, Val_loss_cl_mean: 0.2921, Took 9.1056secs\n",
      "Iter: 0501, Val_loss_mean: 0.1285, Val_loss_rg_mean: 0.0741, Val_loss_cl_mean: 0.2916, Took 9.1331secs\n",
      "\n",
      "Epoch 10 ended\n",
      "Training:  1:34\n",
      "Validation:  0:52\n",
      "Loss decreased from 0.1286 to 0.1286\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_20220131\n",
      "\n",
      "\n",
      "Starting epoch: 11\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1294, Tr_loss_rg_mean: 0.0752, Tr_loss_cl_mean: 0.2919, Took 2.8394secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1286, Tr_loss_rg_mean: 0.0736, Tr_loss_cl_mean: 0.2935, Took 16.8106secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1281, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2911, Took 16.7065secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1281, Tr_loss_rg_mean: 0.0739, Tr_loss_cl_mean: 0.2906, Took 16.9803secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1278, Tr_loss_rg_mean: 0.0737, Tr_loss_cl_mean: 0.2900, Took 16.7200secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1278, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2897, Took 16.9484secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1234, Val_loss_rg_mean: 0.0696, Val_loss_cl_mean: 0.2848, Took 2.7103secs\n",
      "Iter: 0101, Val_loss_mean: 0.1285, Val_loss_rg_mean: 0.0738, Val_loss_cl_mean: 0.2924, Took 9.0660secs\n",
      "Iter: 0201, Val_loss_mean: 0.1289, Val_loss_rg_mean: 0.0742, Val_loss_cl_mean: 0.2932, Took 8.8438secs\n",
      "Iter: 0301, Val_loss_mean: 0.1286, Val_loss_rg_mean: 0.0739, Val_loss_cl_mean: 0.2930, Took 9.1227secs\n",
      "Iter: 0401, Val_loss_mean: 0.1287, Val_loss_rg_mean: 0.0741, Val_loss_cl_mean: 0.2923, Took 8.9968secs\n",
      "Iter: 0501, Val_loss_mean: 0.1285, Val_loss_rg_mean: 0.0742, Val_loss_cl_mean: 0.2916, Took 8.7014secs\n",
      "\n",
      "Epoch 11 ended\n",
      "Training:  1:34\n",
      "Validation:  0:51\n",
      "Loss decreased from 0.1286 to 0.1286\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_20220131\n",
      "Learning rate decreased to: 1.000e-09\n",
      "\n",
      "\n",
      "Starting epoch: 12\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1310, Tr_loss_rg_mean: 0.0752, Tr_loss_cl_mean: 0.2984, Took 3.0989secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1285, Tr_loss_rg_mean: 0.0735, Tr_loss_cl_mean: 0.2934, Took 16.8336secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1282, Tr_loss_rg_mean: 0.0739, Tr_loss_cl_mean: 0.2912, Took 16.7524secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1279, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2904, Took 17.0132secs\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "curr_loss = 1e5\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    print(f'\\n\\nStarting epoch: {e}')\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Batchwise losses\n",
    "    training_loss = []\n",
    "    training_loss_regress = []\n",
    "    training_loss_class = []\n",
    "    val_loss = []\n",
    "    val_loss_regress = []\n",
    "    val_loss_class = []\n",
    "\n",
    "    # Train\n",
    "    print('Training...')\n",
    "    start = time.time()\n",
    "    for i, (graph_data_tr, targets_tr) in enumerate(get_batch(data_gen_train.generator())):\n",
    "        losses_tr_rg, losses_tr_cl, losses_tr = train_step(graph_data_tr, targets_tr)\n",
    "\n",
    "        training_loss.append(losses_tr.numpy())\n",
    "        training_loss_regress.append(losses_tr_rg.numpy())\n",
    "        training_loss_class.append(losses_tr_cl.numpy())\n",
    "\n",
    "        if not (i-1)%log_freq:\n",
    "            end = time.time()\n",
    "            print(f'Iter: {i:04d}, ', end='')\n",
    "            print(f'Tr_loss_mean: {np.mean(training_loss):.4f}, ', end='')\n",
    "            print(f'Tr_loss_rg_mean: {np.mean(training_loss_regress):.4f}, ', end='') \n",
    "            print(f'Tr_loss_cl_mean: {np.mean(training_loss_class):.4f}, ', end='') \n",
    "            print(f'Took {end-start:.4f}secs')\n",
    "            start = time.time()\n",
    "                  \n",
    "    training_loss_epoch.append(training_loss)\n",
    "    training_loss_regress_epoch.append(training_loss_regress)\n",
    "    training_loss_class_epoch.append(training_loss_class)\n",
    "    training_end = time.time()\n",
    "\n",
    "    # validate\n",
    "    print('\\nValidation...')\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    all_energies = []\n",
    "    start = time.time()\n",
    "    for i, (graph_data_val, targets_val) in enumerate(get_batch(data_gen_val.generator())):\n",
    "        losses_val_rg, losses_val_cl, losses_val, regress_vals, class_vals = val_step(graph_data_val, targets_val)\n",
    "\n",
    "        targets_val = targets_val.numpy()\n",
    "        regress_vals = regress_vals.numpy()\n",
    "        class_vals = class_vals.numpy()\n",
    "\n",
    "        targets_val[:,0] = 10**targets_val[:,0]\n",
    "        regress_vals = 10**regress_vals\n",
    "        class_vals =  tf.math.sigmoid(class_vals)\n",
    "\n",
    "        output_vals = np.hstack([regress_vals, class_vals])\n",
    "\n",
    "        val_loss.append(losses_val.numpy())\n",
    "        val_loss_regress.append(losses_val_rg.numpy())\n",
    "        val_loss_class.append(losses_val_cl.numpy())\n",
    "\n",
    "        all_targets.append(targets_val)\n",
    "        all_outputs.append(output_vals)\n",
    "\n",
    "        if not (i-1)%log_freq:\n",
    "            end = time.time()\n",
    "            print(f'Iter: {i:04d}, ', end='')\n",
    "            print(f'Val_loss_mean: {np.mean(val_loss):.4f}, ', end='')\n",
    "            print(f'Val_loss_rg_mean: {np.mean(val_loss_regress):.4f}, ', end='') \n",
    "            print(f'Val_loss_cl_mean: {np.mean(val_loss_class):.4f}, ', end='') \n",
    "            print(f'Took {end-start:.4f}secs')\n",
    "            start = time.time()\n",
    "\n",
    "    epoch_end = time.time()\n",
    "\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "\n",
    "    val_loss_epoch.append(val_loss)\n",
    "    val_loss_regress_epoch.append(val_loss_regress)\n",
    "    val_loss_class_epoch.append(val_loss_class)\n",
    "\n",
    "    \n",
    "    # Book keeping\n",
    "    val_mins = int((epoch_end - training_end)/60)\n",
    "    val_secs = int((epoch_end - training_end)%60)\n",
    "    training_mins = int((training_end - epoch_start)/60)\n",
    "    training_secs = int((training_end - epoch_start)%60)\n",
    "    print(f'\\nEpoch {e} ended')\n",
    "    print(f'Training: {training_mins:2d}:{training_secs:02d}')\n",
    "    print(f'Validation: {val_mins:2d}:{val_secs:02d}')\n",
    "    \n",
    "    \n",
    "    # Save losses\n",
    "    np.savez(save_dir+'/losses', \n",
    "            training=training_loss_epoch, validation=val_loss_epoch,\n",
    "            training_regress=training_loss_regress_epoch, validation_regress=val_loss_regress_epoch,\n",
    "            training_class=training_loss_class_epoch, validation_class=val_loss_class_epoch,\n",
    "            )\n",
    "\n",
    "    \n",
    "    # Checkpoint if validation loss improved\n",
    "    if np.mean(val_loss)<curr_loss:\n",
    "        print(f'Loss decreased from {curr_loss:.4f} to {np.mean(val_loss):.4f}')\n",
    "        print(f'Checkpointing and saving predictions to:\\n{save_dir}')\n",
    "        curr_loss = np.mean(val_loss)\n",
    "        np.savez(save_dir+'/predictions', \n",
    "                targets=all_targets, \n",
    "                outputs=all_outputs,\n",
    "                energies=all_energies)\n",
    "        checkpoint.save(checkpoint_prefix)\n",
    "    else: \n",
    "        print(f'Loss didnt decrease from {curr_loss:.4f}')\n",
    "    \n",
    "    \n",
    "    # Decrease learning rate every few epochs\n",
    "    if not (e+1)%2:   #%20:\n",
    "        optimizer.learning_rate = optimizer.learning_rate/10\n",
    "        print(f'Learning rate decreased to: {optimizer.learning_rate.value():.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "nbdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
