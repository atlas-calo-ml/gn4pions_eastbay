{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import uproot as ur\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets.graphs import GraphsTuple\n",
    "import sonnet as snt\n",
    "import argparse\n",
    "import yaml\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "from gn4pions.modules.data import GraphDataGenerator\n",
    "from gn4pions.modules.models import MultiOutWeightedRegressModel\n",
    "from gn4pions.modules.utils import convert_to_tuple\n",
    "\n",
    "sns.set_context('poster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model config\n",
    "config_file = 'gn4pions/configs/weightedRegress.yaml'\n",
    "config = yaml.load(open(config_file), Loader=yaml.FullLoader)\n",
    "\n",
    "# Data config\n",
    "data_config = config['data']\n",
    "\n",
    "data_dir = data_config['data_dir']\n",
    "num_train_files = data_config['num_train_files']\n",
    "num_val_files = data_config['num_val_files']\n",
    "batch_size = data_config['batch_size']\n",
    "shuffle = data_config['shuffle']\n",
    "num_procs = data_config['num_procs']\n",
    "preprocess = data_config['preprocess']\n",
    "output_dir = data_config['output_dir']\n",
    "already_preprocessed = data_config['already_preprocessed']  # Set to false when running training for first time\n",
    "\n",
    "# Model Config\n",
    "model_config = config['model']\n",
    "\n",
    "concat_input = model_config['concat_input']\n",
    "\n",
    "\n",
    "# Traning Config\n",
    "train_config = config['training']\n",
    "\n",
    "epochs = train_config['epochs']\n",
    "learning_rate = train_config['learning_rate']\n",
    "alpha = train_config['alpha']\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(train_config['gpu'])\n",
    "log_freq = train_config['log_freq']\n",
    "save_dir = train_config['save_dir'] + config_file.replace('.yaml','').split('/')[-1] + '_' + time.strftime(\"%Y%m%d\")\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "yaml.dump(config, open(save_dir + '/config.yaml', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing and saving data to /clusterfs/ml4hep/mpettee/ml4pions/data//train/\n",
      "Proceesing file number 0\n",
      "Proceesing file number 1\n",
      "Proceesing file number 2\n",
      "Proceesing file number 3\n",
      "Proceesing file number 4\n",
      "Proceesing file number 5\n",
      "Proceesing file number 6\n",
      "Proceesing file number 7\n",
      "Proceesing file number 8\n",
      "Proceesing file number 9\n",
      "Finished processing 9 files\n",
      "Finished processing 6 files\n",
      "Finished processing 3 files\n",
      "Finished processing 8 files\n",
      "Finished processing 2 files\n",
      "Finished processing 4 files\n",
      "Finished processing 7 files\n",
      "Finished processing 1 files\n",
      "Finished processing 0 files\n",
      "Finished processing 5 files\n",
      "\n",
      "Preprocessing and saving data to /clusterfs/ml4hep/mpettee/ml4pions/data//val/\n",
      "Proceesing file number 0\n",
      "Proceesing file number 1\n",
      "Proceesing file number 2\n",
      "Proceesing file number 3\n",
      "Proceesing file number 4\n",
      "Proceesing file number 5\n",
      "Proceesing file number 6\n",
      "Proceesing file number 7\n",
      "Proceesing file number 8\n",
      "Proceesing file number 9\n",
      "Finished processing 0 files\n",
      "Finished processing 1 files\n",
      "Finished processing 8 files\n",
      "Finished processing 6 files\n",
      "Finished processing 3 files\n",
      "Finished processing 2 files\n",
      "Finished processing 4 files\n",
      "Finished processing 5 files\n",
      "Finished processing 9 files\n",
      "Finished processing 7 files\n"
     ]
    }
   ],
   "source": [
    "# Read data and create data generators\n",
    "\n",
    "pi0_files = np.sort(glob.glob(data_dir+'*pi0*/*root'))\n",
    "pion_files = np.sort(glob.glob(data_dir+'*pion*/*root'))\n",
    "\n",
    "train_start = 10\n",
    "train_end = train_start + num_train_files\n",
    "val_end = train_end + num_val_files\n",
    "\n",
    "pi0_train_files = pi0_files[train_start:train_end]\n",
    "pi0_val_files = pi0_files[train_end:val_end]\n",
    "pion_train_files = pion_files[train_start:train_end]\n",
    "pion_val_files = pion_files[train_end:val_end]\n",
    "\n",
    "train_output_dir = None\n",
    "val_output_dir = None\n",
    "\n",
    "# Get Data\n",
    "if preprocess:\n",
    "    train_output_dir = output_dir + '/train/'\n",
    "    val_output_dir = output_dir + '/val/'\n",
    "\n",
    "    if already_preprocessed:\n",
    "        train_files = np.sort(glob.glob(train_output_dir+'*.p'))[:num_train_files]\n",
    "        val_files = np.sort(glob.glob(val_output_dir+'*.p'))[:num_val_files]\n",
    "\n",
    "        pi0_train_files = train_files\n",
    "        pi0_val_files = val_files\n",
    "        pion_train_files = None\n",
    "        pion_val_files = None\n",
    "\n",
    "        train_output_dir = None\n",
    "        val_output_dir = None\n",
    "\n",
    "# Traning Data Generator\n",
    "# Will preprocess data if it doesnt find pickled files\n",
    "data_gen_train = GraphDataGenerator(pi0_file_list=pi0_train_files,\n",
    "                                    pion_file_list=pion_train_files,\n",
    "                                    cellGeo_file=data_dir+'cell_geo.root',\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=shuffle,\n",
    "                                    num_procs=num_procs,\n",
    "                                    preprocess=preprocess,\n",
    "                                    output_dir=train_output_dir)\n",
    "\n",
    "# Validation Data generator\n",
    "# Will preprocess data if it doesnt find pickled files\n",
    "data_gen_val = GraphDataGenerator(pi0_file_list=pi0_val_files,\n",
    "                                  pion_file_list=pion_val_files,\n",
    "                                  cellGeo_file=data_dir+'cell_geo.root',\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=shuffle,\n",
    "                                  num_procs=num_procs,\n",
    "                                  preprocess=preprocess,\n",
    "                                  output_dir=val_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get batch of data\n",
    "def get_batch(data_iter):\n",
    "    for graphs, targets in data_iter:\n",
    "        graphs = convert_to_tuple(graphs)\n",
    "        targets = tf.convert_to_tensor(targets)\n",
    "        yield graphs, targets\n",
    "        \n",
    "# Define loss function        \n",
    "mae_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def loss_fn(targets, regress_preds, class_preds):\n",
    "    regress_loss = mae_loss(targets[:,:1], regress_preds)\n",
    "    class_loss = bce_loss(targets[:,1:], class_preds)\n",
    "    combined_loss = alpha*regress_loss + (1 - alpha)*class_loss \n",
    "    return regress_loss, class_loss, combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 09:25:41.728895: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 09:25:42.672750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19934 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Get a sample graph for tf.function decorator\n",
    "samp_graph, samp_target = next(get_batch(data_gen_train.generator()))\n",
    "data_gen_train.kill_procs()\n",
    "graph_spec = utils_tf.specs_from_graphs_tuple(samp_graph, True, True, True)\n",
    "\n",
    "# Traning set\n",
    "@tf.function(input_signature=[graph_spec, tf.TensorSpec(shape=[None,2], dtype=tf.float32)])\n",
    "def train_step(graphs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        regress_output, class_output = model(graphs)\n",
    "        regress_preds = regress_output.globals\n",
    "        class_preds = class_output.globals\n",
    "        regress_loss, class_loss, loss = loss_fn(targets, regress_preds, class_preds)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return regress_loss, class_loss, loss\n",
    "\n",
    "# Validation Step\n",
    "@tf.function(input_signature=[graph_spec, tf.TensorSpec(shape=[None,2], dtype=tf.float32)])\n",
    "def val_step(graphs, targets):\n",
    "    regress_output, class_output = model(graphs)\n",
    "    regress_preds = regress_output.globals\n",
    "    class_preds = class_output.globals\n",
    "    regress_loss, class_loss, loss = loss_fn(targets, regress_preds, class_preds)\n",
    "    return regress_loss, class_loss, loss, regress_preds, class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "model = MultiOutWeightedRegressModel(global_output_size=1, num_outputs=2, model_config=model_config)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Average epoch losses\n",
    "training_loss_epoch = []\n",
    "training_loss_regress_epoch = []\n",
    "training_loss_class_epoch = []\n",
    "val_loss_epoch = []\n",
    "val_loss_regress_epoch = []\n",
    "val_loss_class_epoch = []\n",
    "\n",
    "# Model checkpointing, load latest model if available\n",
    "checkpoint = tf.train.Checkpoint(module=model)\n",
    "checkpoint_prefix = os.path.join(save_dir, 'latest_model')\n",
    "latest = tf.train.latest_checkpoint(save_dir)\n",
    "if latest is not None:\n",
    "    checkpoint.restore(latest)\n",
    "else:\n",
    "    checkpoint.save(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting epoch: 0\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-06 09:26:36.522935: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 71), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/nbdev/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 71), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2021-12-06 09:26:38.034459: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0001, Tr_loss_mean: 1.1915, Tr_loss_rg_mean: 1.2371, Tr_loss_cl_mean: 1.0545, Took 12.5393secs\n",
      "Iter: 0101, Tr_loss_mean: 0.3314, Tr_loss_rg_mean: 0.2268, Tr_loss_cl_mean: 0.6450, Took 17.2426secs\n",
      "Iter: 0201, Tr_loss_mean: 0.2984, Tr_loss_rg_mean: 0.1869, Tr_loss_cl_mean: 0.6332, Took 17.4371secs\n",
      "Iter: 0301, Tr_loss_mean: 0.2863, Tr_loss_rg_mean: 0.1741, Tr_loss_cl_mean: 0.6231, Took 17.7486secs\n",
      "Iter: 0401, Tr_loss_mean: 0.2698, Tr_loss_rg_mean: 0.1604, Tr_loss_cl_mean: 0.5979, Took 17.3182secs\n",
      "Iter: 0501, Tr_loss_mean: 0.2540, Tr_loss_rg_mean: 0.1490, Tr_loss_cl_mean: 0.5692, Took 17.0685secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1729, Val_loss_rg_mean: 0.0972, Val_loss_cl_mean: 0.4002, Took 3.8958secs\n",
      "Iter: 0101, Val_loss_mean: 0.1743, Val_loss_rg_mean: 0.0970, Val_loss_cl_mean: 0.4062, Took 10.3665secs\n",
      "Iter: 0201, Val_loss_mean: 0.1738, Val_loss_rg_mean: 0.0967, Val_loss_cl_mean: 0.4052, Took 10.3486secs\n",
      "Iter: 0301, Val_loss_mean: 0.1739, Val_loss_rg_mean: 0.0966, Val_loss_cl_mean: 0.4056, Took 10.6893secs\n",
      "Iter: 0401, Val_loss_mean: 0.1740, Val_loss_rg_mean: 0.0967, Val_loss_cl_mean: 0.4059, Took 10.4231secs\n",
      "Iter: 0501, Val_loss_mean: 0.1740, Val_loss_rg_mean: 0.0967, Val_loss_cl_mean: 0.4060, Took 9.8810secs\n",
      "\n",
      "Epoch 0 ended\n",
      "Training:  1:46\n",
      "Validation:  1:00\n",
      "Loss decreased from 100000.0000 to 0.1739\n",
      "Checkpointing and saving predictions to:\n",
      "results/weightedRegress_20211206\n",
      "\n",
      "\n",
      "Starting epoch: 1\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1633, Tr_loss_rg_mean: 0.0865, Tr_loss_cl_mean: 0.3938, Took 2.9382secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1651, Tr_loss_rg_mean: 0.0919, Tr_loss_cl_mean: 0.3849, Took 17.4811secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1641, Tr_loss_rg_mean: 0.0927, Tr_loss_cl_mean: 0.3783, Took 18.4709secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1620, Tr_loss_rg_mean: 0.0920, Tr_loss_cl_mean: 0.3719, Took 18.2940secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1603, Tr_loss_rg_mean: 0.0918, Tr_loss_cl_mean: 0.3660, Took 17.7026secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1579, Tr_loss_rg_mean: 0.0909, Tr_loss_cl_mean: 0.3592, Took 17.5819secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1493, Val_loss_rg_mean: 0.0828, Val_loss_cl_mean: 0.3485, Took 2.7660secs\n",
      "Iter: 0101, Val_loss_mean: 0.1508, Val_loss_rg_mean: 0.0841, Val_loss_cl_mean: 0.3508, Took 10.0055secs\n",
      "Iter: 0201, Val_loss_mean: 0.1503, Val_loss_rg_mean: 0.0838, Val_loss_cl_mean: 0.3499, Took 10.1425secs\n",
      "Iter: 0301, Val_loss_mean: 0.1505, Val_loss_rg_mean: 0.0838, Val_loss_cl_mean: 0.3506, Took 10.0007secs\n",
      "Iter: 0401, Val_loss_mean: 0.1507, Val_loss_rg_mean: 0.0839, Val_loss_cl_mean: 0.3509, Took 9.7888secs\n",
      "Iter: 0501, Val_loss_mean: 0.1507, Val_loss_rg_mean: 0.0839, Val_loss_cl_mean: 0.3511, Took 9.4870secs\n",
      "\n",
      "Epoch 1 ended\n",
      "Training:  1:39\n",
      "Validation:  0:56\n",
      "Loss decreased from 0.1739 to 0.1505\n",
      "Checkpointing and saving predictions to:\n",
      "results/weightedRegress_20211206\n",
      "Learning rate decreased to: 1.000e-04\n",
      "\n",
      "\n",
      "Starting epoch: 2\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1422, Tr_loss_rg_mean: 0.0791, Tr_loss_cl_mean: 0.3315, Took 3.2233secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1400, Tr_loss_rg_mean: 0.0775, Tr_loss_cl_mean: 0.3275, Took 16.6730secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1368, Tr_loss_rg_mean: 0.0767, Tr_loss_cl_mean: 0.3171, Took 16.8863secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1355, Tr_loss_rg_mean: 0.0762, Tr_loss_cl_mean: 0.3134, Took 16.8872secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1345, Tr_loss_rg_mean: 0.0761, Tr_loss_cl_mean: 0.3098, Took 16.7140secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1334, Tr_loss_rg_mean: 0.0757, Tr_loss_cl_mean: 0.3066, Took 16.8852secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1331, Val_loss_rg_mean: 0.0778, Val_loss_cl_mean: 0.2991, Took 3.0024secs\n",
      "Iter: 0101, Val_loss_mean: 0.1308, Val_loss_rg_mean: 0.0751, Val_loss_cl_mean: 0.2980, Took 10.5305secs\n",
      "Iter: 0201, Val_loss_mean: 0.1302, Val_loss_rg_mean: 0.0750, Val_loss_cl_mean: 0.2957, Took 10.5593secs\n",
      "Iter: 0301, Val_loss_mean: 0.1301, Val_loss_rg_mean: 0.0750, Val_loss_cl_mean: 0.2955, Took 10.8489secs\n",
      "Iter: 0401, Val_loss_mean: 0.1301, Val_loss_rg_mean: 0.0750, Val_loss_cl_mean: 0.2955, Took 11.0437secs\n",
      "Iter: 0501, Val_loss_mean: 0.1302, Val_loss_rg_mean: 0.0749, Val_loss_cl_mean: 0.2960, Took 10.3466secs\n",
      "\n",
      "Epoch 2 ended\n",
      "Training:  1:34\n",
      "Validation:  1:00\n",
      "Loss decreased from 0.1505 to 0.1301\n",
      "Checkpointing and saving predictions to:\n",
      "results/weightedRegress_20211206\n",
      "\n",
      "\n",
      "Starting epoch: 3\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1227, Tr_loss_rg_mean: 0.0732, Tr_loss_cl_mean: 0.2711, Took 2.9643secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1298, Tr_loss_rg_mean: 0.0749, Tr_loss_cl_mean: 0.2946, Took 17.2776secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1291, Tr_loss_rg_mean: 0.0749, Tr_loss_cl_mean: 0.2919, Took 17.3285secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1289, Tr_loss_rg_mean: 0.0747, Tr_loss_cl_mean: 0.2916, Took 17.6885secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1289, Tr_loss_rg_mean: 0.0748, Tr_loss_cl_mean: 0.2912, Took 17.3670secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1282, Tr_loss_rg_mean: 0.0745, Tr_loss_cl_mean: 0.2892, Took 17.2505secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1302, Val_loss_rg_mean: 0.0750, Val_loss_cl_mean: 0.2959, Took 2.7954secs\n",
      "Iter: 0101, Val_loss_mean: 0.1282, Val_loss_rg_mean: 0.0751, Val_loss_cl_mean: 0.2875, Took 10.3912secs\n",
      "Iter: 0201, Val_loss_mean: 0.1274, Val_loss_rg_mean: 0.0748, Val_loss_cl_mean: 0.2852, Took 10.0163secs\n",
      "Iter: 0301, Val_loss_mean: 0.1274, Val_loss_rg_mean: 0.0747, Val_loss_cl_mean: 0.2854, Took 9.8795secs\n",
      "Iter: 0401, Val_loss_mean: 0.1275, Val_loss_rg_mean: 0.0748, Val_loss_cl_mean: 0.2855, Took 10.0369secs\n",
      "Iter: 0501, Val_loss_mean: 0.1275, Val_loss_rg_mean: 0.0748, Val_loss_cl_mean: 0.2858, Took 9.5341secs\n",
      "\n",
      "Epoch 3 ended\n",
      "Training:  1:37\n",
      "Validation:  0:57\n",
      "Loss decreased from 0.1301 to 0.1274\n",
      "Checkpointing and saving predictions to:\n",
      "results/weightedRegress_20211206\n",
      "Learning rate decreased to: 1.000e-05\n",
      "\n",
      "\n",
      "Starting epoch: 4\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1197, Tr_loss_rg_mean: 0.0710, Tr_loss_cl_mean: 0.2660, Took 2.9690secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1258, Tr_loss_rg_mean: 0.0740, Tr_loss_cl_mean: 0.2814, Took 16.7496secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1253, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2797, Took 16.9998secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1254, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2802, Took 17.5088secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1253, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2798, Took 17.2830secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1249, Tr_loss_rg_mean: 0.0735, Tr_loss_cl_mean: 0.2789, Took 17.2090secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1270, Val_loss_rg_mean: 0.0737, Val_loss_cl_mean: 0.2870, Took 3.0204secs\n",
      "Iter: 0101, Val_loss_mean: 0.1255, Val_loss_rg_mean: 0.0741, Val_loss_cl_mean: 0.2796, Took 10.0276secs\n",
      "Iter: 0201, Val_loss_mean: 0.1247, Val_loss_rg_mean: 0.0737, Val_loss_cl_mean: 0.2778, Took 10.0872secs\n",
      "Iter: 0301, Val_loss_mean: 0.1247, Val_loss_rg_mean: 0.0737, Val_loss_cl_mean: 0.2777, Took 10.1202secs\n",
      "Iter: 0401, Val_loss_mean: 0.1248, Val_loss_rg_mean: 0.0738, Val_loss_cl_mean: 0.2778, Took 9.8095secs\n",
      "Iter: 0501, Val_loss_mean: 0.1249, Val_loss_rg_mean: 0.0738, Val_loss_cl_mean: 0.2781, Took 9.9321secs\n",
      "\n",
      "Epoch 4 ended\n",
      "Training:  1:36\n",
      "Validation:  0:57\n",
      "Loss decreased from 0.1274 to 0.1247\n",
      "Checkpointing and saving predictions to:\n",
      "results/weightedRegress_20211206\n",
      "\n",
      "\n",
      "Starting epoch: 5\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1202, Tr_loss_rg_mean: 0.0757, Tr_loss_cl_mean: 0.2539, Took 3.1304secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1253, Tr_loss_rg_mean: 0.0738, Tr_loss_cl_mean: 0.2798, Took 16.6943secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1247, Tr_loss_rg_mean: 0.0737, Tr_loss_cl_mean: 0.2776, Took 17.0186secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1248, Tr_loss_rg_mean: 0.0736, Tr_loss_cl_mean: 0.2783, Took 17.0850secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1247, Tr_loss_rg_mean: 0.0737, Tr_loss_cl_mean: 0.2779, Took 16.8336secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1243, Tr_loss_rg_mean: 0.0734, Tr_loss_cl_mean: 0.2771, Took 17.1188secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1285, Val_loss_rg_mean: 0.0770, Val_loss_cl_mean: 0.2831, Took 2.7635secs\n",
      "Iter: 0101, Val_loss_mean: 0.1252, Val_loss_rg_mean: 0.0740, Val_loss_cl_mean: 0.2790, Took 10.2591secs\n",
      "Iter: 0201, Val_loss_mean: 0.1243, Val_loss_rg_mean: 0.0736, Val_loss_cl_mean: 0.2765, Took 9.4165secs\n",
      "Iter: 0301, Val_loss_mean: 0.1245, Val_loss_rg_mean: 0.0737, Val_loss_cl_mean: 0.2769, Took 9.1325secs\n",
      "Iter: 0401, Val_loss_mean: 0.1245, Val_loss_rg_mean: 0.0737, Val_loss_cl_mean: 0.2766, Took 9.2452secs\n",
      "Iter: 0501, Val_loss_mean: 0.1245, Val_loss_rg_mean: 0.0737, Val_loss_cl_mean: 0.2769, Took 9.3835secs\n",
      "\n",
      "Epoch 5 ended\n",
      "Training:  1:35\n",
      "Validation:  0:54\n",
      "Loss decreased from 0.1247 to 0.1244\n",
      "Checkpointing and saving predictions to:\n",
      "results/weightedRegress_20211206\n",
      "Learning rate decreased to: 1.000e-06\n",
      "\n",
      "\n",
      "Starting epoch: 6\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1167, Tr_loss_rg_mean: 0.0698, Tr_loss_cl_mean: 0.2575, Took 2.9564secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1246, Tr_loss_rg_mean: 0.0736, Tr_loss_cl_mean: 0.2775, Took 16.7930secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1241, Tr_loss_rg_mean: 0.0734, Tr_loss_cl_mean: 0.2761, Took 17.0278secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1243, Tr_loss_rg_mean: 0.0734, Tr_loss_cl_mean: 0.2768, Took 17.1073secs\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "curr_loss = 1e5\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    print(f'\\n\\nStarting epoch: {e}')\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Batchwise losses\n",
    "    training_loss = []\n",
    "    training_loss_regress = []\n",
    "    training_loss_class = []\n",
    "    val_loss = []\n",
    "    val_loss_regress = []\n",
    "    val_loss_class = []\n",
    "\n",
    "    # Train\n",
    "    print('Training...')\n",
    "    start = time.time()\n",
    "    for i, (graph_data_tr, targets_tr) in enumerate(get_batch(data_gen_train.generator())):\n",
    "        losses_tr_rg, losses_tr_cl, losses_tr = train_step(graph_data_tr, targets_tr)\n",
    "\n",
    "        training_loss.append(losses_tr.numpy())\n",
    "        training_loss_regress.append(losses_tr_rg.numpy())\n",
    "        training_loss_class.append(losses_tr_cl.numpy())\n",
    "\n",
    "        if not (i-1)%log_freq:\n",
    "            end = time.time()\n",
    "            print(f'Iter: {i:04d}, ', end='')\n",
    "            print(f'Tr_loss_mean: {np.mean(training_loss):.4f}, ', end='')\n",
    "            print(f'Tr_loss_rg_mean: {np.mean(training_loss_regress):.4f}, ', end='') \n",
    "            print(f'Tr_loss_cl_mean: {np.mean(training_loss_class):.4f}, ', end='') \n",
    "            print(f'Took {end-start:.4f}secs')\n",
    "            start = time.time()\n",
    "                  \n",
    "    training_loss_epoch.append(training_loss)\n",
    "    training_loss_regress_epoch.append(training_loss_regress)\n",
    "    training_loss_class_epoch.append(training_loss_class)\n",
    "    training_end = time.time()\n",
    "\n",
    "    # validate\n",
    "    print('\\nValidation...')\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    start = time.time()\n",
    "    for i, (graph_data_val, targets_val) in enumerate(get_batch(data_gen_val.generator())):\n",
    "        losses_val_rg, losses_val_cl, losses_val, regress_vals, class_vals = val_step(graph_data_val, targets_val)\n",
    "\n",
    "        targets_val = targets_val.numpy()\n",
    "        regress_vals = regress_vals.numpy()\n",
    "        class_vals = class_vals.numpy()\n",
    "\n",
    "        targets_val[:,0] = 10**targets_val[:,0]\n",
    "        regress_vals = 10**regress_vals\n",
    "        class_vals =  tf.math.sigmoid(class_vals)\n",
    "\n",
    "        output_vals = np.hstack([regress_vals, class_vals])\n",
    "\n",
    "        val_loss.append(losses_val.numpy())\n",
    "        val_loss_regress.append(losses_val_rg.numpy())\n",
    "        val_loss_class.append(losses_val_cl.numpy())\n",
    "\n",
    "        all_targets.append(targets_val)\n",
    "        all_outputs.append(output_vals)\n",
    "\n",
    "        if not (i-1)%log_freq:\n",
    "            end = time.time()\n",
    "            print(f'Iter: {i:04d}, ', end='')\n",
    "            print(f'Val_loss_mean: {np.mean(val_loss):.4f}, ', end='')\n",
    "            print(f'Val_loss_rg_mean: {np.mean(val_loss_regress):.4f}, ', end='') \n",
    "            print(f'Val_loss_cl_mean: {np.mean(val_loss_class):.4f}, ', end='') \n",
    "            print(f'Took {end-start:.4f}secs')\n",
    "            start = time.time()\n",
    "\n",
    "    epoch_end = time.time()\n",
    "\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "\n",
    "    val_loss_epoch.append(val_loss)\n",
    "    val_loss_regress_epoch.append(val_loss_regress)\n",
    "    val_loss_class_epoch.append(val_loss_class)\n",
    "\n",
    "    \n",
    "    # Book keeping\n",
    "    val_mins = int((epoch_end - training_end)/60)\n",
    "    val_secs = int((epoch_end - training_end)%60)\n",
    "    training_mins = int((training_end - epoch_start)/60)\n",
    "    training_secs = int((training_end - epoch_start)%60)\n",
    "    print(f'\\nEpoch {e} ended')\n",
    "    print(f'Training: {training_mins:2d}:{training_secs:02d}')\n",
    "    print(f'Validation: {val_mins:2d}:{val_secs:02d}')\n",
    "    \n",
    "    \n",
    "    # Save losses\n",
    "    np.savez(save_dir+'/losses', \n",
    "            training=training_loss_epoch, validation=val_loss_epoch,\n",
    "            training_regress=training_loss_regress_epoch, validation_regress=val_loss_regress_epoch,\n",
    "            training_class=training_loss_class_epoch, validation_class=val_loss_class_epoch,\n",
    "            )\n",
    "\n",
    "    \n",
    "    # Checkpoint if validation loss improved\n",
    "    if np.mean(val_loss)<curr_loss:\n",
    "        print(f'Loss decreased from {curr_loss:.4f} to {np.mean(val_loss):.4f}')\n",
    "        print(f'Checkpointing and saving predictions to:\\n{save_dir}')\n",
    "        curr_loss = np.mean(val_loss)\n",
    "        np.savez(save_dir+'/predictions', \n",
    "                targets=all_targets, \n",
    "                outputs=all_outputs)\n",
    "        checkpoint.save(checkpoint_prefix)\n",
    "    else: \n",
    "        print(f'Loss didnt decrease from {curr_loss:.4f}')\n",
    "    \n",
    "    \n",
    "    # Decrease learning rate every few epochs\n",
    "    if not (e+1)%2:   #%20:\n",
    "        optimizer.learning_rate = optimizer.learning_rate/10\n",
    "        print(f'Learning rate decreased to: {optimizer.learning_rate.value():.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "nbdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
