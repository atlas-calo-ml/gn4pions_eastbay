{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import uproot as ur\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets.graphs import GraphsTuple\n",
    "import sonnet as snt\n",
    "import argparse\n",
    "import yaml\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gn4pions.modules.data import GraphDataGenerator\n",
    "from gn4pions.modules.models import MultiOutWeightedRegressModel\n",
    "from gn4pions.modules.utils import convert_to_tuple\n",
    "\n",
    "sns.set_context('poster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model config\n",
    "config_file = 'gn4pions/configs/ming_tracks_full.yaml' # for a quick run of the notebook\n",
    "# config_file = 'gn4pions/configs/baseline.yaml' # for actual training\n",
    "config = yaml.load(open(config_file), Loader=yaml.FullLoader)\n",
    "\n",
    "# Data config\n",
    "data_config = config['data']\n",
    "\n",
    "data_dir = data_config['data_dir']\n",
    "num_train_files = data_config['num_train_files']\n",
    "num_val_files = data_config['num_val_files']\n",
    "batch_size = data_config['batch_size']\n",
    "shuffle = data_config['shuffle']\n",
    "num_procs = data_config['num_procs']\n",
    "preprocess = data_config['preprocess']\n",
    "output_dir = data_config['output_dir']\n",
    "already_preprocessed = data_config['already_preprocessed']  # Set to false when running training for first time\n",
    "\n",
    "# Model Config\n",
    "model_config = config['model']\n",
    "\n",
    "concat_input = model_config['concat_input']\n",
    "\n",
    "\n",
    "# Traning Config\n",
    "train_config = config['training']\n",
    "\n",
    "epochs = train_config['epochs']\n",
    "learning_rate = train_config['learning_rate']\n",
    "alpha = train_config['alpha']\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(train_config['gpu'])\n",
    "log_freq = train_config['log_freq']\n",
    "save_dir = train_config['save_dir'] + config_file.replace('.yaml','').split('/')[-1] + '_' + time.strftime(\"%Y%m%d\")\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "yaml.dump(config, open(save_dir + '/config.yaml', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing and saving data to /clusterfs/ml4hep/mfong/ML4Pions/graph_data_tracks/train/\n",
      "Processing file number 0\n",
      "Processing file number 1\n",
      "Processing file number 2\n",
      "Processing file number 3\n",
      "Processing file number 4\n",
      "Processing file number 5\n",
      "Processing file number 6\n",
      "Processing file number 7\n",
      "Processing file number 8\n",
      "Processing file number 9\n",
      "Processing file number 10\n",
      "Processing file number 11\n",
      "Processing file number 12\n",
      "Processing file number 13\n",
      "Processing file number 14\n",
      "Processing file number 15\n",
      "Processing file number 16\n",
      "Processing file number 17\n",
      "Processing file number 18Processing file number 19\n",
      "Processing file number 20\n",
      "\n",
      "Processing file number 21\n",
      "Processing file number 22\n",
      "Processing file number 23\n",
      "Processing file number 24\n",
      "Processing file number 25\n",
      "Processing file number 26\n",
      "Processing file number 27\n",
      "Processing file number 28Processing file number 29\n",
      "\n",
      "Processing file number 30\n",
      "Processing file number 31\n"
     ]
    }
   ],
   "source": [
    "# Read data and create data generators\n",
    "\n",
    "pi0_files = np.sort(glob.glob(data_dir+'*pi0*/*root'))\n",
    "pion_files = np.sort(glob.glob(data_dir+'*pion*/*root'))\n",
    "\n",
    "train_start = 0\n",
    "train_end = train_start + num_train_files\n",
    "val_end = train_end + num_val_files\n",
    "\n",
    "pi0_train_files = pi0_files[train_start:train_end]\n",
    "pi0_val_files = pi0_files[train_end:val_end]\n",
    "pion_train_files = pion_files[train_start:train_end]\n",
    "pion_val_files = pion_files[train_end:val_end]\n",
    "\n",
    "train_output_dir = None\n",
    "val_output_dir = None\n",
    "\n",
    "# Get Data\n",
    "if preprocess:\n",
    "    train_output_dir = output_dir + 'train/'\n",
    "    val_output_dir = output_dir + 'val/'\n",
    "\n",
    "    if already_preprocessed:\n",
    "        train_files = np.sort(glob.glob(train_output_dir+'*.p'))[:num_train_files]\n",
    "        val_files = np.sort(glob.glob(val_output_dir+'*.p'))[:num_val_files]\n",
    "\n",
    "        pi0_train_files = train_files\n",
    "        pi0_val_files = val_files\n",
    "        pion_train_files = None\n",
    "        pion_val_files = None\n",
    "\n",
    "        train_output_dir = None\n",
    "        val_output_dir = None\n",
    "\n",
    "# Traning Data Generator\n",
    "# Will preprocess data if it doesnt find pickled files\n",
    "data_gen_train = GraphDataGenerator(pi0_file_list=pi0_train_files,\n",
    "                                    pion_file_list=pion_train_files,\n",
    "                                    cellGeo_file=data_dir+'CellGeo.neighbours.root',\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=shuffle,\n",
    "                                    num_procs=num_procs,\n",
    "                                    preprocess=preprocess,\n",
    "                                    output_dir=train_output_dir)\n",
    "\n",
    "# Validation Data generator\n",
    "# Will preprocess data if it doesnt find pickled files\n",
    "data_gen_val = GraphDataGenerator(pi0_file_list=pi0_val_files,\n",
    "                                  pion_file_list=pion_val_files,\n",
    "                                  cellGeo_file=data_dir+'CellGeo.neighbours.root',\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=shuffle,\n",
    "                                  num_procs=num_procs,\n",
    "                                  preprocess=preprocess,\n",
    "                                  output_dir=val_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get batch of data\n",
    "def get_batch(data_iter):\n",
    "    for graphs, targets in data_iter:\n",
    "        graphs = convert_to_tuple(graphs)\n",
    "        targets = tf.convert_to_tensor(targets)\n",
    "        yield graphs, targets\n",
    "        \n",
    "# Define loss function        \n",
    "mae_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def loss_fn(targets, regress_preds, class_preds):\n",
    "    regress_loss = mae_loss(targets[:,:1], regress_preds)\n",
    "    class_loss = bce_loss(targets[:,1:], class_preds)\n",
    "    combined_loss = alpha*regress_loss + (1 - alpha)*class_loss \n",
    "    return regress_loss, class_loss, combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a sample graph for tf.function decorator\n",
    "samp_graph, samp_target = next(get_batch(data_gen_train.generator()))\n",
    "data_gen_train.kill_procs()\n",
    "graph_spec = utils_tf.specs_from_graphs_tuple(samp_graph, True, True, True)\n",
    "\n",
    "# Training set\n",
    "@tf.function(input_signature=[graph_spec, tf.TensorSpec(shape=[None,2], dtype=tf.float32)])\n",
    "def train_step(graphs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        regress_output, class_output = model(graphs)\n",
    "        regress_preds = regress_output.globals\n",
    "        class_preds = class_output.globals\n",
    "        regress_loss, class_loss, loss = loss_fn(targets, regress_preds, class_preds)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return regress_loss, class_loss, loss\n",
    "\n",
    "# Validation Step\n",
    "@tf.function(input_signature=[graph_spec, tf.TensorSpec(shape=[None,2], dtype=tf.float32)])\n",
    "def val_step(graphs, targets):\n",
    "    regress_output, class_output = model(graphs)\n",
    "    regress_preds = regress_output.globals\n",
    "    class_preds = class_output.globals\n",
    "    regress_loss, class_loss, loss = loss_fn(targets, regress_preds, class_preds)\n",
    "    return regress_loss, class_loss, loss, regress_preds, class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphsTuple(nodes=<tf.Tensor: shape=(65955, 11), dtype=float32, numpy=\n",
       "array([[-0.2833688 ,  0.21428572, -2.1990156 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.68640566,  0.21428572, -2.1989949 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.883118  ,  0.21428572, -2.1990368 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-1.2855362 ,  0.        ,  0.74026513, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-2.298425  ,  0.03571429,  0.7353846 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.9210465 ,  0.03571429,  0.73531723, ...,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)>, edges=<tf.Tensor: shape=(205464, 10), dtype=float32, numpy=\n",
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, receivers=<tf.Tensor: shape=(205464,), dtype=int32, numpy=array([    1,     2,     3, ..., 65917, 65918, 65919], dtype=int32)>, senders=<tf.Tensor: shape=(205464,), dtype=int32, numpy=array([    0,     0,     0, ..., 65917, 65918, 65919], dtype=int32)>, globals=<tf.Tensor: shape=(1024, 1), dtype=float32, numpy=\n",
       "array([[0.64995015],\n",
       "       [0.66356677],\n",
       "       [0.36523914],\n",
       "       ...,\n",
       "       [2.6480541 ],\n",
       "       [1.4730654 ],\n",
       "       [0.39790484]], dtype=float32)>, n_node=<tf.Tensor: shape=(1024,), dtype=int64, numpy=array([ 42,  37,  15, ..., 143,  51,  40])>, n_edge=<tf.Tensor: shape=(1024,), dtype=int64, numpy=array([119,  98,  45, ..., 460, 143, 111])>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "model = MultiOutWeightedRegressModel(global_output_size=1, num_outputs=2, model_config=model_config)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Average epoch losses\n",
    "training_loss_epoch = []\n",
    "training_loss_regress_epoch = []\n",
    "training_loss_class_epoch = []\n",
    "val_loss_epoch = []\n",
    "val_loss_regress_epoch = []\n",
    "val_loss_class_epoch = []\n",
    "\n",
    "# Model checkpointing, load latest model if available\n",
    "checkpoint = tf.train.Checkpoint(module=model)\n",
    "checkpoint_prefix = os.path.join(save_dir, 'latest_model')\n",
    "latest = tf.train.latest_checkpoint(save_dir)\n",
    "if latest is not None:\n",
    "    checkpoint.restore(latest)\n",
    "else:\n",
    "    checkpoint.save(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (graph_data_tr, targets_tr) in enumerate(get_batch(data_gen_train.generator())):\n",
    "#     print(\"Targets: \",targets_tr[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting epoch: 0\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 01:51:16.242679: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "/global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_3/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 128), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_2/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Reshape:0\", shape=(None, 75), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_receiver_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_sender_nodes_to_edges/Reshape:0\", shape=(None, 75), dtype=float32), dense_shape=Tensor(\"gradient_tape/MultiOutWeightedRegressModel/core_1/graph_network/edge_block/broadcast_sender_nodes_to_edges/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2022-02-25 01:51:17.874576: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0001, Tr_loss_mean: 1.1912, Tr_loss_rg_mean: 1.3625, Tr_loss_cl_mean: 0.6774, Took 12.8812secs\n",
      "Iter: 0101, Tr_loss_mean: 0.3749, Tr_loss_rg_mean: 0.2921, Tr_loss_cl_mean: 0.6232, Took 18.3654secs\n",
      "Iter: 0201, Tr_loss_mean: 0.3174, Tr_loss_rg_mean: 0.2196, Tr_loss_cl_mean: 0.6107, Took 18.9418secs\n",
      "Iter: 0301, Tr_loss_mean: 0.2811, Tr_loss_rg_mean: 0.1868, Tr_loss_cl_mean: 0.5639, Took 19.0842secs\n",
      "Iter: 0401, Tr_loss_mean: 0.2584, Tr_loss_rg_mean: 0.1696, Tr_loss_cl_mean: 0.5247, Took 19.2515secs\n",
      "Iter: 0501, Tr_loss_mean: 0.2436, Tr_loss_rg_mean: 0.1601, Tr_loss_cl_mean: 0.4940, Took 18.8412secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1558, Val_loss_rg_mean: 0.0948, Val_loss_cl_mean: 0.3385, Took 4.2126secs\n",
      "Iter: 0101, Val_loss_mean: 0.1601, Val_loss_rg_mean: 0.0923, Val_loss_cl_mean: 0.3633, Took 11.0951secs\n",
      "Iter: 0201, Val_loss_mean: 0.1608, Val_loss_rg_mean: 0.0922, Val_loss_cl_mean: 0.3666, Took 11.2257secs\n",
      "Iter: 0301, Val_loss_mean: 0.1606, Val_loss_rg_mean: 0.0921, Val_loss_cl_mean: 0.3661, Took 11.4922secs\n",
      "Iter: 0401, Val_loss_mean: 0.1605, Val_loss_rg_mean: 0.0921, Val_loss_cl_mean: 0.3657, Took 11.4252secs\n",
      "Iter: 0501, Val_loss_mean: 0.1606, Val_loss_rg_mean: 0.0922, Val_loss_cl_mean: 0.3657, Took 10.7895secs\n",
      "\n",
      "Epoch 0 ended\n",
      "Training:  1:55\n",
      "Validation:  1:04\n",
      "Loss decreased from 100000.0000 to 0.1606\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_ming_20220224\n",
      "\n",
      "\n",
      "Starting epoch: 1\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1659, Tr_loss_rg_mean: 0.0936, Tr_loss_cl_mean: 0.3830, Took 3.4086secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1583, Tr_loss_rg_mean: 0.0944, Tr_loss_cl_mean: 0.3502, Took 18.2608secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1591, Tr_loss_rg_mean: 0.0971, Tr_loss_cl_mean: 0.3453, Took 19.1188secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1580, Tr_loss_rg_mean: 0.0965, Tr_loss_cl_mean: 0.3424, Took 18.6843secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1552, Tr_loss_rg_mean: 0.0946, Tr_loss_cl_mean: 0.3367, Took 18.9421secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1538, Tr_loss_rg_mean: 0.0943, Tr_loss_cl_mean: 0.3323, Took 18.2788secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1618, Val_loss_rg_mean: 0.1045, Val_loss_cl_mean: 0.3338, Took 2.9278secs\n",
      "Iter: 0101, Val_loss_mean: 0.1655, Val_loss_rg_mean: 0.1029, Val_loss_cl_mean: 0.3534, Took 10.3991secs\n",
      "Iter: 0201, Val_loss_mean: 0.1662, Val_loss_rg_mean: 0.1027, Val_loss_cl_mean: 0.3568, Took 10.8286secs\n",
      "Iter: 0301, Val_loss_mean: 0.1659, Val_loss_rg_mean: 0.1025, Val_loss_cl_mean: 0.3560, Took 10.8609secs\n",
      "Iter: 0401, Val_loss_mean: 0.1657, Val_loss_rg_mean: 0.1024, Val_loss_cl_mean: 0.3557, Took 10.7531secs\n",
      "Iter: 0501, Val_loss_mean: 0.1659, Val_loss_rg_mean: 0.1025, Val_loss_cl_mean: 0.3561, Took 9.9732secs\n",
      "\n",
      "Epoch 1 ended\n",
      "Training:  1:44\n",
      "Validation:  1:00\n",
      "Loss didnt decrease from 0.1606\n",
      "Learning rate decreased to: 1.000e-04\n",
      "\n",
      "\n",
      "Starting epoch: 2\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1711, Tr_loss_rg_mean: 0.1038, Tr_loss_cl_mean: 0.3730, Took 3.1232secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1344, Tr_loss_rg_mean: 0.0786, Tr_loss_cl_mean: 0.3016, Took 18.1918secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1322, Tr_loss_rg_mean: 0.0767, Tr_loss_cl_mean: 0.2988, Took 19.0819secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1316, Tr_loss_rg_mean: 0.0762, Tr_loss_cl_mean: 0.2979, Took 19.1905secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1310, Tr_loss_rg_mean: 0.0758, Tr_loss_cl_mean: 0.2963, Took 18.9452secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1305, Tr_loss_rg_mean: 0.0757, Tr_loss_cl_mean: 0.2949, Took 18.1384secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1273, Val_loss_rg_mean: 0.0797, Val_loss_cl_mean: 0.2702, Took 3.1964secs\n",
      "Iter: 0101, Val_loss_mean: 0.1282, Val_loss_rg_mean: 0.0752, Val_loss_cl_mean: 0.2870, Took 10.8238secs\n",
      "Iter: 0201, Val_loss_mean: 0.1288, Val_loss_rg_mean: 0.0752, Val_loss_cl_mean: 0.2896, Took 10.9257secs\n",
      "Iter: 0301, Val_loss_mean: 0.1285, Val_loss_rg_mean: 0.0750, Val_loss_cl_mean: 0.2891, Took 10.8081secs\n",
      "Iter: 0401, Val_loss_mean: 0.1285, Val_loss_rg_mean: 0.0750, Val_loss_cl_mean: 0.2892, Took 10.7829secs\n",
      "Iter: 0501, Val_loss_mean: 0.1286, Val_loss_rg_mean: 0.0751, Val_loss_cl_mean: 0.2892, Took 10.1081secs\n",
      "\n",
      "Epoch 2 ended\n",
      "Training:  1:45\n",
      "Validation:  1:01\n",
      "Loss decreased from 0.1606 to 0.1287\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_ming_20220224\n",
      "\n",
      "\n",
      "Starting epoch: 3\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1316, Tr_loss_rg_mean: 0.0766, Tr_loss_cl_mean: 0.2966, Took 3.3654secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1283, Tr_loss_rg_mean: 0.0750, Tr_loss_cl_mean: 0.2883, Took 18.1246secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1277, Tr_loss_rg_mean: 0.0743, Tr_loss_cl_mean: 0.2880, Took 19.0135secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1277, Tr_loss_rg_mean: 0.0742, Tr_loss_cl_mean: 0.2880, Took 19.2291secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1274, Tr_loss_rg_mean: 0.0742, Tr_loss_cl_mean: 0.2870, Took 19.3993secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1271, Tr_loss_rg_mean: 0.0742, Tr_loss_cl_mean: 0.2859, Took 18.2560secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1275, Val_loss_rg_mean: 0.0800, Val_loss_cl_mean: 0.2699, Took 3.1678secs\n",
      "Iter: 0101, Val_loss_mean: 0.1259, Val_loss_rg_mean: 0.0746, Val_loss_cl_mean: 0.2797, Took 10.4285secs\n",
      "Iter: 0201, Val_loss_mean: 0.1262, Val_loss_rg_mean: 0.0743, Val_loss_cl_mean: 0.2818, Took 10.6450secs\n",
      "Iter: 0301, Val_loss_mean: 0.1258, Val_loss_rg_mean: 0.0741, Val_loss_cl_mean: 0.2808, Took 10.8426secs\n",
      "Iter: 0401, Val_loss_mean: 0.1258, Val_loss_rg_mean: 0.0741, Val_loss_cl_mean: 0.2810, Took 10.8026secs\n",
      "Iter: 0501, Val_loss_mean: 0.1259, Val_loss_rg_mean: 0.0742, Val_loss_cl_mean: 0.2811, Took 10.6254secs\n",
      "\n",
      "Epoch 3 ended\n",
      "Training:  1:45\n",
      "Validation:  1:01\n",
      "Loss decreased from 0.1287 to 0.1260\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_ming_20220224\n",
      "Learning rate decreased to: 1.000e-05\n",
      "\n",
      "\n",
      "Starting epoch: 4\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1291, Tr_loss_rg_mean: 0.0719, Tr_loss_cl_mean: 0.3009, Took 3.1206secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1254, Tr_loss_rg_mean: 0.0740, Tr_loss_cl_mean: 0.2797, Took 17.9527secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1248, Tr_loss_rg_mean: 0.0732, Tr_loss_cl_mean: 0.2795, Took 18.1093secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1246, Tr_loss_rg_mean: 0.0730, Tr_loss_cl_mean: 0.2796, Took 17.9802secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1245, Tr_loss_rg_mean: 0.0730, Tr_loss_cl_mean: 0.2790, Took 18.0162secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1244, Tr_loss_rg_mean: 0.0730, Tr_loss_cl_mean: 0.2784, Took 17.8153secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1224, Val_loss_rg_mean: 0.0784, Val_loss_cl_mean: 0.2542, Took 2.9835secs\n",
      "Iter: 0101, Val_loss_mean: 0.1241, Val_loss_rg_mean: 0.0734, Val_loss_cl_mean: 0.2762, Took 10.5121secs\n",
      "Iter: 0201, Val_loss_mean: 0.1245, Val_loss_rg_mean: 0.0733, Val_loss_cl_mean: 0.2781, Took 9.9328secs\n",
      "Iter: 0301, Val_loss_mean: 0.1242, Val_loss_rg_mean: 0.0730, Val_loss_cl_mean: 0.2778, Took 9.9376secs\n",
      "Iter: 0401, Val_loss_mean: 0.1242, Val_loss_rg_mean: 0.0730, Val_loss_cl_mean: 0.2778, Took 9.9830secs\n",
      "Iter: 0501, Val_loss_mean: 0.1243, Val_loss_rg_mean: 0.0731, Val_loss_cl_mean: 0.2780, Took 9.4869secs\n",
      "\n",
      "Epoch 4 ended\n",
      "Training:  1:41\n",
      "Validation:  0:57\n",
      "Loss decreased from 0.1260 to 0.1244\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_ming_20220224\n",
      "\n",
      "\n",
      "Starting epoch: 5\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1239, Tr_loss_rg_mean: 0.0675, Tr_loss_cl_mean: 0.2928, Took 3.1753secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1243, Tr_loss_rg_mean: 0.0734, Tr_loss_cl_mean: 0.2771, Took 18.0374secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1240, Tr_loss_rg_mean: 0.0729, Tr_loss_cl_mean: 0.2775, Took 18.0437secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1241, Tr_loss_rg_mean: 0.0728, Tr_loss_cl_mean: 0.2782, Took 17.8071secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1239, Tr_loss_rg_mean: 0.0728, Tr_loss_cl_mean: 0.2773, Took 18.0988secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1239, Tr_loss_rg_mean: 0.0729, Tr_loss_cl_mean: 0.2768, Took 17.6642secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1238, Val_loss_rg_mean: 0.0789, Val_loss_cl_mean: 0.2588, Took 3.0194secs\n",
      "Iter: 0101, Val_loss_mean: 0.1238, Val_loss_rg_mean: 0.0735, Val_loss_cl_mean: 0.2747, Took 11.1010secs\n",
      "Iter: 0201, Val_loss_mean: 0.1242, Val_loss_rg_mean: 0.0734, Val_loss_cl_mean: 0.2768, Took 11.3632secs\n",
      "Iter: 0301, Val_loss_mean: 0.1240, Val_loss_rg_mean: 0.0732, Val_loss_cl_mean: 0.2764, Took 11.2828secs\n",
      "Iter: 0401, Val_loss_mean: 0.1240, Val_loss_rg_mean: 0.0731, Val_loss_cl_mean: 0.2766, Took 11.3706secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0501, Val_loss_mean: 0.1241, Val_loss_rg_mean: 0.0733, Val_loss_cl_mean: 0.2767, Took 10.7907secs\n",
      "\n",
      "Epoch 5 ended\n",
      "Training:  1:41\n",
      "Validation:  1:03\n",
      "Loss decreased from 0.1244 to 0.1242\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_ming_20220224\n",
      "Learning rate decreased to: 1.000e-06\n",
      "\n",
      "\n",
      "Starting epoch: 6\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1272, Tr_loss_rg_mean: 0.0742, Tr_loss_cl_mean: 0.2860, Took 3.3853secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1240, Tr_loss_rg_mean: 0.0734, Tr_loss_cl_mean: 0.2757, Took 18.3857secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1236, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2762, Took 18.7961secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1237, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2766, Took 18.6806secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1236, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2763, Took 19.2454secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2757, Took 18.4393secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1219, Val_loss_rg_mean: 0.0782, Val_loss_cl_mean: 0.2531, Took 3.2494secs\n",
      "Iter: 0101, Val_loss_mean: 0.1234, Val_loss_rg_mean: 0.0731, Val_loss_cl_mean: 0.2743, Took 10.9233secs\n",
      "Iter: 0201, Val_loss_mean: 0.1239, Val_loss_rg_mean: 0.0730, Val_loss_cl_mean: 0.2765, Took 11.0308secs\n",
      "Iter: 0301, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2762, Took 11.0420secs\n",
      "Iter: 0401, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2763, Took 11.0573secs\n",
      "Iter: 0501, Val_loss_mean: 0.1238, Val_loss_rg_mean: 0.0729, Val_loss_cl_mean: 0.2764, Took 10.9753secs\n",
      "\n",
      "Epoch 6 ended\n",
      "Training:  1:45\n",
      "Validation:  1:02\n",
      "Loss decreased from 0.1242 to 0.1239\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_ming_20220224\n",
      "\n",
      "\n",
      "Starting epoch: 7\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1271, Tr_loss_rg_mean: 0.0705, Tr_loss_cl_mean: 0.2967, Took 3.1407secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1242, Tr_loss_rg_mean: 0.0735, Tr_loss_cl_mean: 0.2763, Took 18.3019secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1236, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2763, Took 18.7718secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2764, Took 19.0098secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2761, Took 19.2083secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2755, Took 18.5882secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1240, Val_loss_rg_mean: 0.0768, Val_loss_cl_mean: 0.2657, Took 2.9239secs\n",
      "Iter: 0101, Val_loss_mean: 0.1235, Val_loss_rg_mean: 0.0732, Val_loss_cl_mean: 0.2745, Took 10.5297secs\n",
      "Iter: 0201, Val_loss_mean: 0.1239, Val_loss_rg_mean: 0.0730, Val_loss_cl_mean: 0.2765, Took 10.6270secs\n",
      "Iter: 0301, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0727, Val_loss_cl_mean: 0.2761, Took 10.4915secs\n",
      "Iter: 0401, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0727, Val_loss_cl_mean: 0.2761, Took 10.8503secs\n",
      "Iter: 0501, Val_loss_mean: 0.1237, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2764, Took 10.6611secs\n",
      "\n",
      "Epoch 7 ended\n",
      "Training:  1:45\n",
      "Validation:  1:00\n",
      "Loss decreased from 0.1239 to 0.1238\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_ming_20220224\n",
      "Learning rate decreased to: 1.000e-07\n",
      "\n",
      "\n",
      "Starting epoch: 8\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1233, Tr_loss_rg_mean: 0.0672, Tr_loss_cl_mean: 0.2915, Took 3.3749secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1238, Tr_loss_rg_mean: 0.0732, Tr_loss_cl_mean: 0.2756, Took 18.1169secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2758, Took 18.1615secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0725, Tr_loss_cl_mean: 0.2764, Took 17.7927secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0725, Tr_loss_cl_mean: 0.2759, Took 18.1671secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1233, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2753, Took 17.9296secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1234, Val_loss_rg_mean: 0.0785, Val_loss_cl_mean: 0.2583, Took 2.9776secs\n",
      "Iter: 0101, Val_loss_mean: 0.1235, Val_loss_rg_mean: 0.0731, Val_loss_cl_mean: 0.2746, Took 11.0137secs\n",
      "Iter: 0201, Val_loss_mean: 0.1238, Val_loss_rg_mean: 0.0730, Val_loss_cl_mean: 0.2765, Took 11.3814secs\n",
      "Iter: 0301, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2761, Took 11.3030secs\n",
      "Iter: 0401, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0727, Val_loss_cl_mean: 0.2761, Took 11.2006secs\n",
      "Iter: 0501, Val_loss_mean: 0.1237, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2764, Took 10.8354secs\n",
      "\n",
      "Epoch 8 ended\n",
      "Training:  1:41\n",
      "Validation:  1:03\n",
      "Loss decreased from 0.1238 to 0.1238\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_ming_20220224\n",
      "\n",
      "\n",
      "Starting epoch: 9\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1268, Tr_loss_rg_mean: 0.0739, Tr_loss_cl_mean: 0.2857, Took 3.1330secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1239, Tr_loss_rg_mean: 0.0733, Tr_loss_cl_mean: 0.2759, Took 17.9983secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2757, Took 17.8909secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2763, Took 18.3131secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2758, Took 18.2706secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1233, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2753, Took 17.8678secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1240, Val_loss_rg_mean: 0.0768, Val_loss_cl_mean: 0.2655, Took 3.1627secs\n",
      "Iter: 0101, Val_loss_mean: 0.1235, Val_loss_rg_mean: 0.0729, Val_loss_cl_mean: 0.2753, Took 11.2063secs\n",
      "Iter: 0201, Val_loss_mean: 0.1239, Val_loss_rg_mean: 0.0729, Val_loss_cl_mean: 0.2769, Took 11.3750secs\n",
      "Iter: 0301, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2761, Took 11.3968secs\n",
      "Iter: 0401, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0727, Val_loss_cl_mean: 0.2760, Took 11.4271secs\n",
      "Iter: 0501, Val_loss_mean: 0.1238, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2765, Took 10.1595secs\n",
      "\n",
      "Epoch 9 ended\n",
      "Training:  1:42\n",
      "Validation:  1:02\n",
      "Loss decreased from 0.1238 to 0.1238\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_ming_20220224\n",
      "Learning rate decreased to: 1.000e-08\n",
      "\n",
      "\n",
      "Starting epoch: 10\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1270, Tr_loss_rg_mean: 0.0705, Tr_loss_cl_mean: 0.2968, Took 3.3685secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1240, Tr_loss_rg_mean: 0.0734, Tr_loss_cl_mean: 0.2759, Took 18.1949secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2760, Took 17.8767secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2762, Took 18.1201secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2758, Took 17.8842secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1233, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2754, Took 17.8464secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1235, Val_loss_rg_mean: 0.0785, Val_loss_cl_mean: 0.2584, Took 2.9533secs\n",
      "Iter: 0101, Val_loss_mean: 0.1234, Val_loss_rg_mean: 0.0730, Val_loss_cl_mean: 0.2747, Took 10.9174secs\n",
      "Iter: 0201, Val_loss_mean: 0.1239, Val_loss_rg_mean: 0.0729, Val_loss_cl_mean: 0.2769, Took 11.1493secs\n",
      "Iter: 0301, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2760, Took 11.0573secs\n",
      "Iter: 0401, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2760, Took 11.0794secs\n",
      "Iter: 0501, Val_loss_mean: 0.1237, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2765, Took 10.1623secs\n",
      "\n",
      "Epoch 10 ended\n",
      "Training:  1:41\n",
      "Validation:  1:01\n",
      "Loss decreased from 0.1238 to 0.1238\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_ming_20220224\n",
      "\n",
      "\n",
      "Starting epoch: 11\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1233, Tr_loss_rg_mean: 0.0672, Tr_loss_cl_mean: 0.2917, Took 3.1608secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1237, Tr_loss_rg_mean: 0.0731, Tr_loss_cl_mean: 0.2753, Took 18.0933secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2756, Took 18.7072secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0725, Tr_loss_cl_mean: 0.2765, Took 18.7069secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1233, Tr_loss_rg_mean: 0.0725, Tr_loss_cl_mean: 0.2757, Took 18.4915secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1233, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2753, Took 18.2682secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1240, Val_loss_rg_mean: 0.0768, Val_loss_cl_mean: 0.2655, Took 3.1682secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0101, Val_loss_mean: 0.1235, Val_loss_rg_mean: 0.0730, Val_loss_cl_mean: 0.2749, Took 10.7699secs\n",
      "Iter: 0201, Val_loss_mean: 0.1238, Val_loss_rg_mean: 0.0730, Val_loss_cl_mean: 0.2764, Took 10.6784secs\n",
      "Iter: 0301, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2762, Took 10.7143secs\n",
      "Iter: 0401, Val_loss_mean: 0.1235, Val_loss_rg_mean: 0.0727, Val_loss_cl_mean: 0.2760, Took 10.7344secs\n",
      "Iter: 0501, Val_loss_mean: 0.1237, Val_loss_rg_mean: 0.0729, Val_loss_cl_mean: 0.2764, Took 9.6659secs\n",
      "\n",
      "Epoch 11 ended\n",
      "Training:  1:43\n",
      "Validation:  1:00\n",
      "Loss decreased from 0.1238 to 0.1238\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_ming_20220224\n",
      "Learning rate decreased to: 1.000e-09\n",
      "\n",
      "\n",
      "Starting epoch: 12\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1270, Tr_loss_rg_mean: 0.0705, Tr_loss_cl_mean: 0.2968, Took 3.3840secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1241, Tr_loss_rg_mean: 0.0734, Tr_loss_cl_mean: 0.2761, Took 18.2638secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2761, Took 18.2852secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0725, Tr_loss_cl_mean: 0.2761, Took 18.6225secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2759, Took 18.8576secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1233, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2753, Took 18.2361secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1234, Val_loss_rg_mean: 0.0785, Val_loss_cl_mean: 0.2584, Took 2.9622secs\n",
      "Iter: 0101, Val_loss_mean: 0.1235, Val_loss_rg_mean: 0.0730, Val_loss_cl_mean: 0.2748, Took 10.6211secs\n",
      "Iter: 0201, Val_loss_mean: 0.1239, Val_loss_rg_mean: 0.0729, Val_loss_cl_mean: 0.2766, Took 11.0266secs\n",
      "Iter: 0301, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2760, Took 10.9799secs\n",
      "Iter: 0401, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0727, Val_loss_cl_mean: 0.2760, Took 10.7190secs\n",
      "Iter: 0501, Val_loss_mean: 0.1237, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2764, Took 10.7660secs\n",
      "\n",
      "Epoch 12 ended\n",
      "Training:  1:43\n",
      "Validation:  1:01\n",
      "Loss decreased from 0.1238 to 0.1238\n",
      "Checkpointing and saving predictions to:\n",
      "results/test_ming_20220224\n",
      "\n",
      "\n",
      "Starting epoch: 13\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1270, Tr_loss_rg_mean: 0.0705, Tr_loss_cl_mean: 0.2968, Took 3.1641secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1240, Tr_loss_rg_mean: 0.0734, Tr_loss_cl_mean: 0.2760, Took 17.9445secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1236, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2761, Took 17.9481secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2761, Took 17.7931secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2759, Took 17.9062secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1233, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2753, Took 18.2994secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1234, Val_loss_rg_mean: 0.0785, Val_loss_cl_mean: 0.2584, Took 3.2221secs\n",
      "Iter: 0101, Val_loss_mean: 0.1234, Val_loss_rg_mean: 0.0730, Val_loss_cl_mean: 0.2747, Took 10.9026secs\n",
      "Iter: 0201, Val_loss_mean: 0.1239, Val_loss_rg_mean: 0.0729, Val_loss_cl_mean: 0.2769, Took 11.3803secs\n",
      "Iter: 0301, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2760, Took 11.2670secs\n",
      "Iter: 0401, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0727, Val_loss_cl_mean: 0.2760, Took 11.2129secs\n",
      "Iter: 0501, Val_loss_mean: 0.1237, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2765, Took 10.7668secs\n",
      "\n",
      "Epoch 13 ended\n",
      "Training:  1:41\n",
      "Validation:  1:03\n",
      "Loss didnt decrease from 0.1238\n",
      "Learning rate decreased to: 1.000e-10\n",
      "\n",
      "\n",
      "Starting epoch: 14\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1270, Tr_loss_rg_mean: 0.0705, Tr_loss_cl_mean: 0.2968, Took 3.3100secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1240, Tr_loss_rg_mean: 0.0734, Tr_loss_cl_mean: 0.2757, Took 18.8782secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2759, Took 19.0787secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2760, Took 19.0188secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2759, Took 18.9540secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1233, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2754, Took 18.4259secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1217, Val_loss_rg_mean: 0.0781, Val_loss_cl_mean: 0.2523, Took 2.9420secs\n",
      "Iter: 0101, Val_loss_mean: 0.1234, Val_loss_rg_mean: 0.0730, Val_loss_cl_mean: 0.2743, Took 10.9064secs\n",
      "Iter: 0201, Val_loss_mean: 0.1238, Val_loss_rg_mean: 0.0729, Val_loss_cl_mean: 0.2762, Took 11.2299secs\n",
      "Iter: 0301, Val_loss_mean: 0.1235, Val_loss_rg_mean: 0.0727, Val_loss_cl_mean: 0.2758, Took 11.1256secs\n",
      "Iter: 0401, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0727, Val_loss_cl_mean: 0.2762, Took 11.0988secs\n",
      "Iter: 0501, Val_loss_mean: 0.1237, Val_loss_rg_mean: 0.0729, Val_loss_cl_mean: 0.2762, Took 11.1529secs\n",
      "\n",
      "Epoch 14 ended\n",
      "Training:  1:46\n",
      "Validation:  1:03\n",
      "Loss didnt decrease from 0.1238\n",
      "\n",
      "\n",
      "Starting epoch: 15\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1270, Tr_loss_rg_mean: 0.0705, Tr_loss_cl_mean: 0.2968, Took 3.1351secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1240, Tr_loss_rg_mean: 0.0734, Tr_loss_cl_mean: 0.2759, Took 18.0929secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2760, Took 18.6208secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2762, Took 18.7215secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2758, Took 18.6889secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1233, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2753, Took 18.5774secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1234, Val_loss_rg_mean: 0.0785, Val_loss_cl_mean: 0.2584, Took 2.9788secs\n",
      "Iter: 0101, Val_loss_mean: 0.1235, Val_loss_rg_mean: 0.0731, Val_loss_cl_mean: 0.2746, Took 11.1816secs\n",
      "Iter: 0201, Val_loss_mean: 0.1238, Val_loss_rg_mean: 0.0729, Val_loss_cl_mean: 0.2765, Took 11.2654secs\n",
      "Iter: 0301, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2760, Took 11.4098secs\n",
      "Iter: 0401, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0727, Val_loss_cl_mean: 0.2761, Took 11.3161secs\n",
      "Iter: 0501, Val_loss_mean: 0.1237, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2764, Took 11.5595secs\n",
      "\n",
      "Epoch 15 ended\n",
      "Training:  1:44\n",
      "Validation:  1:04\n",
      "Loss didnt decrease from 0.1238\n",
      "Learning rate decreased to: 1.000e-11\n",
      "\n",
      "\n",
      "Starting epoch: 16\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1270, Tr_loss_rg_mean: 0.0705, Tr_loss_cl_mean: 0.2968, Took 3.5074secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1240, Tr_loss_rg_mean: 0.0734, Tr_loss_cl_mean: 0.2757, Took 19.4779secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2759, Took 20.2817secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2760, Took 20.2738secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2759, Took 19.6932secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1233, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2754, Took 18.7281secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1234, Val_loss_rg_mean: 0.0785, Val_loss_cl_mean: 0.2584, Took 3.1587secs\n",
      "Iter: 0101, Val_loss_mean: 0.1235, Val_loss_rg_mean: 0.0731, Val_loss_cl_mean: 0.2746, Took 10.9158secs\n",
      "Iter: 0201, Val_loss_mean: 0.1238, Val_loss_rg_mean: 0.0729, Val_loss_cl_mean: 0.2765, Took 11.3464secs\n",
      "Iter: 0301, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2760, Took 11.4067secs\n",
      "Iter: 0401, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0727, Val_loss_cl_mean: 0.2761, Took 11.3353secs\n",
      "Iter: 0501, Val_loss_mean: 0.1237, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2764, Took 11.0546secs\n",
      "\n",
      "Epoch 16 ended\n",
      "Training:  1:50\n",
      "Validation:  1:03\n",
      "Loss didnt decrease from 0.1238\n",
      "\n",
      "\n",
      "Starting epoch: 17\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1270, Tr_loss_rg_mean: 0.0705, Tr_loss_cl_mean: 0.2968, Took 3.1644secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1240, Tr_loss_rg_mean: 0.0734, Tr_loss_cl_mean: 0.2759, Took 18.5317secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2760, Took 19.1903secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2762, Took 19.1249secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2758, Took 19.5562secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1233, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2753, Took 18.8066secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1234, Val_loss_rg_mean: 0.0785, Val_loss_cl_mean: 0.2584, Took 2.9635secs\n",
      "Iter: 0101, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0730, Val_loss_cl_mean: 0.2754, Took 10.8453secs\n",
      "Iter: 0201, Val_loss_mean: 0.1239, Val_loss_rg_mean: 0.0729, Val_loss_cl_mean: 0.2769, Took 11.0873secs\n",
      "Iter: 0301, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2761, Took 10.9806secs\n",
      "Iter: 0401, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0727, Val_loss_cl_mean: 0.2760, Took 10.9541secs\n",
      "Iter: 0501, Val_loss_mean: 0.1238, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2765, Took 10.3336secs\n",
      "\n",
      "Epoch 17 ended\n",
      "Training:  1:46\n",
      "Validation:  1:01\n",
      "Loss didnt decrease from 0.1238\n",
      "Learning rate decreased to: 1.000e-12\n",
      "\n",
      "\n",
      "Starting epoch: 18\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1268, Tr_loss_rg_mean: 0.0739, Tr_loss_cl_mean: 0.2857, Took 3.1704secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1239, Tr_loss_rg_mean: 0.0733, Tr_loss_cl_mean: 0.2757, Took 18.4677secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2760, Took 18.9411secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2762, Took 18.9374secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2759, Took 20.2233secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1233, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2754, Took 19.4287secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1234, Val_loss_rg_mean: 0.0785, Val_loss_cl_mean: 0.2584, Took 3.3262secs\n",
      "Iter: 0101, Val_loss_mean: 0.1234, Val_loss_rg_mean: 0.0730, Val_loss_cl_mean: 0.2747, Took 11.4754secs\n",
      "Iter: 0201, Val_loss_mean: 0.1239, Val_loss_rg_mean: 0.0729, Val_loss_cl_mean: 0.2769, Took 11.8295secs\n",
      "Iter: 0301, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2760, Took 12.0650secs\n",
      "Iter: 0401, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0727, Val_loss_cl_mean: 0.2760, Took 11.5009secs\n",
      "Iter: 0501, Val_loss_mean: 0.1237, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2765, Took 11.0604secs\n",
      "\n",
      "Epoch 18 ended\n",
      "Training:  1:48\n",
      "Validation:  1:05\n",
      "Loss didnt decrease from 0.1238\n",
      "\n",
      "\n",
      "Starting epoch: 19\n",
      "Training...\n",
      "Iter: 0001, Tr_loss_mean: 0.1270, Tr_loss_rg_mean: 0.0705, Tr_loss_cl_mean: 0.2968, Took 3.3901secs\n",
      "Iter: 0101, Tr_loss_mean: 0.1241, Tr_loss_rg_mean: 0.0734, Tr_loss_cl_mean: 0.2761, Took 18.0818secs\n",
      "Iter: 0201, Tr_loss_mean: 0.1235, Tr_loss_rg_mean: 0.0727, Tr_loss_cl_mean: 0.2761, Took 18.1742secs\n",
      "Iter: 0301, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0725, Tr_loss_cl_mean: 0.2761, Took 18.6877secs\n",
      "Iter: 0401, Tr_loss_mean: 0.1234, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2759, Took 19.0402secs\n",
      "Iter: 0501, Tr_loss_mean: 0.1233, Tr_loss_rg_mean: 0.0726, Tr_loss_cl_mean: 0.2753, Took 18.7110secs\n",
      "\n",
      "Validation...\n",
      "Iter: 0001, Val_loss_mean: 0.1234, Val_loss_rg_mean: 0.0785, Val_loss_cl_mean: 0.2584, Took 2.9546secs\n",
      "Iter: 0101, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0730, Val_loss_cl_mean: 0.2753, Took 10.6961secs\n",
      "Iter: 0201, Val_loss_mean: 0.1238, Val_loss_rg_mean: 0.0729, Val_loss_cl_mean: 0.2765, Took 10.9388secs\n",
      "Iter: 0301, Val_loss_mean: 0.1236, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2761, Took 10.9706secs\n",
      "Iter: 0401, Val_loss_mean: 0.1235, Val_loss_rg_mean: 0.0727, Val_loss_cl_mean: 0.2760, Took 10.9192secs\n",
      "Iter: 0501, Val_loss_mean: 0.1237, Val_loss_rg_mean: 0.0728, Val_loss_cl_mean: 0.2764, Took 10.4870secs\n",
      "\n",
      "Epoch 19 ended\n",
      "Training:  1:44\n",
      "Validation:  1:01\n",
      "Loss didnt decrease from 0.1238\n",
      "Learning rate decreased to: 1.000e-13\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "curr_loss = 1e5\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    print(f'\\n\\nStarting epoch: {e}')\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Batchwise losses\n",
    "    training_loss = []\n",
    "    training_loss_regress = []\n",
    "    training_loss_class = []\n",
    "    val_loss = []\n",
    "    val_loss_regress = []\n",
    "    val_loss_class = []\n",
    "\n",
    "    # Train\n",
    "    print('Training...')\n",
    "    start = time.time()\n",
    "    for i, (graph_data_tr, targets_tr) in enumerate(get_batch(data_gen_train.generator())):\n",
    "        losses_tr_rg, losses_tr_cl, losses_tr = train_step(graph_data_tr, targets_tr)\n",
    "\n",
    "        training_loss.append(losses_tr.numpy())\n",
    "        training_loss_regress.append(losses_tr_rg.numpy())\n",
    "        training_loss_class.append(losses_tr_cl.numpy())\n",
    "\n",
    "        if not (i-1)%log_freq:\n",
    "            end = time.time()\n",
    "            print(f'Iter: {i:04d}, ', end='')\n",
    "            print(f'Tr_loss_mean: {np.mean(training_loss):.4f}, ', end='')\n",
    "            print(f'Tr_loss_rg_mean: {np.mean(training_loss_regress):.4f}, ', end='') \n",
    "            print(f'Tr_loss_cl_mean: {np.mean(training_loss_class):.4f}, ', end='') \n",
    "            print(f'Took {end-start:.4f}secs')\n",
    "            start = time.time()\n",
    "                  \n",
    "    training_loss_epoch.append(training_loss)\n",
    "    training_loss_regress_epoch.append(training_loss_regress)\n",
    "    training_loss_class_epoch.append(training_loss_class)\n",
    "    training_end = time.time()\n",
    "\n",
    "    # validate\n",
    "    print('\\nValidation...')\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    all_energies = []\n",
    "    start = time.time()\n",
    "    for i, (graph_data_val, targets_val) in enumerate(get_batch(data_gen_val.generator())):\n",
    "        losses_val_rg, losses_val_cl, losses_val, regress_vals, class_vals = val_step(graph_data_val, targets_val)\n",
    "\n",
    "        targets_val = targets_val.numpy()\n",
    "        regress_vals = regress_vals.numpy()\n",
    "        class_vals = class_vals.numpy()\n",
    "\n",
    "        targets_val[:,0] = 10**targets_val[:,0]\n",
    "        regress_vals = 10**regress_vals\n",
    "        class_vals =  tf.math.sigmoid(class_vals)\n",
    "\n",
    "        output_vals = np.hstack([regress_vals, class_vals])\n",
    "\n",
    "        val_loss.append(losses_val.numpy())\n",
    "        val_loss_regress.append(losses_val_rg.numpy())\n",
    "        val_loss_class.append(losses_val_cl.numpy())\n",
    "\n",
    "        all_targets.append(targets_val)\n",
    "        all_outputs.append(output_vals)\n",
    "\n",
    "        if not (i-1)%log_freq:\n",
    "            end = time.time()\n",
    "            print(f'Iter: {i:04d}, ', end='')\n",
    "            print(f'Val_loss_mean: {np.mean(val_loss):.4f}, ', end='')\n",
    "            print(f'Val_loss_rg_mean: {np.mean(val_loss_regress):.4f}, ', end='') \n",
    "            print(f'Val_loss_cl_mean: {np.mean(val_loss_class):.4f}, ', end='') \n",
    "            print(f'Took {end-start:.4f}secs')\n",
    "            start = time.time()\n",
    "\n",
    "    epoch_end = time.time()\n",
    "\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "\n",
    "    val_loss_epoch.append(val_loss)\n",
    "    val_loss_regress_epoch.append(val_loss_regress)\n",
    "    val_loss_class_epoch.append(val_loss_class)\n",
    "\n",
    "    \n",
    "    # Book keeping\n",
    "    val_mins = int((epoch_end - training_end)/60)\n",
    "    val_secs = int((epoch_end - training_end)%60)\n",
    "    training_mins = int((training_end - epoch_start)/60)\n",
    "    training_secs = int((training_end - epoch_start)%60)\n",
    "    print(f'\\nEpoch {e} ended')\n",
    "    print(f'Training: {training_mins:2d}:{training_secs:02d}')\n",
    "    print(f'Validation: {val_mins:2d}:{val_secs:02d}')\n",
    "    \n",
    "    \n",
    "    # Save losses\n",
    "    np.savez(save_dir+'/losses', \n",
    "            training=training_loss_epoch, validation=val_loss_epoch,\n",
    "            training_regress=training_loss_regress_epoch, validation_regress=val_loss_regress_epoch,\n",
    "            training_class=training_loss_class_epoch, validation_class=val_loss_class_epoch,\n",
    "            )\n",
    "\n",
    "    \n",
    "    # Checkpoint if validation loss improved\n",
    "    if np.mean(val_loss)<curr_loss:\n",
    "        print(f'Loss decreased from {curr_loss:.4f} to {np.mean(val_loss):.4f}')\n",
    "        print(f'Checkpointing and saving predictions to:\\n{save_dir}')\n",
    "        curr_loss = np.mean(val_loss)\n",
    "        np.savez(save_dir+'/predictions', \n",
    "                targets=all_targets, \n",
    "                outputs=all_outputs,\n",
    "                energies=all_energies)\n",
    "        checkpoint.save(checkpoint_prefix)\n",
    "    else: \n",
    "        print(f'Loss didnt decrease from {curr_loss:.4f}')\n",
    "    \n",
    "    \n",
    "    # Decrease learning rate every few epochs\n",
    "    if not (e+1)%2:   #%20:\n",
    "        optimizer.learning_rate = optimizer.learning_rate/10\n",
    "        print(f'Learning rate decreased to: {optimizer.learning_rate.value():.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphsTuple(nodes=<tf.Tensor: shape=(8905, 11), dtype=float32, numpy=\n",
       "array([[ 2.0124032 ,  0.25      ,  2.9625819 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.2589308 ,  0.25      ,  2.9625587 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.87126935,  0.25      ,  2.9625828 , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-1.6004568 ,  0.03571429,  0.55865055, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.7220358 ,  0.03571429,  0.56489515, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.8561385 ,  0.03571429,  0.5742621 , ...,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)>, edges=<tf.Tensor: shape=(28045, 10), dtype=float32, numpy=\n",
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, receivers=<tf.Tensor: shape=(28045,), dtype=int32, numpy=array([   1,    2,    3, ..., 8878, 8879, 8880], dtype=int32)>, senders=<tf.Tensor: shape=(28045,), dtype=int32, numpy=array([   0,    0,    0, ..., 8878, 8879, 8880], dtype=int32)>, globals=<tf.Tensor: shape=(129, 1), dtype=float32, numpy=\n",
       "array([[ 2.7204571e+00],\n",
       "       [ 3.0322423e+00],\n",
       "       [ 6.8783188e-01],\n",
       "       [ 6.4269626e-01],\n",
       "       [ 1.5564526e+00],\n",
       "       [ 2.8667731e+00],\n",
       "       [ 1.8595273e+00],\n",
       "       [ 2.5294609e+00],\n",
       "       [-6.3292217e-01],\n",
       "       [ 1.8067677e+00],\n",
       "       [ 1.1134659e+00],\n",
       "       [ 2.6055484e+00],\n",
       "       [ 1.0120918e-01],\n",
       "       [ 1.5354551e+00],\n",
       "       [ 7.8146756e-01],\n",
       "       [ 2.7852821e+00],\n",
       "       [ 6.9904786e-01],\n",
       "       [ 3.0430207e+00],\n",
       "       [ 3.1318219e+00],\n",
       "       [ 1.1278143e+00],\n",
       "       [-4.2054877e-03],\n",
       "       [ 6.7391497e-01],\n",
       "       [ 2.0255287e+00],\n",
       "       [ 2.9743889e-01],\n",
       "       [-5.5708545e-01],\n",
       "       [-6.6880143e-01],\n",
       "       [ 1.4406241e-01],\n",
       "       [ 3.5875642e-01],\n",
       "       [ 8.9585048e-01],\n",
       "       [-1.6149507e+00],\n",
       "       [ 1.7525545e+00],\n",
       "       [ 2.8125994e+00],\n",
       "       [-1.1516737e-01],\n",
       "       [ 7.0495516e-01],\n",
       "       [ 2.1855206e+00],\n",
       "       [ 2.9745305e+00],\n",
       "       [ 1.5569082e-01],\n",
       "       [ 7.8584540e-01],\n",
       "       [ 1.6380128e+00],\n",
       "       [ 2.1896033e+00],\n",
       "       [-3.1290230e-01],\n",
       "       [ 1.5011604e+00],\n",
       "       [ 9.7413903e-01],\n",
       "       [ 2.1154006e+00],\n",
       "       [ 2.9133000e+00],\n",
       "       [ 9.4274133e-01],\n",
       "       [ 1.6640000e+00],\n",
       "       [ 4.6628240e-02],\n",
       "       [ 7.4751824e-01],\n",
       "       [ 4.0607476e-01],\n",
       "       [ 8.1871045e-01],\n",
       "       [ 1.3812294e+00],\n",
       "       [ 2.7838111e+00],\n",
       "       [ 3.1582019e+00],\n",
       "       [ 2.9209468e+00],\n",
       "       [ 8.2645011e-01],\n",
       "       [ 1.0357765e+00],\n",
       "       [ 1.9119469e+00],\n",
       "       [ 2.9689620e+00],\n",
       "       [ 2.2435360e+00],\n",
       "       [ 1.2940052e+00],\n",
       "       [-1.4489521e-01],\n",
       "       [ 4.1937536e-01],\n",
       "       [ 6.4381731e-01],\n",
       "       [ 2.4605207e+00],\n",
       "       [ 2.7592344e+00],\n",
       "       [ 2.1702683e-01],\n",
       "       [-2.7915201e-01],\n",
       "       [ 2.1525683e+00],\n",
       "       [ 9.9089688e-01],\n",
       "       [-3.6600459e-02],\n",
       "       [ 1.0869642e+00],\n",
       "       [ 3.1323710e-01],\n",
       "       [-1.4112868e+00],\n",
       "       [ 2.3151295e+00],\n",
       "       [ 1.7912765e+00],\n",
       "       [ 1.7631333e+00],\n",
       "       [ 4.6374011e-01],\n",
       "       [ 1.5983963e+00],\n",
       "       [-4.3533492e-01],\n",
       "       [ 2.5069909e+00],\n",
       "       [ 1.5538769e+00],\n",
       "       [ 6.0927235e-02],\n",
       "       [ 5.1259446e-01],\n",
       "       [ 6.0755771e-01],\n",
       "       [ 2.2171991e+00],\n",
       "       [ 3.1784682e+00],\n",
       "       [-3.5340247e-01],\n",
       "       [ 7.5238174e-01],\n",
       "       [ 1.6684512e+00],\n",
       "       [-4.7183018e-03],\n",
       "       [ 1.0764369e+00],\n",
       "       [ 2.4168410e+00],\n",
       "       [-1.3281529e-01],\n",
       "       [-1.9466166e-01],\n",
       "       [ 1.3026025e+00],\n",
       "       [ 2.0063589e+00],\n",
       "       [ 2.2808056e+00],\n",
       "       [ 4.1298518e-01],\n",
       "       [ 4.1202715e-01],\n",
       "       [ 3.4438622e-01],\n",
       "       [ 2.6919153e+00],\n",
       "       [-2.6528025e-01],\n",
       "       [ 1.3201264e+00],\n",
       "       [-7.9646742e-01],\n",
       "       [ 1.2081247e-03],\n",
       "       [ 1.5584366e+00],\n",
       "       [ 1.1905677e+00],\n",
       "       [-2.3133636e-02],\n",
       "       [ 1.0559707e+00],\n",
       "       [ 2.1030092e+00],\n",
       "       [-4.0867725e-01],\n",
       "       [-3.2275581e-01],\n",
       "       [ 6.8939775e-02],\n",
       "       [ 8.6327004e-01],\n",
       "       [-1.3351470e-01],\n",
       "       [ 1.4077567e+00],\n",
       "       [ 8.0207598e-01],\n",
       "       [-8.5340878e-03],\n",
       "       [ 2.6348549e-01],\n",
       "       [ 6.3630641e-01],\n",
       "       [ 2.7147622e+00],\n",
       "       [ 1.1701272e+00],\n",
       "       [-1.3297768e-03],\n",
       "       [ 2.8811424e+00],\n",
       "       [ 2.9327580e-01],\n",
       "       [ 1.1609617e+00],\n",
       "       [ 1.2282019e+00],\n",
       "       [ 1.4343916e-01]], dtype=float32)>, n_node=<tf.Tensor: shape=(129,), dtype=int64, numpy=\n",
       "array([ 92, 282,  48,  24,  61, 226,  96, 136,  10,  60,  32, 164,  16,\n",
       "        12,  50, 240,  51,  91,  74,  38,   2,  29, 148,  26,  11,   7,\n",
       "         9,   7,  63,  10, 135, 125,   5,  23,  69,  74,  34,  23,  84,\n",
       "       163,   9,  90,  98,  18, 137,  60, 113,   5,  20,  15,  22,  20,\n",
       "        96, 281, 460,  71,  17,  88, 451, 190,  15,  14,  24,  30,  25,\n",
       "       258,  17,  11, 155,  29,  11,  22,  26,   3, 174,  91,  85,  49,\n",
       "         9,  11, 227,  38,  16,  45,  38,  37, 361,  11,  54, 179,  22,\n",
       "        35,  99,  14,   9,  67, 113, 112,  19,  48,  17, 152,  12,   7,\n",
       "         3,   9,  36,  41,   9,   1,  23,   6,   7,   8,  27,   5,  48,\n",
       "        56,  13,  10,  52, 248,  22,   8, 240,   1,  47,  54,  29])>, n_edge=<tf.Tensor: shape=(129,), dtype=int64, numpy=\n",
       "array([ 380,  965,  115,   57,  206,  768,  288,  383,   15,  200,   65,\n",
       "        616,   44,   37,  147,  808,  155,  263,  181,  113,    6,   58,\n",
       "        467,   68,   28,   18,   26,   17,  172,   18,  411,  496,   13,\n",
       "         71,  164,  306,   86,   48,  261,  536,   23,  219,  247,   57,\n",
       "        571,  144,  352,   15,   70,   31,   74,   49,  372, 1036, 1534,\n",
       "        208,   48,  275, 1721,  579,   40,   29,   55,   82,   87,  913,\n",
       "         42,   31,  500,   66,   28,   51,   55,    8,  575,  198,  243,\n",
       "        124,   27,   25,  709,   95,   40,  111,   88,  146, 1232,   27,\n",
       "        118,  533,   47,   76,  253,   23,   24,  200,  319,  337,   47,\n",
       "         98,   43,  528,   31,   21,    6,   17,   83,  135,   21,    5,\n",
       "         77,   14,   12,   22,   59,   15,  116,  144,   30,   30,  160,\n",
       "        880,   53,   16,  789,    5,  126,  147,   57])>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(129, 2), dtype=float32, numpy=\n",
       "array([[ 2.755323  ,  1.        ],\n",
       "       [ 3.0317209 ,  0.        ],\n",
       "       [ 0.8071258 ,  1.        ],\n",
       "       [ 0.44958335,  1.        ],\n",
       "       [ 1.6490003 ,  1.        ],\n",
       "       [ 2.8600674 ,  0.        ],\n",
       "       [ 1.84583   ,  0.        ],\n",
       "       [ 2.3885376 ,  1.        ],\n",
       "       [-0.6348547 ,  0.        ],\n",
       "       [ 1.8926154 ,  1.        ],\n",
       "       [ 1.1382633 ,  1.        ],\n",
       "       [ 2.632147  ,  1.        ],\n",
       "       [ 0.23887107,  1.        ],\n",
       "       [ 1.5309668 ,  1.        ],\n",
       "       [ 0.7487196 ,  1.        ],\n",
       "       [ 2.7842493 ,  0.        ],\n",
       "       [ 0.6117757 ,  1.        ],\n",
       "       [ 2.8833387 ,  1.        ],\n",
       "       [ 3.156846  ,  1.        ],\n",
       "       [ 1.1219668 ,  0.        ],\n",
       "       [-1.6813831 ,  1.        ],\n",
       "       [ 0.9821644 ,  1.        ],\n",
       "       [ 2.0277817 ,  0.        ],\n",
       "       [ 0.294213  ,  1.        ],\n",
       "       [-0.7414676 ,  1.        ],\n",
       "       [-0.32059962,  1.        ],\n",
       "       [ 0.01045107,  1.        ],\n",
       "       [ 0.4159835 ,  1.        ],\n",
       "       [ 0.8553886 ,  0.        ],\n",
       "       [-0.6322687 ,  1.        ],\n",
       "       [ 1.7781004 ,  1.        ],\n",
       "       [ 2.8605118 ,  1.        ],\n",
       "       [-0.14294383,  1.        ],\n",
       "       [ 0.7553091 ,  0.        ],\n",
       "       [ 1.0843163 ,  1.        ],\n",
       "       [ 2.9943724 ,  1.        ],\n",
       "       [ 0.04820906,  1.        ],\n",
       "       [ 0.81653404,  1.        ],\n",
       "       [ 1.6787362 ,  1.        ],\n",
       "       [ 2.153521  ,  0.        ],\n",
       "       [-0.33922836,  1.        ],\n",
       "       [ 1.5556794 ,  1.        ],\n",
       "       [ 1.0173199 ,  1.        ],\n",
       "       [ 2.1092613 ,  0.        ],\n",
       "       [ 2.9577367 ,  1.        ],\n",
       "       [ 1.1590706 ,  1.        ],\n",
       "       [ 1.6800265 ,  1.        ],\n",
       "       [-0.8121228 ,  0.        ],\n",
       "       [ 0.77607214,  0.        ],\n",
       "       [ 0.4470694 ,  1.        ],\n",
       "       [ 0.87709   ,  1.        ],\n",
       "       [-0.14231563,  0.        ],\n",
       "       [ 2.8459313 ,  1.        ],\n",
       "       [ 3.1608982 ,  1.        ],\n",
       "       [ 2.9603918 ,  1.        ],\n",
       "       [ 0.78286827,  1.        ],\n",
       "       [ 1.0227532 ,  0.        ],\n",
       "       [ 1.9025342 ,  0.        ],\n",
       "       [ 2.990617  ,  1.        ],\n",
       "       [ 2.26565   ,  1.        ],\n",
       "       [ 1.2955947 ,  1.        ],\n",
       "       [-0.22971678,  0.        ],\n",
       "       [ 0.3861898 ,  0.        ],\n",
       "       [ 0.6878571 ,  1.        ],\n",
       "       [ 2.454283  ,  0.        ],\n",
       "       [ 2.7542036 ,  0.        ],\n",
       "       [ 0.16784503,  1.        ],\n",
       "       [-0.45879072,  1.        ],\n",
       "       [ 2.151664  ,  0.        ],\n",
       "       [ 1.1345888 ,  1.        ],\n",
       "       [-0.03375666,  1.        ],\n",
       "       [ 1.062676  ,  1.        ],\n",
       "       [ 0.2141164 ,  1.        ],\n",
       "       [-2.8403869 ,  1.        ],\n",
       "       [ 2.3077576 ,  0.        ],\n",
       "       [ 1.8454009 ,  1.        ],\n",
       "       [ 1.7590573 ,  0.        ],\n",
       "       [ 0.46475637,  0.        ],\n",
       "       [ 1.6317978 ,  1.        ],\n",
       "       [-0.3191898 ,  1.        ],\n",
       "       [ 2.537388  ,  1.        ],\n",
       "       [ 1.6036605 ,  1.        ],\n",
       "       [-0.20796794,  0.        ],\n",
       "       [ 0.33196968,  0.        ],\n",
       "       [ 0.67555916,  1.        ],\n",
       "       [ 2.295976  ,  1.        ],\n",
       "       [ 3.174825  ,  0.        ],\n",
       "       [-0.38324508,  0.        ],\n",
       "       [ 0.85565805,  1.        ],\n",
       "       [ 1.6978259 ,  1.        ],\n",
       "       [-0.01370193,  1.        ],\n",
       "       [ 1.2373518 ,  1.        ],\n",
       "       [ 2.4647918 ,  1.        ],\n",
       "       [-0.16453509,  1.        ],\n",
       "       [-0.32348177,  1.        ],\n",
       "       [ 1.2953942 ,  0.        ],\n",
       "       [ 2.0016637 ,  0.        ],\n",
       "       [ 2.281684  ,  0.        ],\n",
       "       [ 0.36328158,  1.        ],\n",
       "       [ 0.45001236,  1.        ],\n",
       "       [ 0.39214033,  1.        ],\n",
       "       [ 2.6830974 ,  0.        ],\n",
       "       [-0.4121926 ,  0.        ],\n",
       "       [-0.275792  ,  1.        ],\n",
       "       [-0.66231745,  1.        ],\n",
       "       [-0.18048932,  1.        ],\n",
       "       [ 1.4738117 ,  1.        ],\n",
       "       [ 1.2475516 ,  1.        ],\n",
       "       [ 0.15358238,  1.        ],\n",
       "       [ 0.16844222,  1.        ],\n",
       "       [ 2.108901  ,  1.        ],\n",
       "       [-0.1385641 ,  1.        ],\n",
       "       [-0.5364121 ,  1.        ],\n",
       "       [-0.25704777,  1.        ],\n",
       "       [ 0.91401577,  1.        ],\n",
       "       [-0.06338137,  1.        ],\n",
       "       [ 1.4059516 ,  0.        ],\n",
       "       [ 0.7767335 ,  1.        ],\n",
       "       [-0.02235584,  0.        ],\n",
       "       [-0.01814531,  0.        ],\n",
       "       [ 0.59680223,  0.        ],\n",
       "       [ 2.6964788 ,  0.        ],\n",
       "       [ 1.5120888 ,  1.        ],\n",
       "       [-0.32050246,  1.        ],\n",
       "       [ 2.8819127 ,  0.        ],\n",
       "       [-0.7044329 ,  1.        ],\n",
       "       [ 1.1043947 ,  0.        ],\n",
       "       [ 1.2091606 ,  0.        ],\n",
       "       [ 0.23436344,  1.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_tr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
