{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e40e26a7-fa01-4130-ba3c-fb6dd43ef193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import uproot as ur\n",
    "import awkward as ak\n",
    "import time as t\n",
    "import os\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28bca62d-036c-42c3-8a7e-2c09a22a0def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "== Single Track Multiple Cluster Script ==\n",
      "===========================================\n",
      "\n",
      "Awkward version: 1.7.0\n",
      "Uproot version: 4.1.8\n"
     ]
    }
   ],
   "source": [
    "Nfile=1\n",
    "\n",
    "print()\n",
    "print('='*43)\n",
    "print('== Single Track Multiple Cluster Script ==')\n",
    "print('='*43)\n",
    "print()\n",
    "print(\"Awkward version: \"+str(ak.__version__))\n",
    "print(\"Uproot version: \"+str(ur.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "187524ad-462c-4858-b937-b2cf6af1d805",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DeltaR(coords, ref):\n",
    "    ''' Straight forward function, expects Nx2 inputs for coords, 1x2 input for ref '''\n",
    "    ref = np.tile(ref, (len(coords[:,0]), 1))\n",
    "    DeltaCoords = np.subtract(coords, ref)\n",
    "    ## Mirroring ##\n",
    "    gt_pi_mask = DeltaCoords > np.pi\n",
    "    lt_pi_mask = DeltaCoords < - np.pi\n",
    "    DeltaCoords[lt_pi_mask] = DeltaCoords[lt_pi_mask] + 2*np.pi\n",
    "    DeltaCoords[gt_pi_mask] = DeltaCoords[gt_pi_mask] - 2*np.pi\n",
    "    return np.sqrt(DeltaCoords[:,0]**2 + DeltaCoords[:,1]**2)\n",
    "\n",
    "def find_max_dim_tuple(events, event_dict):\n",
    "    nEvents = len(events)\n",
    "    max_clust = 0\n",
    "    \n",
    "    for i in range(nEvents):\n",
    "        event = events[i,0]\n",
    "        track_nums = events[i,1]\n",
    "        clust_nums = events[i,2]\n",
    "        \n",
    "        clust_num_total = 0\n",
    "        # set this to six for now to handle single track events, change later\n",
    "        track_num_total = 10 # max 9 but keep a buffer of 1\n",
    "        \n",
    "        # Check if there are clusters, None type object may be associated with it\n",
    "        if clust_nums is not None:\n",
    "            # Search through cluster indices\n",
    "            for clst_idx in clust_nums:\n",
    "                nInClust = len(event_dict['cluster_cell_ID'][event][clst_idx])\n",
    "                # add the number in each cluster to the total\n",
    "                clust_num_total += nInClust\n",
    "\n",
    "        total_size = clust_num_total + track_num_total\n",
    "        if total_size > max_clust:\n",
    "            max_clust = total_size\n",
    "    \n",
    "    # 6 for energy, eta, phi, rperp, track flag, sample layer\n",
    "    return (nEvents, max_clust, 6)\n",
    "\n",
    "def dict_from_tree(tree, branches=None, np_branches=None):\n",
    "    ''' Loads branches as default awkward arrays and np_branches as numpy arrays. '''\n",
    "    dictionary = dict()\n",
    "    if branches is not None:\n",
    "        for key in branches:\n",
    "            branch = tree.arrays()[key]\n",
    "            dictionary[key] = branch\n",
    "            \n",
    "    if np_branches is not None:\n",
    "        for np_key in np_branches:\n",
    "            np_branch = np.ndarray.flatten(tree.arrays()[np_key].to_numpy())\n",
    "            dictionary[np_key] = np_branch\n",
    "    \n",
    "    if branches is None and np_branches is None:\n",
    "        raise ValueError(\"No branches passed to function.\")\n",
    "        \n",
    "    return dictionary\n",
    "\n",
    "def find_index_1D(values, dictionary):\n",
    "    ''' Use a for loop and a dictionary. values are the IDs to search for. dict must be in format \n",
    "    (cell IDs: index) '''\n",
    "    idx_vec = np.zeros(len(values), dtype=np.int32)\n",
    "    for i in range(len(values)):\n",
    "        idx_vec[i] = dictionary[values[i]]\n",
    "    return idx_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f3ec6f2-ddca-4eab-9fdb-f83dd3a0d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_branches = [\"cluster_nCells\", \"cluster_cell_ID\", \"cluster_cell_E\", 'cluster_nCells', \"nCluster\", \"eventNumber\",\n",
    "                  \"nTrack\", \"nTruthPart\", \"truthPartPdgId\", \"cluster_Eta\", \"cluster_Phi\", 'trackPt', 'trackP',\n",
    "                  'trackMass', 'trackEta', 'trackPhi', 'truthPartE', 'cluster_ENG_CALIB_TOT', \"cluster_E\", 'truthPartPt']\n",
    "\n",
    "ak_event_branches = [\"cluster_nCells\", \"cluster_cell_ID\", \"cluster_cell_E\", \"cluster_nCells\",\n",
    "                  \"nTruthPart\", \"truthPartPdgId\", \"cluster_Eta\", \"cluster_Phi\", \"trackPt\", \"trackP\",\n",
    "                  \"trackMass\", \"trackEta\", \"trackPhi\", \"truthPartE\", \"cluster_ENG_CALIB_TOT\", \"cluster_E\", \"truthPartPt\"]\n",
    "\n",
    "np_event_branches = [\"nCluster\", \"eventNumber\", \"nTrack\", \"nTruthPart\"]\n",
    "\n",
    "geo_branches = [\"cell_geo_ID\", \"cell_geo_eta\", \"cell_geo_phi\", \"cell_geo_rPerp\", \"cell_geo_sampling\"]\n",
    "\n",
    "trk_em_eta = ['trackEta_EMB2', 'trackEta_EME2']\n",
    "trk_em_phi = ['trackPhi_EMB2', 'trackPhi_EME2']\n",
    "\n",
    "trk_proj_eta = ['trackEta_EMB1', 'trackEta_EMB2', 'trackEta_EMB3',\n",
    "    'trackEta_EME1', 'trackEta_EME2', 'trackEta_EME3', 'trackEta_HEC0',\n",
    "    'trackEta_HEC1', 'trackEta_HEC2', 'trackEta_HEC3', 'trackEta_TileBar0',\n",
    "    'trackEta_TileBar1', 'trackEta_TileBar2', 'trackEta_TileGap1',\n",
    "    'trackEta_TileGap2', 'trackEta_TileGap3', 'trackEta_TileExt0',\n",
    "    'trackEta_TileExt1', 'trackEta_TileExt2']\n",
    "trk_proj_phi = ['trackPhi_EMB1', 'trackPhi_EMB2', 'trackPhi_EMB3',\n",
    "    'trackPhi_EME1', 'trackPhi_EME2', 'trackPhi_EME3', 'trackPhi_HEC0',\n",
    "    'trackPhi_HEC1', 'trackPhi_HEC2', 'trackPhi_HEC3', 'trackPhi_TileBar0',\n",
    "    'trackPhi_TileBar1', 'trackPhi_TileBar2', 'trackPhi_TileGap1',\n",
    "    'trackPhi_TileGap2', 'trackPhi_TileGap3', 'trackPhi_TileExt0',\n",
    "    'trackPhi_TileExt1', 'trackPhi_TileExt2']\n",
    "calo_numbers = [1,2,3,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "eta_trk_dict = dict(zip(trk_proj_eta, calo_numbers))\n",
    "\n",
    "calo_layers = ['EMB1', 'EMB2', 'EMB3', 'EME1', 'EME2', 'EME3', 'HEC0', 'HEC1',\n",
    "    'HEC2', 'HEC3', 'TileBar0', 'TileBar1', 'TileBar2', 'TileGap1', 'TileGap2',\n",
    "    'TileGap3', 'TileExt0', 'TileExt1', 'TileExt2']\n",
    "calo_numbers2 = [1,2,3,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "calo_dict = dict(zip(calo_numbers2, calo_layers))\n",
    "\n",
    "fixed_z_numbers = [5,6,7,8,9,10,11]\n",
    "fixed_z_vals = [3790.03, 3983.68, 4195.84, 4461.25, 4869.50, 5424.50, 5905.00]\n",
    "z_calo_dict = dict(zip(fixed_z_numbers, fixed_z_vals))\n",
    "\n",
    "fixed_r_numbers = [1,2,3,12,13,14,15,16,17,18,19,20]\n",
    "fixed_r_vals = [1532.18, 1723.89, 1923.02, 2450.00, 2995.00, 3630.00, 3215.00,\n",
    "                3630.00, 2246.50, 2450.00, 2870.00, 3480.00]\n",
    "r_calo_dict = dict(zip(fixed_r_numbers, fixed_r_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "887a9784-3b13-41f2-ba85-426fb83d65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pion_dir = '/clusterfs/ml4hep/mpettee/ml4pions/data/user.angerami.mc16_13TeV.900247.PG_singlepion_logE0p2to2000.e8312_e7400_s3170_r12383.v01-45-gaa27bcb_OutputStream/'\n",
    "fileNames = sorted(glob(pion_dir+\"*.root\"))[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "201d577f-1614-406b-8a81-d90ebb84d0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "256f6f8e-8a4f-4f48-a402-b1c64f0cd00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GEOMETRY DICTIONARY ##\n",
    "geo_file = ur.open('/clusterfs/ml4hep/mpettee/ml4pions/data/cell_geo.root')\n",
    "CellGeo_tree = geo_file[\"CellGeo\"]\n",
    "geo_dict = dict_from_tree(tree=CellGeo_tree, branches=None, np_branches=geo_branches)\n",
    "\n",
    "# cell geometry data\n",
    "cell_geo_ID = geo_dict['cell_geo_ID']\n",
    "cell_ID_dict = dict(zip(cell_geo_ID, np.arange(len(cell_geo_ID))))\n",
    "\n",
    "## MEMORY MAPPED ARRAY ALLOCATION ##\n",
    "X_large = np.lib.format.open_memmap('X_large.npy', mode='w+', dtype=np.float64,\n",
    "                       shape=(2500000,1500,6), fortran_order=False, version=None)\n",
    "Y_large = np.lib.format.open_memmap('Y_large.npy', mode='w+', dtype=np.float64,\n",
    "                       shape=(2500000,3), fortran_order=False, version=None)\n",
    "Eta_large = np.empty(2500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7418107-9cc3-4708-bd57-bf5ee7c26d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Loop Definitions ##\n",
    "#======================================\n",
    "k = 1 # tally used to keep track of file number\n",
    "tot_nEvts = 0 # used for keeping track of total number of events\n",
    "max_nPoints = 0 # used for keeping track of the largest 'point cloud'\n",
    "t_tot = 0 # total time\n",
    "# for event dictionary\n",
    "events_prefix = ''\n",
    "num_zero_tracks = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c129c74-b701-4fb3-97c6-588627ac89a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on File: /clusterfs/ml4hep/mpettee/ml4pions/data/user.angerami.mc16_13TeV.900247.PG_singlepion_logE0p2to2000.e8312_e7400_s3170_r12383.v01-45-gaa27bcb_OutputStream/user.angerami.24559744.OutputStream._000001.root - 1/1\n",
      "load event dictionary\n",
      "load track dictionary\n",
      "Apply cuts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "event: 100%|██████████| 10366/10366 [00:05<00:00, 2006.99it/s]\n",
      "Fill entries: 100%|██████████| 10250/10250 [01:06<00:00, 153.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array dimension: (10250, 1198, 6)\n",
      "Number of null track projection: 1\n",
      "Time to create dicts and select events: 58.60843110084534\n",
      "Time to find dimensions and make new array: 3.335378646850586\n",
      "Time to construct index array: 5.174484729766846\n",
      "Time to populate elements: 66.91101312637329\n",
      "Time to copy to memory map: 1.0458204746246338\n",
      "Time for this file: 135.0751280784607\n",
      "Total events: 10250\n",
      "Current size: (10250, 1198, 6)\n",
      "Total time: 135.0751280784607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Main File Loop ##\n",
    "#======================================\n",
    "for currFile in fileNames:\n",
    "    \n",
    "    # Check for file, a few are missing\n",
    "    if not os.path.isfile(events_prefix+currFile):\n",
    "        print()\n",
    "        print('File '+events_prefix+currFile+' not found..')\n",
    "        print()\n",
    "        k += 1\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        print()\n",
    "        print('Working on File: '+str(currFile)+' - '+str(k)+'/'+str(Nfile))\n",
    "        k += 1\n",
    "        \n",
    "    t0 = t.time()\n",
    "    ## EVENT DICTIONARY ##\n",
    "    print(\"load event dictionary\")\n",
    "    event = ur.open(events_prefix+currFile)\n",
    "    event_tree = event[\"EventTree\"]\n",
    "    event_dict = dict_from_tree(tree=event_tree, branches=ak_event_branches, np_branches=np_event_branches)\n",
    "    \n",
    "    ## TRACK DICTIONARY ##\n",
    "    print(\"load track dictionary\")\n",
    "    track_dict = dict_from_tree(tree=event_tree,\n",
    "                branches=deepcopy(trk_proj_eta+trk_proj_phi))\n",
    "    \n",
    "    #===================\n",
    "    # APPLY CUTS =======\n",
    "    #===================\n",
    "    print(\"Apply cuts...\")\n",
    "    # create ordered list of events to use for index slicing\n",
    "    nEvents = len(event_dict['eventNumber'])\n",
    "    all_events = np.arange(0,nEvents,1,dtype=np.int32)\n",
    "\n",
    "    # SINGLE TRACK CUT\n",
    "    single_track_mask = event_dict['nTrack'] == np.full(nEvents, 1)\n",
    "    single_track_filter = all_events[single_track_mask]\n",
    "    \n",
    "    # TRACKS WITH CLUSTERS\n",
    "    nCluster = event_dict['nCluster'][single_track_filter]\n",
    "    nz_clust_mask = nCluster != 0\n",
    "    filtered_event = single_track_filter[nz_clust_mask]\n",
    "    t1 = t.time()\n",
    "    events_cuts_time = t1 - t0\n",
    "    \n",
    "    #============================================#\n",
    "    ## CREATE INDEX ARRAY FOR TRACKS + CLUSTERS ##\n",
    "    #============================================#\n",
    "    event_indices = []\n",
    "    t0 = t.time()\n",
    "\n",
    "    for evt in tqdm(filtered_event, desc=\"event\"):\n",
    "\n",
    "        # pull cluster number, don't need zero as it's loaded as a np array\n",
    "        nClust = event_dict[\"nCluster\"][evt]\n",
    "        cluster_idx = np.arange(nClust)\n",
    "\n",
    "        # Notes: this will need to handle more complex scenarios in the future for tracks with\n",
    "        # no clusters\n",
    "\n",
    "        ## DELTA R ##\n",
    "        # pull coordinates of tracks and clusters from event\n",
    "        # we can get away with the zeroth index because we are working with single track events\n",
    "        trackCoords = np.array([event_dict[\"trackEta\"][evt][0],\n",
    "                                 event_dict[\"trackPhi\"][evt][0]])\n",
    "        clusterCoords = np.stack((event_dict[\"cluster_Eta\"][evt].to_numpy(),\n",
    "                                   event_dict[\"cluster_Phi\"][evt].to_numpy()), axis=1)\n",
    "\n",
    "        _DeltaR = DeltaR(clusterCoords, trackCoords)\n",
    "        DeltaR_mask = _DeltaR < 1.2\n",
    "        matched_clusters = cluster_idx[DeltaR_mask]\n",
    "\n",
    "        ## CREATE LIST ##\n",
    "        # Note: currently do not have track only events. Do this in the future    \n",
    "        if np.count_nonzero(DeltaR_mask) > 0:\n",
    "            event_indices.append((evt, 0, matched_clusters))\n",
    "    \n",
    "    event_indices = np.array(event_indices, dtype=np.object_)\n",
    "    t1 = t.time()\n",
    "    indices_time = t1 - t0\n",
    "    \n",
    "    #=========================#\n",
    "    ## DIMENSIONS OF X ARRAY ##\n",
    "    #=========================#\n",
    "    t0 = t.time()\n",
    "    max_dims = find_max_dim_tuple(event_indices, event_dict)\n",
    "    evt_tot = max_dims[0]\n",
    "    tot_nEvts += max_dims[0]\n",
    "    # keep track of the largest point cloud to use for saving later\n",
    "    if max_dims[1] > max_nPoints:\n",
    "        max_nPoints = max_dims[1]\n",
    "    \n",
    "    # Create arrays\n",
    "    Y_new = np.zeros((max_dims[0],3))\n",
    "    X_new = np.zeros(max_dims)\n",
    "    Eta_new = np.zeros(max_dims[0])\n",
    "    t1 = t.time()\n",
    "    find_create_max_dims_time = t1 - t0    \n",
    "    \n",
    "    #===================#\n",
    "    ## FILL IN ENTRIES ##==============================================================\n",
    "    #===================#\n",
    "    t0 = t.time()\n",
    "    for i in tqdm(range(max_dims[0]), desc=\"Fill entries\"):\n",
    "        # pull all relevant indices\n",
    "        evt = event_indices[i,0]\n",
    "        track_idx = event_indices[i,1]\n",
    "        # recall this now returns an array\n",
    "        cluster_nums = event_indices[i,2]\n",
    "\n",
    "        ## Centering ##\n",
    "        trk_bool_em = np.zeros(2, dtype=bool)\n",
    "        trk_full_em = np.empty((2,2))\n",
    "    \n",
    "        for l, (eta_key, phi_key) in enumerate(zip(trk_em_eta, trk_em_phi)):\n",
    "\n",
    "            eta_em = track_dict[eta_key][evt][track_idx]\n",
    "            phi_em = track_dict[phi_key][evt][track_idx]\n",
    "\n",
    "            if np.abs(eta_em) < 2.5 and np.abs(phi_em) <= np.pi:\n",
    "                trk_bool_em[l] = True\n",
    "                trk_full_em[l,0] = eta_em\n",
    "                trk_full_em[l,1] = phi_em\n",
    "                \n",
    "        nProj_em = np.count_nonzero(trk_bool_em)\n",
    "        if nProj_em == 1:\n",
    "            eta_ctr = trk_full_em[trk_bool_em, 0]\n",
    "            phi_ctr = trk_full_em[trk_bool_em, 1]\n",
    "            \n",
    "        elif nProj_em == 2:\n",
    "            trk_av_em = np.mean(trk_full_em, axis=1)\n",
    "            eta_ctr = trk_av_em[0]\n",
    "            phi_ctr = trk_av_em[1]\n",
    "            \n",
    "        elif nProj_em == 0:\n",
    "            eta_ctr = event_dict['trackEta'][evt][track_idx]\n",
    "            phi_ctr = event_dict['trackPhi'][evt][track_idx]      \n",
    "        \n",
    "        ##############\n",
    "        ## CLUSTERS ##\n",
    "        ##############\n",
    "        # set up to have no clusters, further this with setting up the same thing for tracks\n",
    "        target_ENG_CALIB_TOT = -1\n",
    "        if cluster_nums is not None:\n",
    "\n",
    "            # find averaged center of clusters\n",
    "            cluster_Eta = event_dict['cluster_Eta'][evt].to_numpy()\n",
    "            cluster_Phi = event_dict['cluster_Phi'][evt].to_numpy()\n",
    "            cluster_E = event_dict['cluster_E'][evt].to_numpy()\n",
    "            cl_E_tot = np.sum(cluster_E)\n",
    "\n",
    "            nClust_current_total = 0\n",
    "            target_ENG_CALIB_TOT = 0\n",
    "            for c in cluster_nums:            \n",
    "                # cluster data\n",
    "                target_ENG_CALIB_TOT += event_dict['cluster_ENG_CALIB_TOT'][evt][c]\n",
    "                cluster_cell_ID = event_dict['cluster_cell_ID'][evt][c].to_numpy()\n",
    "                nInClust = len(cluster_cell_ID)\n",
    "                cluster_cell_E = event_dict['cluster_cell_E'][evt][c].to_numpy()            \n",
    "                cell_indices = find_index_1D(cluster_cell_ID, cell_ID_dict)\n",
    "\n",
    "                cluster_cell_Eta = geo_dict['cell_geo_eta'][cell_indices]\n",
    "                cluster_cell_Phi = geo_dict['cell_geo_phi'][cell_indices]\n",
    "                cluster_cell_rPerp = geo_dict['cell_geo_rPerp'][cell_indices]\n",
    "                cluster_cell_sampling = geo_dict['cell_geo_sampling'][cell_indices]\n",
    "\n",
    "                # input all the data\n",
    "                # note here we leave the fourth entry zeros (zero for flag!!!)\n",
    "                low = nClust_current_total\n",
    "                high = low + nInClust\n",
    "                X_new[i,low:high,0] = cluster_cell_E\n",
    "                # Normalize to average cluster centers\n",
    "                X_new[i,low:high,1] = cluster_cell_Eta - eta_ctr\n",
    "                X_new[i,low:high,2] = cluster_cell_Phi - eta_ctr\n",
    "                X_new[i,low:high,3] = cluster_cell_rPerp\n",
    "                X_new[i,low:high,5] = cluster_cell_sampling\n",
    "\n",
    "                nClust_current_total += nInClust\n",
    "\n",
    "        #####################\n",
    "        ## TARGET ENERGIES ##\n",
    "        #####################\n",
    "        # this should be flattened or loaded as np array instead of zeroth index in future\n",
    "        Y_new[i,0] = event_dict['truthPartE'][evt][0]\n",
    "        Y_new[i,1] = event_dict['truthPartPt'][evt][track_idx]\n",
    "        Y_new[i,2] = target_ENG_CALIB_TOT\n",
    "        \n",
    "        #########\n",
    "        ## ETA ##\n",
    "        #########\n",
    "        # again only get away with this because we have a single track\n",
    "        Eta_new[i] = event_dict[\"trackEta\"][evt][track_idx]\n",
    "\n",
    "        ############\n",
    "        ## TRACKS ##\n",
    "        ############\n",
    "        \n",
    "        trk_bool = np.zeros(len(calo_numbers), dtype=bool)\n",
    "        trk_full = np.empty((len(calo_numbers), 4))\n",
    "        \n",
    "        for j, (eta_key, phi_key) in enumerate(zip(trk_proj_eta, trk_proj_phi)):\n",
    "            \n",
    "            cnum = eta_trk_dict[eta_key]\n",
    "            layer = calo_dict[cnum]\n",
    "            \n",
    "            eta = track_dict[eta_key][evt][track_idx]\n",
    "            phi = track_dict[phi_key][evt][track_idx]\n",
    "            \n",
    "            if np.abs(eta) < 2.5 and np.abs(phi) <= np.pi:\n",
    "                trk_bool[j] = True\n",
    "                trk_full[j,0] = eta\n",
    "                trk_full[j,1] = phi\n",
    "                trk_full[j,3] = cnum\n",
    "                \n",
    "                if cnum in fixed_r_numbers:\n",
    "                    rPerp = r_calo_dict[cnum]\n",
    "                    \n",
    "                elif cnum in fixed_z_numbers:\n",
    "                    z = z_calo_dict[cnum]\n",
    "                    aeta = np.abs(eta)\n",
    "                    rPerp = z*2*np.exp(aeta)/(np.exp(2*aeta) - 1)\n",
    "                    \n",
    "                else:\n",
    "                    raise ValueError('Calo sample num not found in dicts..')\n",
    "                \n",
    "                if rPerp < 0:\n",
    "                    print()\n",
    "                    print('Found negative rPerp'); print()\n",
    "                    print('Event number: {}'.format(evt))\n",
    "                    print('Eta: {}'.format(eta))\n",
    "                    print('Phi: {}'.format(phi))\n",
    "                    print('rPerp: {}'.format(rPerp))\n",
    "                    raise ValueError('Found negative rPerp')\n",
    "                    \n",
    "                trk_full[j,2] = rPerp\n",
    "                \n",
    "        # Fill in track array\n",
    "        trk_proj_num = np.count_nonzero(trk_bool)\n",
    "        \n",
    "        if trk_proj_num == 0:\n",
    "            trk_proj_num = 1\n",
    "            trk_arr = np.empty((1, 6))\n",
    "            num_zero_tracks += 1\n",
    "            trk_arr[:,0] = event_dict['trackP'][evt][track_idx]\n",
    "            trk_arr[:,1] = event_dict['trackEta'][evt][track_idx] - eta_ctr\n",
    "            trk_arr[:,2] = event_dict['trackPhi'][evt][track_idx] - phi_ctr\n",
    "            trk_arr[:,3] = 1532.18 # just place it in EMB1\n",
    "            trk_arr[:,4] = 1 # track flag\n",
    "            trk_arr[:,5] = 1 # place layer in EMB1\n",
    "        else:\n",
    "            trk_arr = np.empty((trk_proj_num, 6))\n",
    "            trackP = event_dict['trackP'][evt][track_idx]\n",
    "            trk_arr[:,1:4] = np.ndarray.copy(trk_full[trk_bool,:3])\n",
    "            trk_arr[:,4] = np.ones(trk_proj_num)\n",
    "            trk_arr[:,5] = np.ndarray.copy(trk_full[trk_bool,3])\n",
    "            trk_arr[:,0] = trackP/trk_proj_num\n",
    "\n",
    "            trk_arr[:,1] = trk_arr[:,1] - eta_ctr\n",
    "            trk_arr[:,2] = trk_arr[:,2] - phi_ctr\n",
    "\n",
    "        X_new[i,high:high+trk_proj_num,:] = np.ndarray.copy(trk_arr)\n",
    "    \n",
    "    #=========================================================================#\n",
    "    t1 = t.time()\n",
    "    array_construction_time = t1 - t0\n",
    "    \n",
    "    #=======================#\n",
    "    ## ARRAY CONCATENATION ##\n",
    "    #=======================#\n",
    "    t0 = t.time()\n",
    "    # Write to X\n",
    "    old_tot = tot_nEvts - max_dims[0]\n",
    "    X_large[old_tot:tot_nEvts, :max_dims[1], :6] = np.ndarray.copy(X_new)\n",
    "    # pad the remainder with zeros (just to be sure)\n",
    "    fill_shape = (tot_nEvts - old_tot, 1500 - max_dims[1], 6)\n",
    "    X_large[old_tot:tot_nEvts, max_dims[1]:1500, :6] = np.zeros(fill_shape)\n",
    "    \n",
    "    # Write to Y\n",
    "    Y_large[old_tot:tot_nEvts,:] = np.ndarray.copy(Y_new)\n",
    "    \n",
    "    # Eta\n",
    "    Eta_large[old_tot:tot_nEvts] = np.ndarray.copy(Eta_new)\n",
    "        \n",
    "    t1 = t.time()\n",
    "    time_to_memmap = t1-t0\n",
    "    thisfile_t_tot = events_cuts_time+find_create_max_dims_time+indices_time\\\n",
    "          +array_construction_time+time_to_memmap\n",
    "    t_tot += thisfile_t_tot\n",
    "    \n",
    "    print('Array dimension: '+str(max_dims))\n",
    "    print('Number of null track projection: '+str(num_zero_tracks))\n",
    "    print('Time to create dicts and select events: '+str(events_cuts_time))\n",
    "    print('Time to find dimensions and make new array: '+str(find_create_max_dims_time))\n",
    "    print('Time to construct index array: '+str(indices_time))\n",
    "    print('Time to populate elements: '+str(array_construction_time))\n",
    "    print('Time to copy to memory map: '+str(time_to_memmap))\n",
    "    print('Time for this file: '+str(thisfile_t_tot))\n",
    "    print('Total events: '+str(tot_nEvts))\n",
    "    print('Current size: '+str((tot_nEvts,max_nPoints,6)))\n",
    "    print('Total time: '+str(t_tot))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d72e403-7389-49de-b965-fe640dd390a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to copy new and delete old: 1.5085344314575195 (s)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = t.time()\n",
    "X = np.lib.format.open_memmap('X_STMC_v2_'+str(Nfile)+'_files.npy',\n",
    "                             mode='w+', dtype=np.float64, shape=(tot_nEvts, max_nPoints, 6))\n",
    "np.copyto(dst=X, src=X_large[:tot_nEvts,:max_nPoints,:], casting='same_kind', where=True)\n",
    "del X_large\n",
    "os.system('rm X_large.npy')\n",
    "\n",
    "Y = np.lib.format.open_memmap('Y_STMC_v2_'+str(Nfile)+'_files.npy',\n",
    "                             mode='w+', dtype=np.float64, shape=(tot_nEvts, 3))\n",
    "np.copyto(dst=Y, src=Y_large[:tot_nEvts,:], casting='same_kind', where=True)\n",
    "del Y_large\n",
    "os.system('rm Y_large.npy')\n",
    "\n",
    "np.save('Eta_STMC_v2_'+str(Nfile)+'_files', Eta_large[:tot_nEvts])\n",
    "\n",
    "t1 = t.time()\n",
    "print()\n",
    "print('Time to copy new and delete old: '+str(t1-t0)+' (s)')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "549cfae8-0a51-40b6-b7c0-183513ffa0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10250, 1198, 6)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"X_STMC_v2_1_files.npy\").shape # (n_events, max_n_nodes, node_features)\n",
    "# node features = 6 for energy, eta, phi, rperp, track flag, sample layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2adedc1b-3c55-4de5-ad9f-320900cfe810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10250, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"Y_STMC_v2_1_files.npy\").shape\n",
    "#         Y_new[i,0] = event_dict['truthPartE'][evt][0]\n",
    "#        Y_new[i,1] = event_dict['truthPartPt'][evt][track_idx]\n",
    "#        Y_new[i,2] = target_ENG_CALIB_TOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29138931-6e35-4901-b497-ba0e9b658258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10250,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(\"Eta_STMC_v2_1_files.npy\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362b858-847a-41e5-8331-845492df3bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbdev",
   "language": "python",
   "name": "nbdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
