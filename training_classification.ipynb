{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import uproot as ur\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets.graphs import GraphsTuple\n",
    "import sonnet as snt\n",
    "import argparse\n",
    "import yaml\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "from gn4pions.modules.data import GraphDataGenerator\n",
    "# from gn4pions.modules.models import MultiOutWeightedRegressModel\n",
    "from gn4pions.modules.models import MLPGraphNetwork\n",
    "from gn4pions.modules.utils import convert_to_tuple\n",
    "\n",
    "sns.set_context('poster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model config\n",
    "config_file = 'gn4pions/configs/classification.yaml'\n",
    "config = yaml.load(open(config_file), Loader=yaml.FullLoader)\n",
    "\n",
    "# Data config\n",
    "data_config = config['data']\n",
    "\n",
    "data_dir = data_config['data_dir']\n",
    "num_train_files = data_config['num_train_files']\n",
    "num_val_files = data_config['num_val_files']\n",
    "batch_size = data_config['batch_size']\n",
    "shuffle = data_config['shuffle']\n",
    "num_procs = data_config['num_procs']\n",
    "preprocess = data_config['preprocess']\n",
    "output_dir = data_config['output_dir']\n",
    "already_preprocessed = data_config['already_preprocessed']  # Set to false when running training for first time\n",
    "\n",
    "# Model Config\n",
    "model_config = config['model']\n",
    "\n",
    "concat_input = model_config['concat_input']\n",
    "\n",
    "\n",
    "# Traning Config\n",
    "train_config = config['training']\n",
    "\n",
    "epochs = train_config['epochs']\n",
    "learning_rate = train_config['learning_rate']\n",
    "alpha = train_config['alpha']\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(train_config['gpu'])\n",
    "log_freq = train_config['log_freq']\n",
    "save_dir = train_config['save_dir'] + config_file.replace('.yaml','').split('/')[-1] + '_' + time.strftime(\"%Y%m%d\")\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "yaml.dump(config, open(save_dir + '/config.yaml', 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and create data generators\n",
    "\n",
    "pi0_files = np.sort(glob.glob(data_dir+'*pi0*/*root'))\n",
    "pion_files = np.sort(glob.glob(data_dir+'*pion*/*root'))\n",
    "\n",
    "train_start = 0\n",
    "train_end = train_start + num_train_files\n",
    "val_end = train_end + num_val_files\n",
    "\n",
    "pi0_train_files = pi0_files[train_start:train_end]\n",
    "pi0_val_files = pi0_files[train_end:val_end]\n",
    "pion_train_files = pion_files[train_start:train_end]\n",
    "pion_val_files = pion_files[train_end:val_end]\n",
    "\n",
    "train_output_dir = None\n",
    "val_output_dir = None\n",
    "\n",
    "# Get Data\n",
    "if preprocess:\n",
    "    train_output_dir = output_dir + '/train/'\n",
    "    val_output_dir = output_dir + '/val/'\n",
    "\n",
    "    if already_preprocessed:\n",
    "        train_files = np.sort(glob.glob(train_output_dir+'*.p'))[:num_train_files]\n",
    "        val_files = np.sort(glob.glob(val_output_dir+'*.p'))[:num_val_files]\n",
    "\n",
    "        pi0_train_files = train_files\n",
    "        pi0_val_files = val_files\n",
    "        pion_train_files = None\n",
    "        pion_val_files = None\n",
    "\n",
    "        train_output_dir = None\n",
    "        val_output_dir = None\n",
    "\n",
    "# Traning Data Generator\n",
    "# Will preprocess data if it doesnt find pickled files\n",
    "data_gen_train = GraphDataGenerator(pi0_file_list=pi0_train_files,\n",
    "                                    pion_file_list=pion_train_files,\n",
    "                                    cellGeo_file=data_dir+'CellGeo.neighbours.root',\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=shuffle,\n",
    "                                    num_procs=num_procs,\n",
    "                                    preprocess=preprocess,\n",
    "                                    output_dir=train_output_dir)\n",
    "\n",
    "# Validation Data generator\n",
    "# Will preprocess data if it doesnt find pickled files\n",
    "data_gen_val = GraphDataGenerator(pi0_file_list=pi0_val_files,\n",
    "                                  pion_file_list=pion_val_files,\n",
    "                                  cellGeo_file=data_dir+'CellGeo.neighbours.root',\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=shuffle,\n",
    "                                  num_procs=num_procs,\n",
    "                                  preprocess=preprocess,\n",
    "                                  output_dir=val_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get batch of data\n",
    "def get_batch(data_iter):\n",
    "    for graphs, targets in data_iter:\n",
    "        graphs = convert_to_tuple(graphs)\n",
    "        targets = tf.convert_to_tensor(targets)\n",
    "        yield graphs, targets\n",
    "        \n",
    "# Define loss function\n",
    "mae_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "# weighted regression loss:\n",
    "# def loss_fn(targets, regress_preds, class_preds):\n",
    "#     regress_loss = mae_loss(targets[:,:1], regress_preds)\n",
    "#     class_loss = bce_loss(targets[:,1:], class_preds)\n",
    "#     combined_loss = alpha*regress_loss + (1 - alpha)*class_loss \n",
    "#     return regress_loss, class_loss, combined_loss\n",
    "\n",
    "def loss_fn(targets, class_preds):\n",
    "    class_loss = bce_loss(targets[:,1:], class_preds)\n",
    "    return class_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-31 17:53:38.144509: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-31 17:53:38.643246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22537 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Get a sample graph for tf.function decorator\n",
    "samp_graph, samp_target = next(get_batch(data_gen_train.generator()))\n",
    "data_gen_train.kill_procs()\n",
    "graph_spec = utils_tf.specs_from_graphs_tuple(samp_graph, True, True, True)\n",
    "\n",
    "# Traning set\n",
    "# @tf.function(input_signature=[graph_spec, tf.TensorSpec(shape=[None,2], dtype=tf.float32)])\n",
    "# def train_step(graphs, targets):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         regress_output, class_output = model(graphs)\n",
    "#         regress_preds = regress_output.globals\n",
    "#         class_preds = class_output.globals\n",
    "#         regress_loss, class_loss, loss = loss_fn(targets, regress_preds, class_preds)\n",
    "\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "#     return regress_loss, class_loss, loss\n",
    "@tf.function(input_signature=[graph_spec, tf.TensorSpec(shape=[None,2], dtype=tf.float32)])\n",
    "def train_step(graphs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        class_output = model(graphs)\n",
    "        print(\"class_output: \" + str(class_output))\n",
    "        class_preds = class_output.globals\n",
    "        print(\"class_output.globals: \" + str(class_output.globals))\n",
    "        loss = loss_fn(targets, class_preds)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Validation Step\n",
    "# @tf.function(input_signature=[graph_spec, tf.TensorSpec(shape=[None,2], dtype=tf.float32)])\n",
    "# def val_step(graphs, targets):\n",
    "#     regress_output, class_output = model(graphs)\n",
    "#     regress_preds = regress_output.globals\n",
    "#     class_preds = class_output.globals\n",
    "#     regress_loss, class_loss, loss = loss_fn(targets, regress_preds, class_preds)\n",
    "#     return regress_loss, class_loss, loss, regress_preds, class_preds\n",
    "@tf.function(input_signature=[graph_spec, tf.TensorSpec(shape=[None,2], dtype=tf.float32)])\n",
    "def val_step(graphs, targets):\n",
    "    class_output = model(graphs)\n",
    "    class_preds = class_output.globals\n",
    "    loss = loss_fn(targets, class_preds)\n",
    "    return loss, class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "# model = MultiOutWeightedRegressModel(global_output_size=1, num_outputs=2, model_config=model_config)\n",
    "model = MLPGraphNetwork(model_config=model_config)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Average epoch losses\n",
    "training_loss_epoch = []\n",
    "training_loss_regress_epoch = []\n",
    "training_loss_class_epoch = []\n",
    "val_loss_epoch = []\n",
    "val_loss_regress_epoch = []\n",
    "val_loss_class_epoch = []\n",
    "\n",
    "# Model checkpointing, load latest model if available\n",
    "checkpoint = tf.train.Checkpoint(module=model)\n",
    "checkpoint_prefix = os.path.join(save_dir, 'latest_model')\n",
    "latest = tf.train.latest_checkpoint(save_dir)\n",
    "if latest is not None:\n",
    "    checkpoint.restore(latest)\n",
    "else:\n",
    "    checkpoint.save(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-31 17:54:46.035025: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "MLPGraphNetwork(\n    model_config={'block_type': 'graphnet',\n                  'concat_input': True,\n                  'edge_block_opt': {'use_edges': True,\n                                     'use_globals': True,\n                                     'use_receiver_nodes': True,\n                                     'use_sender_nodes': True},\n                  'global_block_opt': {'use_edges': True,\n                                       'use_globals': True,\n                                       'use_nodes': True},\n                  'latent_size': 64,\n                  'node_block_opt': {'use_globals': True,\n                                     'use_nodes': True,\n                                     'use_received_edges': True,\n                                     'use_sent_edges': True},\n                  'num_blocks': 4,\n                  'num_layers': 4,\n                  'reducer': 'mean'},\n) does not currently contain any variables.\n\nMost Sonnet modules create variables the first time they are called with an\ninput and requesting variables before this typically indicates a coding error.\n\nYou should refactor your code such that you request module variables after you\npass an example input to the module. For example:\n\n    module = MLPGraphNetwork(\n    model_config={'block_type': 'graphnet',\n                  'concat_input': True,\n                  'edge_block_opt': {'use_edges': True,\n                                     'use_globals': True,\n                                     'use_receiver_nodes': True,\n                                     'use_sender_nodes': True},\n                  'global_block_opt': {'use_edges': True,\n                                       'use_globals': True,\n                                       'use_nodes': True},\n                  'latent_size': 64,\n                  'node_block_opt': {'use_globals': True,\n                                     'use_nodes': True,\n                                     'use_received_edges': True,\n                                     'use_sent_edges': True},\n                  'num_blocks': 4,\n                  'num_layers': 4,\n                  'reducer': 'mean'},\n)\n    output = module(input)\n    params = module.variables\n\nIf the module is stateless consider using `snt.allow_empty_variables(module)` to\nsuppress this error:\n\n    module = MLPGraphNetwork(\n    model_config={'block_type': 'graphnet',\n                  'concat_input': True,\n                  'edge_block_opt': {'use_edges': True,\n                                     'use_globals': True,\n                                     'use_receiver_nodes': True,\n                                     'use_sender_nodes': True},\n                  'global_block_opt': {'use_edges': True,\n                                       'use_globals': True,\n                                       'use_nodes': True},\n                  'latent_size': 64,\n                  'node_block_opt': {'use_globals': True,\n                                     'use_nodes': True,\n                                     'use_received_edges': True,\n                                     'use_sent_edges': True},\n                  'num_blocks': 4,\n                  'num_layers': 4,\n                  'reducer': 'mean'},\n)\n    snt.allow_empty_variables(module)\n    params = module.variables\n\nYou can annotate your own subclasses directly if you prefer:\n\n    @snt.allow_empty_variables\n    class MyStatelessModule(snt.Module):\n      pass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8481/3432006072.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/gn4pions/lib/python3.9/site-packages/sonnet/src/utils.py\u001b[0m in \u001b[0;36m_decorate_unbound_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mbound_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_decorate_unbound_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gn4pions/lib/python3.9/site-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mwrap_with_name_scope\u001b[0;34m(method, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# snt.Module enters the module name scope for all methods. To disable this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;31m# for a particular method annotate it with `@snt.no_name_scope`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gn4pions/lib/python3.9/site-packages/sonnet/src/base.py\u001b[0m in \u001b[0;36mvariables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m       \u001b[0;31m# module to another by zipping both module variables and assigning one to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0;31m# the other).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m    449\u001b[0m           NO_VARIABLES_ERROR.format(module=self, property=\"variables\"))\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: MLPGraphNetwork(\n    model_config={'block_type': 'graphnet',\n                  'concat_input': True,\n                  'edge_block_opt': {'use_edges': True,\n                                     'use_globals': True,\n                                     'use_receiver_nodes': True,\n                                     'use_sender_nodes': True},\n                  'global_block_opt': {'use_edges': True,\n                                       'use_globals': True,\n                                       'use_nodes': True},\n                  'latent_size': 64,\n                  'node_block_opt': {'use_globals': True,\n                                     'use_nodes': True,\n                                     'use_received_edges': True,\n                                     'use_sent_edges': True},\n                  'num_blocks': 4,\n                  'num_layers': 4,\n                  'reducer': 'mean'},\n) does not currently contain any variables.\n\nMost Sonnet modules create variables the first time they are called with an\ninput and requesting variables before this typically indicates a coding error.\n\nYou should refactor your code such that you request module variables after you\npass an example input to the module. For example:\n\n    module = MLPGraphNetwork(\n    model_config={'block_type': 'graphnet',\n                  'concat_input': True,\n                  'edge_block_opt': {'use_edges': True,\n                                     'use_globals': True,\n                                     'use_receiver_nodes': True,\n                                     'use_sender_nodes': True},\n                  'global_block_opt': {'use_edges': True,\n                                       'use_globals': True,\n                                       'use_nodes': True},\n                  'latent_size': 64,\n                  'node_block_opt': {'use_globals': True,\n                                     'use_nodes': True,\n                                     'use_received_edges': True,\n                                     'use_sent_edges': True},\n                  'num_blocks': 4,\n                  'num_layers': 4,\n                  'reducer': 'mean'},\n)\n    output = module(input)\n    params = module.variables\n\nIf the module is stateless consider using `snt.allow_empty_variables(module)` to\nsuppress this error:\n\n    module = MLPGraphNetwork(\n    model_config={'block_type': 'graphnet',\n                  'concat_input': True,\n                  'edge_block_opt': {'use_edges': True,\n                                     'use_globals': True,\n                                     'use_receiver_nodes': True,\n                                     'use_sender_nodes': True},\n                  'global_block_opt': {'use_edges': True,\n                                       'use_globals': True,\n                                       'use_nodes': True},\n                  'latent_size': 64,\n                  'node_block_opt': {'use_globals': True,\n                                     'use_nodes': True,\n                                     'use_received_edges': True,\n                                     'use_sent_edges': True},\n                  'num_blocks': 4,\n                  'num_layers': 4,\n                  'reducer': 'mean'},\n)\n    snt.allow_empty_variables(module)\n    params = module.variables\n\nYou can annotate your own subclasses directly if you prefer:\n\n    @snt.allow_empty_variables\n    class MyStatelessModule(snt.Module):\n      pass"
     ]
    }
   ],
   "source": [
    "model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting epoch: 0\n",
      "Training...\n",
      "GraphsTuple(nodes=<tf.Tensor 'MLPGraphNetwork/graph_network/node_block/layer_norm/batchnorm/add_1:0' shape=(None, 64) dtype=float32>, edges=<tf.Tensor 'MLPGraphNetwork/graph_network/edge_block/layer_norm/batchnorm/add_1:0' shape=(None, 64) dtype=float32>, receivers=<tf.Tensor 'graphs_2:0' shape=(None,) dtype=int32>, senders=<tf.Tensor 'graphs_3:0' shape=(None,) dtype=int32>, globals=<tf.Tensor 'MLPGraphNetwork/graph_network/global_block/layer_norm/batchnorm/add_1:0' shape=(None, 64) dtype=float32>, n_node=<tf.Tensor 'graphs_5:0' shape=(None,) dtype=int64>, n_edge=<tf.Tensor 'graphs_6:0' shape=(None,) dtype=int64>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /tmp/ipykernel_7623/2085968999.py:24 train_step  *\n        loss = loss_fn(targets, class_preds)\n    /tmp/ipykernel_7623/3924948353.py:19 loss_fn  *\n        class_loss = bce_loss(targets[:,1:], class_preds)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/keras/losses.py:141 __call__  **\n        losses = call_fn(y_true, y_pred)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/keras/losses.py:245 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/keras/losses.py:1809 binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/keras/backend.py:5000 binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/ops/nn_impl.py:245 sigmoid_cross_entropy_with_logits_v2\n        return sigmoid_cross_entropy_with_logits(\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/ops/nn_impl.py:132 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((None, 64) vs (None, 1))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7623/1708186415.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgraph_data_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_tr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#         losses_tr_rg, losses_tr_cl, losses_tr = train_step(graph_data_tr, targets_tr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mlosses_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_data_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtraining_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 759\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    760\u001b[0m             *args, **kwds))\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /tmp/ipykernel_7623/2085968999.py:24 train_step  *\n        loss = loss_fn(targets, class_preds)\n    /tmp/ipykernel_7623/3924948353.py:19 loss_fn  *\n        class_loss = bce_loss(targets[:,1:], class_preds)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/keras/losses.py:141 __call__  **\n        losses = call_fn(y_true, y_pred)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/keras/losses.py:245 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/keras/losses.py:1809 binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/keras/backend.py:5000 binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/ops/nn_impl.py:245 sigmoid_cross_entropy_with_logits_v2\n        return sigmoid_cross_entropy_with_logits(\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/site-packages/tensorflow/python/ops/nn_impl.py:132 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((None, 64) vs (None, 1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-115:\n",
      "Process Process-120:\n",
      "Process Process-118:\n",
      "Process Process-113:\n",
      "Process Process-116:\n",
      "Process Process-117:\n",
      "Process Process-112:\n",
      "Process Process-111:\n",
      "Process Process-114:\n",
      "Process Process-119:\n",
      "Process Process-105:\n",
      "Process Process-108:\n",
      "Process Process-110:\n",
      "Process Process-109:\n",
      "Process Process-102:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/global/home/users/mfong/anaconda3/envs/gn4pions/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "curr_loss = 1e5\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    print(f'\\n\\nStarting epoch: {e}')\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Batchwise losses\n",
    "    training_loss = []\n",
    "    training_loss_regress = []\n",
    "    training_loss_class = []\n",
    "    val_loss = []\n",
    "    val_loss_regress = []\n",
    "    val_loss_class = []\n",
    "\n",
    "    # Train\n",
    "    print('Training...')\n",
    "    start = time.time()\n",
    "    for i, (graph_data_tr, targets_tr) in enumerate(get_batch(data_gen_train.generator())):\n",
    "#         losses_tr_rg, losses_tr_cl, losses_tr = train_step(graph_data_tr, targets_tr)\n",
    "        losses_tr = train_step(graph_data_tr, targets_tr)\n",
    "\n",
    "        training_loss.append(losses_tr.numpy())\n",
    "#         training_loss_regress.append(losses_tr_rg.numpy())\n",
    "#         training_loss_class.append(losses_tr_cl.numpy())\n",
    "\n",
    "        if not (i-1)%log_freq:\n",
    "            end = time.time()\n",
    "            print(f'Iter: {i:04d}, ', end='')\n",
    "            print(f'Tr_loss_mean: {np.mean(training_loss):.4f}, ', end='')\n",
    "#             print(f'Tr_loss_rg_mean: {np.mean(training_loss_regress):.4f}, ', end='') \n",
    "#             print(f'Tr_loss_cl_mean: {np.mean(training_loss_class):.4f}, ', end='') \n",
    "            print(f'Took {end-start:.4f}secs')\n",
    "            start = time.time()\n",
    "                  \n",
    "    training_loss_epoch.append(training_loss)\n",
    "#     training_loss_regress_epoch.append(training_loss_regress)\n",
    "#     training_loss_class_epoch.append(training_loss_class)\n",
    "    training_end = time.time()\n",
    "\n",
    "    # validate\n",
    "    print('\\nValidation...')\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    start = time.time()\n",
    "    for i, (graph_data_val, targets_val) in enumerate(get_batch(data_gen_val.generator())):\n",
    "#         losses_val_rg, losses_val_cl, losses_val, regress_vals, class_vals = val_step(graph_data_val, targets_val)\n",
    "        losses_val, class_vals = val_step(graph_data_val, targets_val)\n",
    "\n",
    "        targets_val = targets_val.numpy()\n",
    "#         regress_vals = regress_vals.numpy()\n",
    "        class_vals = class_vals.numpy()\n",
    "\n",
    "        targets_val[:,0] = 10**targets_val[:,0]\n",
    "#         regress_vals = 10**regress_vals\n",
    "        class_vals =  tf.math.sigmoid(class_vals)\n",
    "\n",
    "        output_vals = np.hstack([regress_vals, class_vals])\n",
    "\n",
    "        val_loss.append(losses_val.numpy())\n",
    "#         val_loss_regress.append(losses_val_rg.numpy())\n",
    "        val_loss_class.append(losses_val_cl.numpy())\n",
    "\n",
    "        all_targets.append(targets_val)\n",
    "        all_outputs.append(output_vals)\n",
    "\n",
    "        if not (i-1)%log_freq:\n",
    "            end = time.time()\n",
    "            print(f'Iter: {i:04d}, ', end='')\n",
    "            print(f'Val_loss_mean: {np.mean(val_loss):.4f}, ', end='')\n",
    "#             print(f'Val_loss_rg_mean: {np.mean(val_loss_regress):.4f}, ', end='') \n",
    "#             print(f'Val_loss_cl_mean: {np.mean(val_loss_class):.4f}, ', end='') \n",
    "            print(f'Took {end-start:.4f}secs')\n",
    "            start = time.time()\n",
    "\n",
    "    epoch_end = time.time()\n",
    "\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "\n",
    "    val_loss_epoch.append(val_loss)\n",
    "#     val_loss_regress_epoch.append(val_loss_regress)\n",
    "    val_loss_class_epoch.append(val_loss_class)\n",
    "\n",
    "    \n",
    "    # Book keeping\n",
    "    val_mins = int((epoch_end - training_end)/60)\n",
    "    val_secs = int((epoch_end - training_end)%60)\n",
    "    training_mins = int((training_end - epoch_start)/60)\n",
    "    training_secs = int((training_end - epoch_start)%60)\n",
    "    print(f'\\nEpoch {e} ended')\n",
    "    print(f'Training: {training_mins:2d}:{training_secs:02d}')\n",
    "    print(f'Validation: {val_mins:2d}:{val_secs:02d}')\n",
    "    \n",
    "    \n",
    "    # Save losses\n",
    "    np.savez(save_dir+'/losses', \n",
    "            training=training_loss_epoch, validation=val_loss_epoch,\n",
    "            training_regress=training_loss_regress_epoch, validation_regress=val_loss_regress_epoch,\n",
    "            training_class=training_loss_class_epoch, validation_class=val_loss_class_epoch,\n",
    "            )\n",
    "\n",
    "    \n",
    "    # Checkpoint if validation loss improved\n",
    "    if np.mean(val_loss)<curr_loss:\n",
    "        print(f'Loss decreased from {curr_loss:.4f} to {np.mean(val_loss):.4f}')\n",
    "        print(f'Checkpointing and saving predictions to:\\n{save_dir}')\n",
    "        curr_loss = np.mean(val_loss)\n",
    "        np.savez(save_dir+'/predictions', \n",
    "                targets=all_targets, \n",
    "                outputs=all_outputs)\n",
    "        checkpoint.save(checkpoint_prefix)\n",
    "    else: \n",
    "        print(f'Loss didnt decrease from {curr_loss:.4f}')\n",
    "    \n",
    "    \n",
    "    # Decrease learning rate every few epochs\n",
    "    if not (e+1)%2:   #%20:\n",
    "        optimizer.learning_rate = optimizer.learning_rate/10\n",
    "        print(f'Learning rate decreased to: {optimizer.learning_rate.value():.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f77bc68af40>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([np.mean(x) for x in val_loss_epoch])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
